% !TeX spellcheck = it_IT
\documentclass{beamer}
\usepackage[]{graphicx}
\usepackage[]{color}

\usetheme{CambridgeUS}
\usecolortheme{seahorse}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%  versione per stampa
%\documentclass[handout]{beamer}
\usepackage{pgfpages}
%\pgfpagesuselayout{8 on 1}[a4paper,border shrink=5mm,portrait]
%\usetheme{Pittsburgh}
%\usecolortheme{dove}

%\usecolortheme[RGB={60,165,222}]{structure} 
\usecolortheme[RGB={4,82,196}]{structure} 
\useoutertheme{modificatema}

%\setbeamercolor*{title}{fg=white,bg=gray!30!black}
\setbeamercolor*{frametitle}{parent=palette primary}

\setbeameroption{hide notes}

\usepackage{animate}
\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{boxedminipage}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{verbatim}
%\usepackage{amssymb}
\usepackage{eepic}
\usepackage{amssymb}
\usepackage{color}
\usepackage[italian]{babel}
\setlength{\parindent}{0pt}
%\addtolength{\oddsidemargin}{-2cm}
%\addtolength{\evensidemargin}{-1cm}
%\addtolength{\textwidth}{3cm}
%\addtolength{\topmargin}{-5mm}
%\addtolength{\voffset}{-1cm}
%\addtolength{\textheight}{45mm}
\newcommand{\cov}{\ensuremath{\text{cov}}}
\newcommand{\var}{\ensuremath{\text{var}}}

\newcommand{\spa}{\vspace{3mm}\centerline{ *\hspace{1cm}*\hspace{1cm}*}\vspace{0.5cm}}

\newcommand{\tstim}{\ensuremath{\hat{\theta}}}
\newcommand{\tvero}{\ensuremath{\theta_0}}
\newcommand{\epsi}{\ensuremath{\varepsilon}}

\newcommand{\corr}[1]{{\textcolor{red}{#1}} }

\newcommand{\spaziobianco}{\vspace{2mm}}
\newcommand{\spazio}{\noindent\makebox[\linewidth]{\resizebox{0.1\linewidth}{1pt}{{$\bullet$}}}}

\newcommand{\thicksimH}{\overset{H_0}{\thicksim}}
\newcommand{\thicksimind}{\overset{\text{{\tiny IND}}}{\thicksim}}

\newcommand{\espon}[1]{\exp\left\{#1\right\}}

\newcommand{\N}[2]{\ensuremath{\mathcal N\left(#1,#2\right)}}
\newcommand{\Nbiv}[5]{\ensuremath{\mathcal N\left(\left[\begin{matrix}#1 \\ #2\end{matrix}\right],\left[\begin{matrix}#3 & #5 \\ #5 & #4\end{matrix}\right]\right)}}
\newcommand{\eps}{\ensuremath{\varepsilon}}

\setbeamercolor{verdescuro}{fg=structure!50!black,bg=white}
\setbeamercolor{titolo}{fg=white,bg=structure!50!black}
\setbeamercolor{corpo}{fg=black,bg=gray!20!white}


\usepackage{bm}

\newcommand{\vbeta}{\bm{\beta}}
\newcommand{\vb}{\bm{b}}
\newcommand{\vf}{\bm{f}}
\newcommand{\vu}{\bm{u}}
\newcommand{\vy}{\bm{y}}
\newcommand{\vx}{\bm{x}}
\newcommand{\veps}{\bm{\varepsilon}}
\definecolor{light-gray}{gray}{0.95}

\setbeamercolor{evidenzia}{fg=black,bg=yellow!20!white}


\setbeamertemplate{frametitle continuation}[from second][{\small}]
\beamertemplatenavigationsymbolsempty
\newenvironment{scatola}[1]{%
\begin{center}~%
\begin{beamerboxesrounded}[upper=titolo,lower=corpo,shadow=true,width=0.7\textwidth]{#1}}%
{%
\end{beamerboxesrounded}~%
\end{center}}

\newenvironment{scatolone}[1]{%
\begin{center}~%
\begin{beamerboxesrounded}[upper=titolo,lower=corpo,shadow=true,width=0.95\textwidth]{#1}}%
{%
\end{beamerboxesrounded}~%
\end{center}}

%\newenvironment{definizione}%
%{\vspace{3mm}{DEFINIZIONE,}\begin{it}}%
%{\end{it}\vspace{3mm}}

\newenvironment{definizione}%
{\vspace{3mm}\begin{scatola}{Definizione}}%
{\end{scatola}\vspace{3mm}}

\newenvironment{teorema}%
{\vspace{3mm}\begin{scatola}{Teorema}}%
{\end{scatola}\vspace{3mm}}

\newenvironment{proprieta}%
{\vspace{3mm}\begin{scatola}{Propriet\`a}}%
{\end{scatola}\vspace{3mm}}


%\newcounter{nteo}
%\newenvironment{teorema}%
%{\vspace{3mm}{TEOREMA,}\begin{it}}%
%{\end{it}\vspace{3mm}}

%%\newcounter{nprop}
%\newenvironment{proprieta}%
%{\vspace{3mm}{PROPRIET\`A,}\begin{it}}%
%{\end{it}\vspace{3mm}}

\usepackage{bm}

\newcounter{nlucido}

%\author[ Trieste, 7 marzo 2012, lucido \thenlucido]{FP AS SZ}

\title[Regressione semiparametrica]{5. Regressione semiparametrica}

\author[F. Pauli]{Francesco Pauli}


\institute{DEAMS \\ Universit\`a di Trieste}

\date{A.A. 2017/2018}

\AtBeginSection[] % Do nothing for \section*
{
\begin{frame}
\frametitle{Indice}
\tableofcontents[currentsection,hideallsubsections]
%\tableofcontents[currentsection]
\end{frame}
}
\AtBeginSubsection[] % Do nothing for \section*
{
\begin{frame}<beamer>
\frametitle{Indice}
\tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/shaded/hide]
%\tableofcontents[currentsection,currentsubsection]
\end{frame}
}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\SweaveOpts{concordance=TRUE}

%\input{richiami}




\maketitle
%\input{traccia}

<<prelim,echo=FALSE,include=FALSE,warning=FALSE>>=
library(knitr)
options(width=50)
mypref=function() opts_chunk$set(comment=NA, fig.width=6, fig.height=4,out.width='0.7\\textwidth',echo=FALSE,results='hide',fig.path="figure/020-regression-1",tidy=FALSE)
mypref()
library(xtable)
library(mgcv)
library(splines)
library(MASS)
@


\begin{frame}{Modelli parametrici, semiparametrici, non parametrici}
Si ha un modello parametrico quando la famiglia di distribuzioni all'-interno della quale cerchiamo una distribuzione che descriva i dati \`e indicizzata da un parametro $\theta\in\mathbb R^d$, \textbf{con $d$ non troppo grande e fisso}.

\spazio

Muoversi verso i metodi semiparametrici o non parametrici significa
\begin{itemize}
\item ridurre le assunzioni
\item aumentare il numero di parametri e, in qualche senso, stimarlo
\end{itemize}
(non c'\`e una separazione netta).

\spazio

\onslide*<1>{
Esempio: stima della distribuzione di $X_1,\ldots,X_n\thicksim F()$
\begin{itemize}
\item parametrica: si assume $F\in\mathcal F=\{F_\theta():\theta\in\mathbb R^d\}$, si stima $\theta$ (ad es. SMV),  $F_{\hat{\theta}}$ \`e la stima di $F$.
\item non parametrica: si assume $F$ sia una FdR: una buona stima \`e la {\bf FdR empirica}.
\end{itemize}
}
\onslide*<2>{
Esempio: stima della funzione di regressione $E(Y|X=x)$ da un campione $(x_i,Y_i)$, $i=1,\ldots,n$
\begin{itemize}
\item parametrica: si assume $E(Y|X=x)=\beta_1+\beta_2x$, si stima $\theta=(\beta_1,\beta_2)$.
\item non parametrica: si assume $E(Y|X=x)=f(x)$  dove $f$ appartiene a una classe di funzioni sufficientemente flessibile (non ci sono parametri di interesse diretto).
\end{itemize}
}

\end{frame}


\begin{frame}[t]{Regressione non parametrica}
Si assume che
\[ E(Y|X= x) = f(x);\;\;\; V(Y|X=x)=\sigma^2   \]
dove $f$ \`e una funzione ``regolare'' (continua con qualche derivata continua).

\spazio

Due approcci
\begin{itemize}
\item tecniche ``locali''%
\onslide*<1>{%
\begin{itemize}
\item se avessimo tante osservazioni per ciascun $x_0$ potremmo stimare $f(x_0)$ come una media campionaria.
\item in generale, avendo un'osservazione per ciascun $x$ potremmo usare le osservazioni vicine.
\end{itemize}
}%
\onslide*<2->{: stima $E(Y|X=x_0)$ usando punti vicini a $x_0$.}
\onslide*<2->{
\item tecniche ``globali''  (spline)%
}%
\onslide*<3->{%
: definiamo un modello flessibile per $f(x;{\bm\theta})$
}%
\onslide*<2>{
\begin{itemize}
\item definiamo una classe di funzioni $f(x;{\bm\theta})$ abbastanza flessibile da poter approssimare qualunque funzione regolare $f(\cdot)$
\item stimiamo  $f$ scegliendo il miglio rappresentante in $f(x;{\bm\theta})$
\end{itemize}
}
\end{itemize}
\onslide*<4->{
Un aspetto cruciale in entrambi i metodi \`e determinare quanto liscia debba essere $\hat{f}$ che si traduce in 
\begin{itemize}
\item decidere cosa significa ``vicino''
\item decidere quanto flessibile dev'essere il modello $f(x;{\bm\theta})$ 
\end{itemize}
In entrambi i casi, serve un compromesso tra distorsione e varianza.
}
\end{frame}



\section[Gaussiana]{Regressione non parametrica, caso gaussiano}


<<>>=
cmb=read.table("wmap.dat",header=TRUE)
lidar=read.table("lidar.dat",header=TRUE)
lidar=lidar[sort.list(lidar$range),]
bpd=read.table("bpd.dat",header=TRUE)
#cmb[1,]
#plot(cmb[,1],cmb[,2])
@


\begin{frame}[fragile]{Esempio: dati ``lidar''}
\begin{columns}
\column{0.45\textwidth}
<<plotlidar,out.width="0.9\\textwidth", out.height="0.9\\textwidth",echo=FALSE>>=
par(mar=c(5,4,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="range (standardized)",ylab="logratio")
@
\column{0.55\textwidth}
LIDAR = LIght Detection And Ranging
\begin{itemize}
\item \`e una tecnica per individuare composti chimici nell'atmosfera
\item $x$: distanza percorsa prima della riflessione
\item $y$: logaritmo del rapporto tra luce ricevuta tra le due fonti laser
\end{itemize}
\end{columns}

\begin{itemize}
\item L'obiettivo \`e stimare
\[ f(x) = E(Y|X=x) \]
\item
(Esempio ben noto dove le tecniche banali, trasformazioni o regressione polinomiale, funzionano male.)
\end{itemize}
\end{frame}



\begin{frame}[t]{LIDAR: modello lineare}
\begin{columns}[T]
\column{0.5\textwidth}
Assumiamo
\[ {\bf y} = X{\bm\beta} + {\bm\varepsilon} \]
dove $X\in\mathcal M_{n\times p}$, ${\bm\beta}\in\mathbb R^p$, ${\bm\varepsilon}\thicksim N(0,\sigma^2I)$, 
<<out.width='0.8\\textwidth',fig.width=5,fig.height=4>>=
par(mar=c(2,2,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",ylim=c(-1,0.2))
abline(lm(lidar$logratio~lidar$range))
@

Non molto soddisfacente...
\column{0.5\textwidth}
\onslide*<1>{
Usando la massima verosimiglianza
\[ {\hat{\bm\beta}} = (X^TX)^{-1}X^T{\bf y} \]
sicch\'e
\[ {\bf\hat y} = X{\hat{\bm\beta}}= X(X^TX)^{-1}X^T{\bf y} \]
dove $H=X(X^TX)^{-1}X^T$ \`e la matrice di proiezione da $\mathbb R^n$ al sottospazio generato dalle colonne di $X$, si ricordi che
\[ \mbox{trace}{H}=p \]
}
\onslide*<2>{
Si noti che
\[ {\bf\hat y} = X{\hat{\bm\beta}}{\bf y}= X(X^TX)^{-1}X^T{\bf y} \]
significa che la stima del valore atteso condizionato \`e
\[ \widehat{E(Y|X=x)} = \hat{f}(x) = \sum_{i=1}^n h_i(x) Y_i \]
dove
\[ h({\bf x})^T= {\bf x}^T(X^TX)^{-1}X^T \]
}
\end{columns}
\end{frame}



\begin{frame}{LIDAR: costante a tratti}
Consideriamo una partizione dello spazio della variabile esplicativa, indichiamo gli estremi degli intervalli con
\[ -\infty= c_0 < c_1 < \ldots < c_{K-1} < c_K = +\infty \]
\begin{columns}[T]
\column{0.5\textwidth}
<<out.width='0.8\\textwidth',fig.width=5,fig.height=5>>=
par(mar=c(5,4,0.2,0.2))
x=lidar$range
y=lidar$logratio
c=c(min(x),550,600,650,max(x))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y")
abline(v=c,lty=2)
y.hat=y
for (i in 1:(length(c)-1)) {
  #y.hat[(x>=c[i])&(x<c[i+1])]=mean(y[(x>=c[i])&(x<c[i+1])])
  lines(x[(x>=c[i])&(x<c[i+1])],rep(mean(y[(x>=c[i])&(x<c[i+1])]),length(x[(x>=c[i])&(x<c[i+1])])),lwd=2,col="red")
}

#lines(x,y.hat,lwd=2,col="red")
@
\column{0.5\textwidth}
e stimiamo $E(Y|X=x)$ assumendo sia costante negli intervalli
\[ \hat{f}(x) = \frac{\sum_{k=0}^{K-1}\sum_{i=1}^n y_i I_{[c_k,c_{k+1}]}(x_i)}{\sum_{k=0}^{K-1}\sum_{i=1}^n I_{[c_k,c_{k+1}]}(x_i)} \]

Il risultato 
\begin{itemize}
\item non \`e liscio (addirittura discontinuo), e 
\item dipende dalla scelta degli intervalli.
\end{itemize}
\end{columns}

\end{frame}


\begin{frame}[t]{LIDAR: media mobile\onslide*<2>{ (vicini pi\`u vicini)}}
Se assumiamo che $f(x)$ sia continua, allora \`e ragionevole stimare $f(x)$ come la media di valori di  $Y_i$ che corrispondono a $x_i$ vicini a $x$.
\begin{columns}[T]
\column{0.5\textwidth}
\onslide*<1>{
<<out.width='0.8\\textwidth',fig.width=5,fig.height=5>>=
par(mar=c(5,4,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y")
runmean=function(x,xx,yy,h){
  mean(yy[abs(xx-x)<h])
}
runmean2=function(x,xx,yy,h)  
  mapply(runmean,x,MoreArgs=list(xx=xx,yy=yy,h=h))
curve(runmean2(x,lidar$range,lidar$logratio,h=10),add=TRUE,lwd=2,col="red")
curve(runmean2(x,lidar$range,lidar$logratio,h=50),add=TRUE,lwd=2,col="darkgreen")
#lines(x,y.hat,lwd=2,col="red")
legend(400,-0.6,legend=c("h=10","h=50"),lwd=2,col=c("red","darkgreen"))
@

}
\onslide*<2>{
<<out.width='0.8\\textwidth',fig.width=5,fig.height=5>>=
par(mar=c(5,4,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y")
hneighmean=function(x,xx,yy,h){
  mean(yy[abs(xx-x)<sort(abs(xx-x))[h]])
}
hneighmean2=function(x,xx,yy,h)  
  mapply(hneighmean,x,MoreArgs=list(xx=xx,yy=yy,h=h))
curve(hneighmean2(x,lidar$range,lidar$logratio,h=5),add=TRUE,lwd=2,col="red")
curve(hneighmean2(x,lidar$range,lidar$logratio,h=30),add=TRUE,lwd=2,col="darkgreen")
#lines(x,y.hat,lwd=2,col="red")
legend(400,-0.6,legend=c("k=5","k=30"),lwd=2,col=c("red","darkgreen"))
@

}
\column{0.5\textwidth}
\onslide*<1>{

In particulare potremmo usare la media di quegli $x_i$ che giacciono in un intorno di $x$ di raggio $h$
\[ \hat{f}(x) = \frac{\sum_{i=1}^n y_i I_{h}(|x-x_i|)}{\sum_{i=1}^n I_{h}(|x-x_i|)} \]

}

\onslide<2>{
Alternativamente potremmo usare la media dei $k$ vicini pi\`u vicini ad $x$, 
\[ N_k(x)=\{x_i: |x-x_i|\leq d_{(k)}\} \]
dove $d_i=|x-x_i|$ e $d_{(1)}\leq \ldots\leq d_{(n)}$ sono le distanze ordinate, allora
\[ \hat{f}(x) = \frac{1}{k} \sum_{i=1}^n y_i I_{N_k(x)}(x_i) \]

}
\end{columns}

\end{frame}




\begin{frame}{Errore di stima: distorsione e varianza}
La stima
\[ \hat{f}(x) = \frac{1}{k} \sum_{i=1}^n y_i I_{N_k(x)}(x_i)  
= \frac{1}{k} \sum_{y_i\in N_k(x)} y_i  \]
dove $N_k(x)=\{x_i: |x-x_i|\leq d_{(k)}\}$ \`e basata su  $k$ osservazioni: {\bf pi\`u grande \`e $k$}, 
\begin{itemize}
\item pi\`u osservazioni sono usate e quindi minore \`e la variabilit\`a:
\begin{itemize}
\item<2>  {\bf pi\`u piccola \`e la  varianza}
\end{itemize}
\item d'altra parte, s'impiegano osservazioni pi\`u distanti, a seconda della forma di $f()$ in un intorno di $x$, la media delle osservazioni pu\`o differire pi\`u o meno marcatamente da $E(Y|X=x)=f(x)$:
\begin{itemize}
\item<2>  {\bf pi\`u grande \`e la distorsione}
\end{itemize}
\end{itemize}
Il compromesso tra distorsione e varianza \`e una caratteristica distintiva dei lisciatori.
\end{frame}

\begin{frame}{Distorsione, forma di $f$ e $k$}
<<out.width='0.9\\textwidth',fig.width=7,fig.height=5>>=
par(mar=c(2,1,1,0))
layout(matrix(c(1:8),byrow=FALSE,nrow=2))
n=50
k1=10
k2=40
x=sort(runif(n,0,1))
x0=0.5
ind1=sort.list(abs(x-x0),decreasing = FALSE)[1:k1]
ind2=sort.list(abs(x-x0),decreasing = FALSE)[1:k2]
x1=x[sort.list(abs(x-x0),decreasing = FALSE)[1:k1]]
x2=x[sort.list(abs(x-x0),decreasing = FALSE)[1:k2]]

ff=function(x) 0.5+0*x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf=function(){
  plot(x,y,ylim=c(-0.1,1),yaxt="n",xaxt="n",xlab="",ylab="",yaxs="i")
  axis(1,at=range(x1),labels=c("",""))
  axis(1,at=x0,labels=expression(x[0]))
  rect(min(x1),-0.1,max(x1),1,lwd=2,col=hsv(184/360,.57,.98,alpha=0.2),border=NA)
  lines(range(x1),-0.1*c(1,1),lwd=2,col="blue")
  curve(ff(x),add=TRUE,lwd=1,n=100)
  points(x1,y[ind1],col="blue",pch=20)
  points(x0,mean(y[ind1]),col="red",pch=4,cex=2)

  plot(x,y,ylim=c(-0.1,1),yaxt="n",xaxt="n",xlab="",ylab="",yaxs="i")
  axis(1,at=range(x2),labels=c("",""))
  axis(1,at=x0,labels=expression(x[0]))
  lines(range(x2),-0.1*c(1,1),lwd=2,col="blue")
  rect(min(x2),-0.1,max(x2),1,lwd=2,col=hsv(184/360,.57,.98,alpha=0.2),border=NA)
  curve(ff(x),add=TRUE,lwd=1,n=100)
  points(x2,y[ind2],col="blue",pch=20)
  points(x0,mean(y[ind2]),col="red",pch=4,cex=2)
}
ungraf()

ff=function(x) x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf()

ff=function(x) 1-cos(pi*x^2/2)
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf()

ff=function(x) ((x-x0)*3)^2/2
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf()
@
\end{frame}

\begin{frame}[t]{Derivazione teorica di distorsione e varianza}
Sia $N_k(x)=\{x_i: |x-x_i|\leq d_{(k)}\}$, e lo stimatore 
\[ \hat{f}(x) = \frac{1}{k} \sum_{i=1}^n y_i I_{N_k(x)}(x_i) = \frac{1}{k} \sum_{y_i\in N_k(x)} y_i \]


\onslide*<1>{
La varianza \`e (assumendo $V(Y_i)=\sigma^2$ per ogni $i$)
\[ V(\hat{f}(x)) = \frac{1}{k}\sum_{y_i\in N_k(x)}V(Y_i) = \frac{\sigma^2}{k} \]
}
\onslide*<2>{
La distorsione \`e
\begin{align*}
E(\hat{f}(x))-f(x) 
&= \frac{1}{k} \sum_{N_k(x)} (f(x_i)-f(x)) \\
&\approx \frac{1}{k} \sum_{N_k(x)} \left(f'(x)(x_i-x)+\frac{1}{2}f''(x)(x_i-x)^2\right) \\
\intertext{assumendo le covariate equidistanziate: $x_{i+1}-x_i=\Delta$}
&\approx  \frac{2k(k+2)(k+1)}{6k} f''(x)\Delta^2
\end{align*}
}
\onslide*<3>{
Quindi l'MSE \`e
\[ E((\hat{f}(x)-f(x))^2) \approx \left(\underbrace{\frac{2k(k+2)(k+1)}{6k} f''(x)\Delta^2 }_{distorsione}\right)^2 + \underbrace{\frac{\sigma^2}{k}}_{varianza}\]
e quindi
\begin{itemize}
\item la distorsione cresce con $k$ e con $|f''|$
\item la varianza decresce con $k$
\end{itemize}
}
\end{frame}

<<>>=
sim=data.frame(x=seq(0,1,length=150)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
sim$y=sim$m+rnorm(nrow(sim),0,0.4)
@

\begin{frame}[t]{Distorsione e varianza, esempio}
Consideriamo un campione, la vera $f(\cdot)$ \`e in verde, 
\onslide<2->{
\begin{itemize}
\item calcoliamo distorsione, varianza e quindi MSE per $\hat{f}_k(0.6)$ in funzione di $k$, individuiamo un valore ottimale di $k$.
\onslide<3>{
\item facciamo lo stesso per $\hat{f}_k(0.9)$, otteniamo un {\bf diverso} valore ottimale di $k$.
}
\end{itemize}
} 

\begin{columns}[T]
\column{0.33\textwidth}
\onslide*<1>{%
<<out.width='0.95\\textwidth',fig.width=4,fig.height=5>>=
par(mar=c(5,4,2,0.3))

sig2=0.4^2
DDf=function(x) 12*pi*x*cos(2*pi*x^3) - 36*pi^2*x^4*sin(2*pi*x^3)
Delta=mean(sim$x[-1]-sim$x[-length(sim$x)])

plot(sim$x,sim$y,xlab="x",ylab="y")
lines(sim$x,sim$m,col="darkgreen",lwd=2)
@
}%
\onslide*<2>{%
<<out.width='0.95\\textwidth',fig.width=4,fig.height=5>>=
par(mar=c(5,4,2,0.3))

plot(sim$x,sim$y,xlab="x",ylab="y")
lines(sim$x,sim$m,col="darkgreen",lwd=2)
curve(hneighmean2(x,sim$x,sim$y,h=25),add=TRUE,lwd=2,col="red")
abline(v=0.6)
@
}%
\onslide*<3>{%
<<out.width='0.95\\textwidth',fig.width=4,fig.height=5>>=
par(mar=c(5,4,2,0.3))

plot(sim$x,sim$y,xlab="x",ylab="y")
lines(sim$x,sim$m,col="darkgreen",lwd=2)
curve(hneighmean2(x,sim$x,sim$y,h=25),add=TRUE,lwd=2,col="red")
abline(v=0.6)

lines(sim$x,sim$m,col="darkgreen",lwd=2)
curve(hneighmean2(x,sim$x,sim$y,h=10),add=TRUE,lwd=2,col="blue")
abline(v=0.9)

@
}%
\column{0.33\textwidth}
\onslide*<2-3>{
<<out.width='0.95\\textwidth',fig.width=4,fig.height=5>>=
par(mar=c(5,4,2,0.3))
curve(sig2/x,from=1,to=40,main="x=0.6",xlab="k",ylab="MSE")
curve(((x+2)^2/(6*4)*DDf(0.6)*Delta^2)^2,add=TRUE)
curve(sig2/x+((x+2)^2/(6*4)*DDf(0.6)*Delta^2)^2,add=TRUE,lwd=2)
@
}%
\column{0.33\textwidth}
\onslide*<3>{
<<out.width='0.95\\textwidth',fig.width=4,fig.height=5>>=
par(mar=c(5,4,2,0.3))
curve(sig2/x,from=1,to=40,main="x=0.9",xlab="k",ylab="MSE")
curve(((x+2)^2/(6*4)*DDf(0.9)*Delta^2)^2,add=TRUE)
curve(sig2/x+((x+2)^2/(6*4)*DDf(0.9)*Delta^2)^2,add=TRUE,lwd=2)
@
}%
\end{columns}

\end{frame}



\begin{frame}{Da $MSE(x)$ all'errore complessivo}
Disponiamo dell'MSE per $\hat{f}_k(x)$:
\[
 MSE(\hat{f}_k(x)) =  E((f(x)-\hat{f}_k(x))^2) = (f(x)-E(\hat{f}_k(x)))^2 + V(\hat{f}_k(x)) %= \mbox{bias}_x^2 + \mbox{variance}_x
\]
mettiamoli insieme per ottenere un errore complessivo
\[ R(k) = E\left(\frac{1}{n}\sum_{i=1}^n (\hat{f}_k(x_i) - f(x_i))^2\right) \]
%The MSE is usually estmated by the {\bf leave-one-out cross validation}
%\[
%CV = \hat{R}(h) =\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}_{-i}(x_i))^2
%\]
%where $\hat{f}_i$ is the estimator obtained omitting the $i$-the pair $(x_i,Y_i)$.
\begin{columns}
\column{0.6\textwidth}
<<out.width='0.9\\textwidth',fig.width=5,fig.height=3.5>>=
par(mar=c(5,4,0.3,0.3))

distcompl=apply(outer(1:40,sim$x,FUN=function(x,y) ((x+2)^2/(6*4)*DDf(y)*Delta^2)^2),1,mean)
curve(sig2/x,from=1,to=40,xlab="k",ylab="R(k)")
lines(1:40,distcompl)
lines(1:40,sig2/1:40+distcompl,lwd=2)
@
\column{0.4\textwidth}
La scelta di $k$ potrebbe basarsi su $R(k)$, ha senso scegliere $k=\mbox{argmin}_k R(k)$.
\end{columns}
\end{frame}

\begin{frame}{Stimatore di $R()$}
L'obiettivo \`e stimare
\[ R(k) = E\left(\frac{1}{n}\sum_{i=1}^n (\hat{f}_k(x_i) - f(x_i))^2\right) \]
(principalmente per individuare il valore ottimale di $k$.)

\spazio

Uno stimatore na\"if sarebbe
\[  \frac{1}{n} \sum_{i=1}^n (Y_i-\hat{f}_k(x_i))^2 \]
ma questo \`e ovviamente una sottostima in quanto ...

\end{frame}


\begin{frame}{Stimatore di $R()$: validazione incrociata uno  a uno}
Uno stimatore migliore per $R(k)$ \`e
\[
CV(k) = \hat{R}(k) =\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}_{k,-i}(x_i))^2
\]
dove $\hat{f}_{k,-i}(x_i)$ \`e il lisciatore stimato {\bf senza} l'$i$-esima osservazione.

Si noti che
\begin{eqnarray*}
E(Y_i-\hat{f}_{k,-i}(x_i))^2 
&=& E(Y_i-f(x_i)+f(x_i)-\hat{f}_{k,-i}(x_i))^2 \\
&=& \sigma^2 + E(f(x_i)-\hat{f}_{h,-i}(x_i))^2 \\
&\approx& \sigma^2 + E(f(x_i)-\hat{f}_{k}(x_i))^2 
\end{eqnarray*}
Cio\`e, $\hat{R}$ \`e, approssimativamente, uno stimatore non distorto dell'errore di previsione
\[ E(\hat{R}) \approx R + \sigma^2  \]
\end{frame}


\begin{frame}{Lisciatori lineari}
Discutiamo della stima dell'errore per una classe di lisciatori che comprende quelli visti sopra e molti altri: i {\bf lisciatori lineari}, ovvero dei lisciatori per i quali esiste, per ogni $x$, un vettore $\ell(x)=(\ell_1(x),\ldots,\ell_n(x))^T$ tale che
\[ \hat{f}(x) = \sum_{i=1}^n \ell_i(x) Y_i  \]
il che significa che
\[
{\bf\hat{f}} =
\begin{bmatrix}
\hat{f}(x_1) \\ \vdots \\ \hat{f}(x_n)
\end{bmatrix}
=
\begin{bmatrix}
\ell_1(x_1) & \cdots & \ell_n(x_1) \\ \vdots \\ \ell_1(x_n) & \cdots & \ell_n(x_n) 
\end{bmatrix}
{\bf Y} 
= L{\bf Y}
\]
La matrice $L$ \`e la {\bf matrice di lisciamento}, si definiscono anche i gradi di libert\`a del lisciatore come
\[ \nu = tr(L) \]
\end{frame}

\begin{frame}{Lisciatori lineari}
I precedenti lisciatori sono tutti lisciatori lineari, ricaviamo le matrici $L$ ad essi associati. 
(Senza perdita di generalit\`a, si assume che le $x_i$ siano ordinate).
\begin{itemize}
\item regressogramma:  $L$ \`e diagonale a blocchi e assume valore pari al reciproco del numero di osservazioni in ciascun blocco.
\item medie mobili 
\begin{itemize}
\item vicini pi\`u vicini: $L$ \`e 0 ovunque tranne che su una striscia intorno alla diagonale dove vale $1/k$.
\item raggio: $L$ \`e analoga al caso precedente se le $x_i$ sono equidistanziate, altrimenti...
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Validazione incrociata uno  a uno per lisciatori lineari}
Per un lisciatore lineare definito dalla matrice $L$ 
\[
CV = \hat{R}(k) =\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}_{k,-i}(x_i))^2
= \frac{1}{n} \sum_{i=1}^n\left(\frac{Y_i-\hat{f}_k(x_i)}{1-L_{ii}}\right)^2
\]
sicch\'e non necessita di ricalcolare il lisciatore ma solo di conoscere  $L_{ii}$.

\spazio

Un'ulteriore semplificazione \`e costituita dal {\bf criterio di validazione incrociata generalizzata} che prevede di sostituire $L_{ii}$ con il suo valore medio
\[ 
GCV = \frac{1}{n} \sum_{i=1}^n\left(\frac{Y_i-\hat{f}(x_i)}{1-\nu/n}\right)^2
\]
\end{frame}

\begin{frame}[allowframebreaks=0.95]{Derivazione delle formule per CV e GCV}
Definiamo $\hat{f}_{-i}(x_i)$. Essendo
\[
\hat{f}(x_i) = \sum_{j=1}^n \ell_j(x_i) y_j
\]
e assumendo $\sum_{j=1}^n \ell_j(x_i)=1$ (le costanti sono mantenute), definiamo
\[
\hat{f}_{-i}(x_i) = \frac{\sum_{j\neq i} \ell_j(x_i) y_j}{\sum_{j\neq i} \ell_j(x_i) } = \frac{\sum_{j\neq i} \ell_j(x_i) y_j}{1- \ell_i(x_i) }= \frac{\sum_{j\neq i} \ell_j(x_i) y_j}{1- L_{ii} }
\]

\spazio

Si noti che potremmo definire $\hat{f}_{-i}()$ come il lisciatore ri-stimato senza  $(x_i,y_i)$, la definizione \`e equivalente per lo stimatore a raggio, non per i $k$ pi\`u vicini.

\break

Con la formula sopra si ottiene
\begin{align*}
y_i-\hat{y}_{-i} 
&= y_i - \frac{1}{1-L_{ii}} \sum_{j\neq i} \ell_j(x_i) y_j \\
&= y_i - \frac{1}{1-L_{ii}} \left(\sum_{j=1}^n \ell_j(x_i) y_j - L_{ii}y_i\right) \\
&= y_i - \frac{1}{1-L_{ii}} \left(\hat{y}_i - L_{ii}y_i\right) \\
&= \frac{1}{1-L{ii}} \left((1-L_{ii})y_i - \hat{y}_i + L_{ii}y_i\right) = \frac{1}{1-L{ii}}(y_i-\hat{y}_i) 
\end{align*}
e quindi la formula del GCV.


\end{frame}


\begin{frame}{Altri criteri}
Si noti che, essendo $(1-x)^{-2}\approx 1+2x$ in un intorno di $0$, il GCV \`e approssimativamente uguale al $C_p$ di Mallow.
\[ 
GCV = \frac{1}{n} \sum_{i=1}^n\left(\frac{Y_i-\hat{f}(x_i)}{1-\nu/n}\right)^2
\approx \frac{1}{n} \sum_{i=1}^n\left(Y_i-\hat{f}(x_i)\right)^2 + \frac{2\nu\hat{\sigma}^2}{n} = C_p
\]
Pi\`u in generale, molti criteri usati per la scelta del grado di lisciamento ($k$) hanno la forma
\[
B(k) = \Lambda(n,k)\frac{1}{n} + \sum_{i=1}^n\left(Y_i-\hat{f}(x_i)\right)^2 
\]
per qualche funzione $\Lambda(\cdot,\cdot)$
\end{frame}

\section[Nucleo]{Regressione col metodo del nucleo}

\begin{frame}{Kernel regression}
Con i metodi descritti sin qui, man mano che ci si muove lungo l-asse $x$, si calcola $\hat{f}(x)$ come media di differenti gruppi di osservazioni $y_i$.

\spazio

Questo porta a una stima finale poco ``liscia''.

\spazio

Un modo di lisciare maggiormente \`e di usare una media pesata dove il peso delle osservazioni decresce man mano che ci si allontana da $x$.
\end{frame}

\begin{frame}{Stimatore di Nadaraya-Watson}
Lo stimatore di Nadaraya-Watson \`e  un lisciatore lineare
\[ \hat{f}(x) = \sum_{i=1}^n \ell_i(x) Y_i  \]
in cui
\[ \ell_i(x) = \frac{K\left(\frac{x-x_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-x_j}{h}\right)} \]
dove $K()$ \`e un nucleo.
\end{frame}

\begin{frame}{Nuclei}
\begin{columns}
\column{0.5\textwidth}
Si ha
%\begin{tiny}
\[
\hat{f}_n(x) = \frac{\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)Y_i}{\sum_{i=1}^n K\left(\frac{x-X_i}{h}\right)} 
\]
%\end{tiny}
dove $K$ \`e tale che
\begin{itemize}
\item $ K(x)\geq 0$
\item $ \int K(x)dx=1$
\item $ \int xK(x)dx=0$
\item $ \int x^2K(x)dx>0$
\end{itemize}
\column{0.5\textwidth}
Esempi di nuclei

\begin{tabular}{cc}\hline
& $K(u)$ \\\hline
Uniform & $\frac{1}{2}I_{[-1,1]}(u)$ \\
Triangle & $(1-|u|)I_{[-1,1]}(u)$ \\
Triweight & $\frac{35}{32}(1-u^2)^3I_{[-1,1]}(u)$ \\
Quartic & $\frac{15}{16}(1-u^2)^2I_{[-1,1]}(u)$ \\
Gaussian & $\frac{1}{\sqrt{2\pi}} e^{-u^2/2}$ \\
Epanechnikov & $\frac{3}{4}(1-u^2)I_{[-1,1]}(u)$ \\
Cosine & $\frac{\pi}{4}\cos\left(\frac{\pi}{2}u\right)I_{[-1,1]}(u)$ \\\hline
\end{tabular}
\end{columns}
\end{frame}






\begin{frame}{Kernel functions}

\begin{tabular}{cccc}
Uniform &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Uniform} 
}%
& 
Triangle &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Triangle} 
}%
\\
$\frac{1}{2}I_{[-1,1]}(u)$ & &  $(1-|u|)I_{[-1,1]}(u)$ & \\ & & & \\ & & & \\
Triweight &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Triweight} 
}%
& 
Quartic &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Quartic} 
}%
\\
 $\frac{35}{32}(1-u^2)^3I_{[-1,1]}(u)$ & &   $\frac{15}{16}(1-u^2)^2I_{[-1,1]}(u)$ & \\ & & & \\ & & & \\
Cosine &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Cosine} 
}%
& 
Epanechnikov &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Epanechnikov} 
}%
\\
$\frac{\pi}{4}\cos\left(\frac{\pi}{2}u\right)I_{[-1,1]}(u)$ & &    $\frac{3}{4}(1-u^2)I_{[-1,1]}(u)$  & \\ & & & \\ & & & \\
Gaussian &
\multirow{4}{*}{
\includegraphics[width=0.15\textwidth]{figure/Gaussian} 
}%
& 
 &
\multirow{4}{*}{}%
\\
$\frac{1}{\sqrt{2\pi}} e^{-u^2/2}$ & &      & \\ & & & \\ & & & \\

\end{tabular}
\end{frame}


\begin{frame}{Nadaraya-Watson estimator: risk}
Si mostra che, se $x_i$ proviene dalla densit\`a $g()$, per $h_n\rightarrow 0$ e $nh_n\rightarrow\infty$
\begin{align*}
R = &\frac{h_n^4}{4}\left(\int u^2K(u)du\right)^2\int \left(f''(x)+2f'(x)\frac{g'(x)}{g(x)}\right)^2dx \\
&\phantom{====}+ \frac{\sigma^2\int K^2(u)du}{nh_n}\int\frac{1}{g(x)}dx +o(nh_n^{-1}) +o(h_n^4)
\end{align*}

Dove si nota che
\begin{itemize}
\item la varianza decresce con $h$
\item la distorsione cresce con $h^4$
\item la distorsione cresce con  $f''$
\item la distorsione cresce con  $f'(x)\frac{g'(x)}{g(x)}$: {\it design bias}
\end{itemize}
\end{frame}


\begin{frame}{Design bias e boundary bias}
<<out.width='0.9\\textwidth',fig.width=7,fig.height=5>>=
par(mar=c(2,1,1,0))
laymat=matrix(c(1:8),byrow=FALSE,nrow=2)
laymat=laymat[c(1,1,2),]
layout(laymat)
n=200


ungraf2=function(bw=1){
  miny=min(y)-0.05*(max(y)-min(y))
  maxy=max(y)+0.05*(max(y)-min(y))
  plot(x,y,ylim=c(miny,maxy),yaxs="i",yaxt="n",xlab="",ylab="",col=gray(0.7))
  rug(x,line=0.5)
  a=ksmooth(x,y,bandwidth=bw)
  lines(a$x,a$y,lwd=2,col="blue")
  #rect(min(x1),-0.1,max(x1),1,lwd=2,col=hsv(184/360,.57,.98,alpha=0.2),border=NA)
  #lines(range(x1),-0.1*c(1,1),lwd=2,col="blue")
  curve(ff(x),add=TRUE,lwd=1,n=100,col="red")
  sig=0.25*bw/qnorm(0.75)
  curve(miny+0.1*(maxy-miny)*dnorm(x,0.5,sig)/dnorm(0.5,0.5,sig),add=TRUE,lwd=2,col="lightblue")
  hist(x,freq=FALSE,main="",yaxt="n",col=gray(0.7),border="white")
}


x=seq(0,1,length=n) #sort(runif(n,0,1))
ff=function(x) (x-0.5)^2
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf2(bw=0.4)

x=qnorm(seq(0,1,length=(n+2))[2:(n+1)]) #sort(rnorm(n,0,1))
x=(x-min(x))/(max(x)-min(x))
ff=function(x) (x-0.5)^2
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf2(bw=0.4)

x=seq(0,1,length=n) #sort(runif(n,0,1))
ff=function(x) 0.5+x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf2(bw=0.5)

x=qnorm(seq(0,1,length=(n+2))[2:(n+1)]) #sort(rnorm(n,0,1))
x=(x-min(x))/(max(x)-min(x))
ff=function(x) 0.5+x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf2(bw=0.5)

@
\end{frame}



\begin{frame}{Boundary bias}
<<fig.width=5,fig.height=5>>=
par(mar=c(5,4,0,0),mfrow=c(2,1))
plot(sim$x,sim$y,col=gray(0.7))
lines(sim$x,sim$m,col="red")
lines(ksmooth(sim$x,sim$y,kernel="normal",bandwidth = 0.1))

sim=data.frame(x=seq(0,1,length=150)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
plot(sim$x,sim$m,ylim=c(-2,2),type="l")
lines(sim$x,sim$m,col="red")
for (i in 1:30){
  sim$yR=sim$m+rnorm(nrow(sim),0,0.4)
  lines(ksmooth(sim$x,sim$yR,kernel="normal",bandwidth = 0.1))
}
lines(sim$x,sim$m,col="red",lwd=2)
@
\end{frame}


\begin{frame}{N-W come mimimo \onslide*<2->{$\rightarrow$ polinomi locali}}
\onslide*<1>{
Notiamo che lo stimatore di N-W in $x$, $\hat{f}(x)$, \`e la soluzione di
\[ \underset{a}{\mbox{argmin}} \sum_{i=1}^n K_i\left(\frac{x_i-x}{h}\right)(Y_i - a)^2 \]
cio\`e, lo stimatore di N-W \`e, localmente, uno stimatore dei minimi quadrati pesati.

\spazio
}
\onslide*<1->{
Si potrebbe allora impiegare i minimi quadrati pesati ma con un polinomio anzich\`e una costante, per ogni valore di $x$ si approssima  $f()$ in un intorno di $x$ con il polinomio
\[ p_x(u;{\bf a}) = a_0 + a_1(u-x) + \frac{a_2}{2!}(u-x)^2 +\ldots + \frac{a_p}{p!}(u-x)^p \]
}
\onslide*<2->{
e si stima ${\bf a}(x)$ (rendiamo esplicita la dipendenza da $x$) minimizzando
\[ \hat{\bf a}(x) = \underset{a}{\mbox{argmin}} \sum_{i=1}^n K_i\left(\frac{x_i-x}{h}\right)(Y_i - p_x(X_i;{\bf a}))^2 \]
e definiamo il seguente stimatore di $f(x)$ 
\[ \hat{f}(x) = p_x(x,\hat{\bf a}) = \hat{a}_0(x) \]
}
\end{frame}

\begin{frame}{Polinomi locali, notazione matriciale}
\onslide*<1>{
Sia
\[
X_x=
\begin{bmatrix}
1 & x_1-x & \cdots & \frac{1}{p!}(x_1-x)^p \\
\vdots & \vdots & & \vdots\\
1 & x_n-x & \cdots & \frac{1}{p!}(x_n-x)^p \\
\end{bmatrix}
\]
\[
W_x =\mbox{diag}\left\{K_i\left(\frac{x_i-x}{h}\right), i=1,\ldots, n\right\}
\]
allora la somma dei quadrati pesata \`e
\[
({\bf Y} -X_x{\bf a})^T W_x ({\bf Y} -X_x{\bf a})
\]
e
}
\onslide*<1->{
\[
\hat{\bf a} = (X_x^TW_xX_x)^TX_x^TW_x{\bf Y}
\]
}
\onslide*<2->{
Lo stimatore $\hat{f}(x)=\hat{a}_0(x)$ \`e dunque
\[ 
\hat{f}(x) = e_1^T(X_x^TW_xX_x)^TX_x^TW_x{\bf Y}
\]
dove $e_1^T=(1,0,\ldots,0)$.

Quindi $\hat{f}(x)$ \`e un lisciatore lineare
\[ \hat{f}(x) = \sum_{i=1}^n \ell_i(x) Y_i \]
dove
\[ \ell(x)^T = (\ell_1(x),\ldots,\ell_n(x))^T = e_1^T(X_x^TW_xX_x)^TX_x^TW_x \]
}
\end{frame}

\begin{frame}{Lisciatore lineare locale}
Posto $p=1$, si ottiene lo stimatore lineare locale
\[ \ell_i(x) = \frac{b_i(x)}{\sum_{j=1}^n b_j(x)} \]
dove
\[ b_i(x) = K\left(\frac{x_i-x}{h}\right)(S_{n,2}(x)-(x_i-x)S_{n,1}(x)) \]
\[ S_{n,j}(x) = \sum_{i=1}^nK\left(\frac{x_i-x}{h}\right)(x_i-x)^j,\;\;j=1,2\]
\end{frame}

\begin{frame}{Lisciatore lineare locale: distorsione e varianza}
Si mostra che il rischio in $x$ \`e
\begin{eqnarray*}
R_x = \frac{h_n^4}{4}\left( \int u^2K(u)du\right)^2 f''(x)^2 + \frac{\sigma^2\int K^2(u)du}{g(x)nh_n} +o(nh_n^{-1}) +o(h_n^4)
\end{eqnarray*}
\onslide<2>{
Se lo confrontiamo con quello dello stimatore di N-W notiamo che \`e scomparso il {\it design bias}.
\begin{align*}
R = &\frac{h_n^4}{4}\left(\int u^2K(u)du\right)^2\int \left(f''(x)+2f'(x)\frac{g'(x)}{g(x)}\right)^2dx \\
&\phantom{====}+ \frac{\sigma^2\int K^2(u)du}{nh_n}\int\frac{1}{g(x)}dx +o(nh_n^{-1}) +o(h_n^4)
\end{align*}

}

\end{frame}



\begin{frame}{Graphical representation}
<<animazregloc,echo=FALSE,fig.show='animate',out.width='0.9\\textwidth',out.height='0.8\\textheight',fig.width=10,fig.height=6,cache=TRUE>>=
n=100
x=sort(runif(n,0,1))
truefun=function(x) sin(x*2*pi)*(x<0.5)+0
y=truefun(x)+rnorm(n,0,0.1)
save(x,y,file="sim2.Rdata")
layout(matrix(c(1,1,2),ncol=1))
par(mar=c(2,2,0.2,0.2),cex=1.4)
stima=rep(NA,length(x))
for (i in 1:length(x)){
  plot(x,y,xaxt="n")
  fit=lm(y~x,weights=dnorm(x,x[i],0.1))
  abline(coef(fit))
  stima[i]=fit$fitted[i]
  points(x,y,col=gray(1-dnorm(x,x[i],0.1)/(1/(sqrt(2*pi)*0.1))),pch=20)
  points(x[i],y[i],col="green",pch=20)
  axis(1,at=x[i],lab=expression(x[i]))
  segments(x[i],-1,x[i],y[i])
  if (i>1)   lines(x[1:(i)],stima[1:(i)],pch=20,col="brown",lwd=2)
  points(x[i],stima[i],pch=20,col="red",cex=1.3)
  curve(dnorm(x,x[i],0.1),from=min(x),to=max(x))
}
@
\end{frame}

\begin{frame}{Graphical representation}
<<animazreglochbasso,echo=FALSE,fig.show='animate',out.width='0.9\\textwidth',out.height='0.8\\textheight',fig.width=10,fig.height=6,cache=TRUE>>=
load("sim2.Rdata")
layout(matrix(c(1,1,2),ncol=1))
par(mar=c(2,2,0.2,0.2),cex=1.4)
stima=rep(NA,length(x))
for (i in 1:length(x)){
  plot(x,y,xaxt="n")
  fit=lm(y~x,weights=dnorm(x,x[i],0.04))
  abline(coef(fit))
  stima[i]=fit$fitted[i]
  points(x,y,col=gray(1-dnorm(x,x[i],0.04)/(1/(sqrt(2*pi)*0.04))),pch=20)
  points(x[i],y[i],col="green",pch=20)
  axis(1,at=x[i],lab=expression(x[i]))
  segments(x[i],-1,x[i],y[i])
  if (i>1)   lines(x[1:(i)],stima[1:(i)],pch=20,col="brown",lwd=2)
  points(x[i],stima[i],pch=20,col="red",cex=1.3)
  curve(dnorm(x,x[i],0.04),from=min(x),to=max(x))
}
@
\end{frame}

\begin{frame}{Graphical representation: comparison}
<<animazreglochdoppia,echo=FALSE,fig.show='animate',out.width='0.9\\textwidth',out.height='0.8\\textheight',fig.width=10,fig.height=6,cache=TRUE>>=
load("sim2.Rdata")
layout(matrix(c(1,1,2,3,3,4),ncol=2,nrow=3))
par(mar=c(2,2,0.2,0.2),cex=1.4)
stima=stima1=rep(NA,length(x))
for (i in 1:length(x)){
  plot(x,y,xaxt="n")
  fit=lm(y~x,weights=dnorm(x,x[i],0.1))
  abline(coef(fit))
  stima[i]=fit$fitted[i]
  points(x,y,col=gray(1-dnorm(x,x[i],0.1)/(1/(sqrt(2*pi)*0.1))),pch=20)
  points(x[i],y[i],col="green",pch=20)
  axis(1,at=x[i],lab=expression(x[i]))
  segments(x[i],-1,x[i],y[i])
  if (i>1)   lines(x[1:(i)],stima[1:(i)],pch=20,col="brown",lwd=2)
  points(x[i],stima[i],pch=20,col="red",cex=1.3)
  curve(dnorm(x,x[i],0.1),from=min(x),to=max(x))
  ## second
  plot(x,y,xaxt="n")
  fit=lm(y~x,weights=dnorm(x,x[i],0.04))
  abline(coef(fit))
  stima1[i]=fit$fitted[i]
  points(x,y,col=gray(1-dnorm(x,x[i],0.04)/(1/(sqrt(2*pi)*0.04))),pch=20)
  points(x[i],y[i],col="green",pch=20)
  axis(1,at=x[i],lab=expression(x[i]))
  segments(x[i],-1,x[i],y[i])
  if (i>1)   lines(x[1:(i)],stima1[1:(i)],pch=20,col="brown",lwd=2)
  points(x[i],stima1[i],pch=20,col="red",cex=1.3)
  curve(dnorm(x,x[i],0.04),from=min(x),to=max(x))

}
@
\end{frame}




\begin{frame}{Boundary bias}

N-W versus local linear, same bandwith
\begin{center}
<<out.width='0.7\\textwidth',fig.width=7,fig.height=4,message=FALSE,warning=FALSE>>=
library(sm)
sim=data.frame(x=seq(0,1,length=150)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
par(mfrow=c(1,2),mar=c(2,2,0,0))
plot(sim$x,sim$m,ylim=c(-2,2),type="l")
lines(sim$x,sim$m,col="red")
for (i in 1:30){
  sim$yR=sim$m+rnorm(nrow(sim),0,0.4)
  lines(ksmooth(sim$x,sim$yR,kernel="normal",bandwidth = qnorm(0.75,0,0.07)/0.25))
}

plot(sim$x,sim$m,ylim=c(-2,2),type="l")
lines(sim$x,sim$m,col="red")
for (i in 1:30){
  sim$yR=sim$m+rnorm(nrow(sim),0,0.4)
  a=sm.regression(sim$x,sim$yR,h = 0.07,display='none')
  lines(a$eval.points,a$estimate)
}

@
\end{center}

\end{frame}


\begin{frame}{Design bias and boundary bias}
<<out.width='0.9\\textwidth',fig.width=7,fig.height=5>>=
par(mar=c(2,1,1,0))
laymat=matrix(c(1:8),byrow=FALSE,nrow=2)
laymat=laymat[c(1,1,2),]
layout(laymat)
n=200


ungraf3=function(bw=1){
  miny=min(y)-0.05*(max(y)-min(y))
  maxy=max(y)+0.05*(max(y)-min(y))
  plot(x,y,ylim=c(miny,maxy),yaxs="i",yaxt="n",xlab="",ylab="",col=gray(0.7))
  rug(x,line=0.5)
  sig=0.25*bw/qnorm(0.75)
  a=ksmooth(x,y,bandwidth=bw)
  lines(a$x,a$y,lwd=2,col="blue")
  a=sm.regression(x,y,h = sig,display='none')
  lines(a$eval.points,a$estimate,lwd=2)
  #rect(min(x1),-0.1,max(x1),1,lwd=2,col=hsv(184/360,.57,.98,alpha=0.2),border=NA)
  #lines(range(x1),-0.1*c(1,1),lwd=2,col="blue")
  curve(ff(x),add=TRUE,lwd=1,n=100,col="red")
  curve(miny+0.1*(maxy-miny)*dnorm(x,0.5,sig)/dnorm(0.5,0.5,sig),add=TRUE,lwd=2,col="lightblue")
  hist(x,freq=FALSE,main="",yaxt="n",col=gray(0.7),border="white")
}


x=seq(0,1,length=n) #sort(runif(n,0,1))
ff=function(x) (x-0.5)^2
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf3(bw=0.4)

x=qnorm(seq(0,1,length=(n+2))[2:(n+1)]) #sort(rnorm(n,0,1))
x=(x-min(x))/(max(x)-min(x))
ff=function(x) (x-0.5)^2
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf3(bw=0.4)

x=seq(0,1,length=n) #sort(runif(n,0,1))
ff=function(x) 0.5+x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf3(bw=0.5)

x=qnorm(seq(0,1,length=(n+2))[2:(n+1)]) #sort(rnorm(n,0,1))
x=(x-min(x))/(max(x)-min(x))
ff=function(x) 0.5+x
m=ff(x)
y=m+rnorm(n,0,0.025)
ungraf3(bw=0.5)

@
\end{frame}




<<>>=
lidar$range=(lidar$range-mean(lidar$range))/sd(lidar$range)
@

\begin{frame}[fragile]{LIDAR: modello polinomiale}
Assumiamo che $f()$ sia (approssimabile da) un polinomio
\[ f(x;\vbeta) = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_{p} x^p \]
\begin{columns}[T]
\column{0.5\textwidth}
<<fitpolylidar,out.width='0.8\\textwidth',fig.width=5,fig.height=5>>=
par(mar=c(5,4,0.2,0.2))
x=lidar$range
y=lidar$logratio
fit4=lm(y~x+I(x^2)+I(x^3)+I(x^4))
plot(lidar$range,lidar$logratio,pch=20,xlab="range (standardized)",ylab="logratio")
lines(x,predict(fit4),col="red",pch=20,lwd=3)
fit8=lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8))
lines(x,predict(fit8),col="darkgreen",pch=20,lwd=3)
legend(-1.6,-0.6,legend=c("p=4","p=8"),lwd=2,col=c("red","darkgreen"))
@
\column{0.5\textwidth}
Il ruolo di $k$ \`e svolto da $p$, al crescere di $p$
\begin{itemize}
\item aumenta la varianza
\item diminuisce la distorsione
\end{itemize}
Problema: elevata correlazione dei $\hat\beta_j$
\begin{scriptsize}
<<results='asis'>>=
print(xtable(round(cov2cor(vcov(fit4))[2:5,2:5],2)),floating=FALSE)
@
\end{scriptsize}
\end{columns}

\end{frame}


\begin{frame}[fragile]{LIDAR: modello lineare a tratti}
In alternativa, si potrebbe usare un modello lineare a tratti
\[ f(x;\vbeta) = \beta_{0,j} + \beta_{1,j} x \mbox{ se } c_{j-1}\leq x < c_j,\;j=1,\ldots,J \]
avendo suddiviso il supporto di $x$ in intervalli (cfr costante a tratti).
\begin{columns}[T]
\column{0.5\textwidth}
<<fitplmlidar,out.width='0.8\\textwidth',fig.width=5,fig.height=5>>=
par(mar=c(5,4,0.2,0.2))
x=lidar$range
y=lidar$logratio
bound=c(0,1)
bound1=c(min(x),bound,max(x))
fit=lm(y~x*I(x<0)+x*I(x<1))
plot(lidar$range,lidar$logratio,pch=20,xlab="range (standardized)",ylab="logratio")
points(x,predict(fit),col="red",pch=20)
@
\column{0.5\textwidth}
Il ruolo di $k$ \`e svolto da $J$, al crescere di $J$
\begin{itemize}
\item aumenta la varianza
\item diminuisce la distorsione
\end{itemize}
Problema: la funzione che si ottiene non \`e continua.
\end{columns}

\end{frame}



\section{Spline}


\begin{frame}{Spline lineare: esempio con due nodi}
\onslide*<1>{
Una soluzione pi\`u sofisticata si ottiene con
\[ y_i = \beta_1+\beta_2 x_i + \beta_3 (x_i-\nu_1)_+ + \beta_4 (x_i-\nu_2)_+ +\varepsilon_i \]
dove
\[
(x)_+=\begin{cases} x &\mbox{se }x>0 \\ 0 &\mbox{altrimenti}\end{cases};
\;\;\;\;
(x-\nu)_+=\begin{cases} x-\nu &\mbox{se }x>\nu \\ 0 &\mbox{altrimenti}\end{cases}
\]
\begin{itemize}
\item $\varepsilon_i \thicksim IID(\mathcal N(0,\sigma^2))$, 
\item $\nu_1$ e $\nu_2$, detti nodi, sono valori fissati nel supporto di $x$
\item $\beta_i$ sono stimati come al solito:
\[ \hat{\vbeta} = \underset{\vbeta}{\text{argmin}} \sum_{i=1}^n (y_i-f(x_i;\vbeta))^2 \]
dove
\[ f(x_i;\vbeta) = \beta_1+\beta_2 x_i + \beta_3 (x_i-\nu_1)_+ + \beta_4 (x_i-\nu_2)_+ \]
\end{itemize}
}

\onslide*<2>{
\begin{columns}
\column{0.6\textwidth}
<<baseA,echo=FALSE>>=
nodi=c(0,1)
p=1
@
<<graficobase,out.width="0.95\\textwidth", fig.width=7,fig.height=3,echo=FALSE>>=
par(mfrow=c(1,1),mar=c(2,4,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="range (standardized)",ylab="logratio")
fbase=function(x,k,p) ifelse(x-k>0,(x-k)^p,0)
lidar1=lidar
for (i in nodi){
 lidar1=cbind(lidar1,fbase(lidar1$range,i,p))
 names(lidar1)[ncol(lidar1)]=paste("b",i,sep="")
}
fit=lm(logratio~.,data=lidar1)
lines(lidar$range,predict(fit),col="red",lwd=2)
rug(nodi,lwd=4,col="blue")
@

<<graficobaseB,out.width="0.95\\textwidth",fig.width=7,fig.height=3>>=
fbase1=function(x,k,p) ifelse(x-k>0,(x-k)^p,NA)
par(mar=c(0,6,0,0),oma=c(2,0,0,0),mfrow=c(length(nodi)+1,1))
for (i in c(min(lidar$range),nodi[-length(nodi)])) curve(fbase1(x,i,p),add=FALSE,lwd=2,col="blue",xlim=range(lidar$range),xaxt="n",ylab="",las=1)
curve(fbase1(x,nodi[length(nodi)],p),xlim=range(lidar$range),lwd=2,col="blue",ylab="",las=1)
@

\column{0.4\textwidth}
\`E un modello lineare con esplicative
\[ x_i,\;(x_i-\nu_1)_+,\;(x_i-\nu_2)_+ \]
La funzione $\hat{f}(x)$ \`e una combinazione lineare delle funzioni 
\begin{align*}
B_0(x) &=x\\ 
B_1(x) &=(x-\nu_1)_+\\
B_2(x) &=(x-\nu_2)_+ 
\end{align*}
dette funzioni base (in blu nel grafico).
\end{columns}
}
\end{frame}


\begin{frame}{Spline lineare: $K$ nodi}
\onslide*<1>{
Pi\`u in generale, si fissano $K$ nodi
\[ \nu_1,\ldots, \nu_K \]
e si stima il modello lineare (attenzione alla notazione!)
\[ y_i = \beta_1+\beta_2 x_i + \sum_{k=1}^K b_k (x_i-\nu_k)_+ + \varepsilon_i \]
La funzione spline \`e rappresentata da
\[ f(x) = \beta_1+\beta_2 x + \sum_{k=1}^K b_k (x-\nu_k)_+ \]
ed \`e tanto pi\`u liscia quanti meno nodi si usano (minore $K$).
}
\onslide*<2>{
<<baseC,echo=FALSE>>=
nodi=c(seq(min(lidar$range),max(lidar$range),by=0.25))
p=1
@
<<graficobase3,ref.label="graficobase",out.width="0.8\\textwidth",fig.width=7,fig.height=2,echo=FALSE>>=
@

<<graficobaseB3,out.width="0.8\\textwidth",fig.width=7,fig.height=3,echo=FALSE>>=
par(mar=c(0,6,0,0),oma=c(2,0,0,0),mfrow=c(length(nodi)+1,1))
for (i in c(min(lidar$range),nodi[-length(nodi)])) curve(fbase1(x,i,p),add=FALSE,lwd=2,col="blue",xlim=range(lidar$range),xaxt="n",ylab="",yaxt="n",las=1)
curve(fbase1(x,nodi[length(nodi)],p),xlim=range(lidar$range),lwd=2,col="blue",ylab="",yaxt="n",las=1)
@
}
\onslide*<3>{
<<baseB,echo=FALSE>>=
nodi=c(-1,seq(0,1.5,by=0.1))
p=1
@
<<graficobase2,ref.label="graficobase",out.width="0.8\\textwidth",fig.width=7,fig.height=2,echo=FALSE>>=
@

<<graficobaseB2,out.width="0.8\\textwidth",fig.width=7,fig.height=3,echo=FALSE>>=
par(mar=c(0,6,0,0),oma=c(2,0,0,0),mfrow=c(length(nodi)+1,1))
for (i in c(min(lidar$range),nodi[-length(nodi)])) curve(fbase1(x,i,p),add=FALSE,lwd=2,col="blue",xlim=range(lidar$range),xaxt="n",ylab="",yaxt="n")
curve(fbase1(x,nodi[length(nodi)],p),xlim=range(lidar$range),lwd=2,col="blue",ylab="",yaxt="n")
@
}
\end{frame}




\begin{frame}{Base con potenze troncate}
\onslide*<1>{
Una naturale estensione della base lineare \`e data dalle potenze
\[ y_i = \beta_1+\beta_2 x_i + \ldots + \beta_{p+1}x^p + \sum_{k=1}^K b_k (x_i-\nu_k)_+^p + \varepsilon_i \]
sicch\'e la spline di ordine $p$ con $K$ nodi \`e
\[ f(x) = \beta_1+\beta_2 x + \ldots + \beta_{p+1}x^p + \sum_{k=1}^K b_k (x-\nu_k)_+^p \]
\begin{itemize}
\item Una spline di grado $p$ ha $p-1$ derivate continue, 
\item $p=3$ \`e adeguato per gli scopi usuali.
\end{itemize}
}
\onslide*<2>{
<<truncpowerbasis2,echo=FALSE>>=
nodi=c(-1,0,1)
p=3
@
<<graficobase5,ref.label="graficobase",out.width="0.8\\textwidth",fig.width=7,fig.height=2,echo=FALSE>>=
@

<<graficobaseB5,out.width="0.8\\textwidth",fig.width=7,fig.height=3,echo=FALSE>>=
par(mar=c(0,6,0,0),oma=c(2,0,0,0),mfrow=c(length(nodi),1))
for (i in nodi[-length(nodi)]) curve(fbase1(x,i,p),add=FALSE,lwd=2,col="blue",xlim=range(lidar$range),xaxt="n",ylab="",las=1)
curve(fbase1(x,nodi[length(nodi)],p),xlim=range(lidar$range),lwd=2,col="blue",ylab="",las=1)
@
}
\onslide*<3>{
<<truncpowerbasis,echo=FALSE>>=
nodi=c(seq(min(lidar$range),max(lidar$range),by=0.25))
p=3
@
<<graficobase4,ref.label="graficobase",out.width="0.8\\textwidth",fig.width=7,fig.height=2,echo=FALSE>>=
@


<<graficobaseB4,out.width="0.8\\textwidth",fig.width=7,fig.height=3,echo=FALSE>>=
par(mar=c(0,6,0,0),oma=c(2,0,0,0),mfrow=c(length(nodi),1))
for (i in nodi[-length(nodi)]) curve(fbase1(x,i,p),add=FALSE,lwd=2,col="blue",xlim=range(lidar$range),xaxt="n",ylab="",yaxt="n")
curve(fbase1(x,nodi[length(nodi)],p),xlim=range(lidar$range),lwd=2,col="blue",ylab="",yaxt="n")
@
}
\end{frame}

\begin{frame}[fragile]{TPB: diversi gradi}
<<diffdeg,out.width="0.8\\textwidth", out.height="0.8\\textheight",echo=FALSE>>=
par(mfrow=c(2,2),mar=c(3,3,2,0.2))
nodi=c(-1, 0, 0.5, 1)
fbase=function(x,k,p) ifelse(x-k>0,(x-k)^p,0)
for (p in 1:4) {
plot(lidar$range,lidar$logratio,pch=1,col=gray(0.8),xlab="",ylab="",main=paste("grado ",p))
rug(nodi,lwd=2)
lidar1=lidar
for (i in nodi){
 lidar1=cbind(lidar1,fbase(lidar1$range,i,p))
 names(lidar1)[ncol(lidar1)]=paste("b",i,sep="")}
fit=lm(logratio~.,data=lidar1)
lines(lidar$range,predict(fit),col="red",lwd=2)
}
@
\end{frame}


\begin{frame}{Levigatezza (smoothness) della spline e numero di nodi}
Per quanto visto sin qui, la spline \`e pi\`u o meno liscia (meno o pi\`u flessibile) a seconda del numero di nodi (fissato il grado): 
\begin{itemize}
\item 0 nodi: si riduce a un polinomio di grado $p$;
\item al crescere del numero di nodi, la funzione \`e sempre pi\`u flessibile (meno liscia);
\item tanti nodi quante le osservazioni distinte: $\hat{f}(x)$ interpola i punti esattamente;
\item pi\`u nodi delle osservazioni distinte: il modello non \`e identificato.
\end{itemize}
(Si noti anche che la posizione dei nodi determina in quali regioni la spline \`e pi\`u o meno liscia.)

\spazio

D'altra parte, pi\`u sono i nodi, pi\`u sono i parametri da stimare, quindi la maggior flessibilit\`a si ``paga'' in temrini di variabilit\`a degli stimatori.
\end{frame}

\begin{frame}{Levigatezza della spline e numero di nodi: distorsione v. varianza}
La scelta del numero di nodi ha un ruolo analogo alla scelta del numero $k$ di vicini pi\`u vicini, implicando un {\it trade off} tra distorsione e varianza
\begin{itemize}
\item pi\`u nodi $\leftrightarrow$ meno liscia $\leftrightarrow$ meno dist. pi\`u varianza;
\item meno nodi $\leftrightarrow$ pi\`u liscia $\leftrightarrow$ pi\`u dist. meno varianza;
\end{itemize}
La scelta del grado di levigatezza della spline \`e cruciale.

\spazio

In linea di principio, potremmo individuare il livello ottimale di levigatezza scegliendo il numero di nodi che minimizza l'errore quadratico medio: occcorrerebbe stimare funzioni spline corrispondenti a diverse scelte sul numero di nodi (glissiamo sul problema della loro posizione) e calcolare/stimare per ciascuna l'MSE.

\spazio

Questa strategia \`e ragionevole ma complessa dal punto di vista numerico, nel seguito opteremo per un'alternativa.
\end{frame}

\begin{frame}{Numero di nodi e distorsione}
\begin{center}
Spline di ordine \onslide*<1>{1}\onslide*<2>{2} (in rosso) per diverse scelte dei nodi (rappresentati sull'asse $x$), in verde la vera $f(x)$.

\vspace{3mm}

\onslide*<1>{
<<out.width='0.9\\textwidth',fig.width=7,fig.height=3>>=
x=seq(0,1,length=250) #sort(runif(150,0,1)))
m=(0.5+5*x)*sin(5*pi*x^3)
y=m+rnorm(length(x),0,0.2)
par(mar=c(2,2,0,0),mfrow=c(1,3))
nodi=c(1/3,2/3)
p=1
base=function(x,nodi,p=1){
  X=cbind(outer(x,0:p,FUN=function(x,y) x^y),
          outer(x,nodi,FUN=function(x,y) ifelse(x-y>0,(x-y)^p,0)))
}
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")


nodi=seq(0,1,length=10)[-c(1,10)]
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")


nodi=seq(0,1,length=15)[-c(1,15)]
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")
@
}
\onslide*<2>{
<<out.width='0.9\\textwidth',fig.width=7,fig.height=3>>=
x=seq(0,1,length=250) #sort(runif(150,0,1)))
m=(0.5+5*x)*sin(5*pi*x^3)
y=m+rnorm(length(x),0,0.2)
par(mar=c(2,2,0,0),mfrow=c(1,3))
nodi=c(1/3,2/3)
p=3
base=function(x,nodi,p=1){
  X=cbind(outer(x,0:p,FUN=function(x,y) x^y),
          outer(x,nodi,FUN=function(x,y) ifelse(x-y>0,(x-y)^p,0)))
}
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")


nodi=seq(0,1,length=10)[-c(1,10)]
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")


nodi=seq(0,1,length=15)[-c(1,15)]
X=base(x,nodi,p)
fit=lm(y~X-1)
yt=X %*% fit$coef
plot(x,y)
rug(nodi,lwd=2,col="red")
lines(x,m,col="darkgreen")
lines(x,yt,lwd=2,col="red")
@
}


\end{center}
\end{frame}


\begin{frame}{Levigatezza della spline: nodi fissati}
Una strategia differente prevede di fissare i nodi e quindi
\begin{itemize}
\item[] imporre qualche restrizione sui coefficienti tale che cambiando la restrizione si cambi il grado di levigatezza.
\end{itemize}
o
\begin{itemize}
\item[] invece di stimare i coefficienti col metodo dei minimi quadrati, aggiungere una penalizzazione che favorisca funzioni pi\`u lisce.
\end{itemize}
Operando cos\`i, il grado di levigatezza dipende da un numero, che varia nel continuo.

%\spazio

%The second alternative leads to the idea of penalized sum of squares. (Note that it is equivalent to the first for some choices of constraint/penalization.)

\end{frame}




\section[Penalizzazione]{Verosimiglianza (somma dei quadrati) penalizzata}

\begin{frame}{Penalizzazione per la ruvidit\`a}
Dati i nodi e quindi una base, i parametri sono determinati minimizzando
\[ \sum_{i=1}^n (y_i-f(x_i,\vbeta,\vb))^2 + \lambda S(f(x,\vbeta,\vb)) \]
dove 
\begin{itemize}
\item $S(f(x,\vbeta,\vb))$ \`e una misura di quanto ``ruvida'' \`e $f()$, 
\item $\lambda>0$ \`e (almeno per ora) una costante fissata.
\end{itemize}
\end{frame}


\begin{frame}{Penalizzazione: tipo ridge}
Una penalizzazione semplice \`e data da
\[ S(f(x)) = \sum_{i=1}^K b_i^2 = \vb^T\vb \]
Si pu\`o mostrare che usare una penalizzazione di questo tipo equivale a porre un vincolo del tipo
\[ \sum_{i=1}^Kb_i^2<C \]
per qualche $C$.
\end{frame}



\begin{frame}[fragile]{TPB in notazione matriciale}
Consideriamo la base delle potenze troncate
\[ f(x) = \beta_1+\beta_2 x + \ldots + \beta_{p+1}x^p + \sum_{k=1}^K b_k (x-\nu_k)_+^p \]
e poniamo 
\begin{scriptsize}
\[
{\bm\theta}=\begin{bmatrix} \beta_1 \\ \vdots \\ \beta_{p+1} \\ b_1 \\ \vdots \\ b_K \end{bmatrix},\;\;\;\;
X=\begin{bmatrix}
1 & x_1 & x_1^2 & x_1^3 & (x_1-\nu_1)_+^3 & \ldots & (x_1-\nu_K)_+^3 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \\
1 & x_i & x_i^2 & x_i^3 & (x_i-\nu_1)_+^3 & \ldots & (x_i-\nu_K)_+^3 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \\
1 & x_n & x_n^2 & x_n^3 & (x_n-\nu_1)_+^3 & \ldots & (x_n-\nu_K)_+^3 
\end{bmatrix}
\]
\end{scriptsize}
Quindi $f({\bf x})=X{\bm\theta}$ e il modello \`e
\[ {\bf y} = X{\bm\theta} + {\bm\varepsilon}  \]
\end{frame}

\begin{frame}[fragile]{Penalizzazione e TPB}
Considerimao la penalizzazione ridge
\[ S(f(x,{\bm\theta})) = {\bm\theta}^TD{\bm\theta} \]
dove $D=\text{diag}(0_{p+1},1_K)$,

\spazio

Il minimo di
\[ \sum_{i=1}^n(y_i-f(x_i,{\bm\theta}))^2 - \lambda{\bm\theta}^TD{\bm\theta}  \]
si ha per
\[ {\bm\hat\theta} = (X^TX+\lambda D)^{-1}X^Ty \]
Quindi la spline, scritta in forma di lisciatore lineare, \`e
\[ \hat{y} = X(X^TX+\lambda D)^{-1}X^Ty \]
\end{frame}

\begin{frame}{Quanti sono i parametri?}
``Nominalmente'' il modello ha $K+p+1$ parametri (e la varianza)

\spazio

Per\`o, questi non possono variare liberamente per via della penalizzazione.

\spazio

Il numero effettivo di parametri \`e valutato come la traccia della matrice di lisciamento
\[ \mbox{trace}(X(X^TX+\lambda D)^{-1}X^T) \]

\spazio

(Analogamente, si ricordi che nel ML, il numero di parametri \`e la traccia della matrice di proiezione.)

\end{frame}

\begin{frame}{Penalizzazione: derivata seconda}
Un'alternativa \`e impiegare una derivata della funzione spline
\[ S(f(x)) = \int (f^{(q)}(t))^2dt \]
dove $q\leq\mbox{grado della spline}$. (In genere $q=2$ per una spline cubica.)

Si noti che, se la base \`e ${\bf B}() = (B_1(),\ldots,B_K())$, sicch\`e
\[ \hat{f}(x) = {\bm{b}}^T{\bf B}(x) \]
allora 
\[ S(f(x)) = \int (f^{(q)}(t))^2dt = {\bm{b}}^TD{\bm{b}} \]
dove
\[ D = \int_a^b  {\bf B}^{(q)}(x)[{\bf B}^{(q)}(x)]^Tdx \]
(In certi casi  si usano approssimazioni.)
\end{frame}


\begin{frame}{Funzioni e le loro derivate seconde}
Si riportano tre funzioni e i quadrati delle derivate seconde.
\begin{center}
<<out.width='0.9\\textwidth',fig.width=7,fig.height=3>>=
par(mar=c(2,2,0,0),mfrow=c(2,3))
layout(matrix(1:6,byrow=FALSE,nrow=2))
f=expression(sin(2*pi*x))
curve(eval(f,data.frame(x=x)),from=0,to=1,ylim=c(-1,1))
d2=D(D(f,name="x"),name="x")
curve(eval(d2,data.frame(x=x))^2,ylim=c(0,17000),yaxs="i")

f=expression(sin(3*pi*x))
curve(eval(f,data.frame(x=x)),from=0,to=1,ylim=c(-1,1))
d2=D(D(f,name="x"),name="x")
curve(eval(d2,data.frame(x=x))^2,ylim=c(0,17000),yaxs="i")

#f=expression((0.5+5*x)*sin(5*pi*x^3)/(5.5))
f=expression(0.76*(sin(pi*x)+sin(4*pi*x))-0.5)
curve(eval(f,data.frame(x=x)),from=0,to=1,ylim=c(-1,1))
d2=D(D(f,name="x"),name="x")
curve(eval(d2,data.frame(x=x))^2,ylim=c(0,17000),yaxs="i")
@
\end{center}
\end{frame}




\begin{frame}[fragile]{Spline penalizzate}
\begin{columns}
\column{0.7\textwidth}
<<tpbfunction,out.width="0.95\\textwidth",fig.height=6,fig.width=8,echo=FALSE>>=
tpb=function(x,nodi=NULL,p=3){
  if (is.null(nodi)) nodi=quantile(x,seq(0.1,0.9,by=0.2))
  if (length(nodi)==1) nodi=seq(min(x),max(x),length=nodi+2)[2:(nodi+1)]
  K=length(nodi)
  X=matrix(NA,ncol=1+p+K,nrow=length(x))
  X[,1]=1
  for (i in 1:p) X[,i+1]=x^i
  for (k in 1:K) X[,1+p+k]=ifelse(x-nodi[k]>0,(x-nodi[k])^p,0)
  D=diag(c(rep(0,p+1),rep(1,K)))
  return(list(X=X,D=D,nodi=nodi))
}
y.teo=function(y,x,l=0,nodi=NULL,p=3){
  bb=tpb(x,nodi,p)
  X=bb$X
  D=bb$D
  part=solve(t(X) %*% X + l*D ) %*% t(X)
  dof=sum(diag(part %*% X))
  yteo=X %*% part %*% y
  return(list(yteo=yteo,dof=dof,nodi=bb$nodi))
}

par(mfrow=c(1,1),mar=c(5,4,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",main="")
lines(lidar$range,y.teo(lidar$logratio,lidar$range,l=0,nodi=40)$yteo,col="red",lwd=2)
lines(lidar$range,y.teo(lidar$logratio,lidar$range,l=0.001,nodi=40)$yteo,col="blue",lwd=2)
lines(lidar$range,y.teo(lidar$logratio,lidar$range,l=0.00001,nodi=40)$yteo,col="green",lwd=2)
rug(seq(min(lidar$range),max(lidar$range),length=40+2)[2:(40+1)])
legend(-1.5,-0.4,legend=c(expression(paste(lambda,"=0.001")),expression(paste(lambda,"=0.00001")),expression(paste(lambda,"=0"))),lwd=2,col=c("blue","green","red"))
#y.teo(lidar$logratio,lidar$range,l=0.00001,nodi=40)$dof

@
\column{0.3\textwidth}
Nel grafico si riportano 3 spline stimate usando 40 nodi con diversi pesi per la penalizzazione (tipo ridge).
\end{columns}
\end{frame}



\begin{frame}[fragile]{Spline penalizzate e non}
<<knotseffectANpenunpen,out.width="0.95\\textwidth", out.height="0.8\\textheight",fig.width=9,fig.height=8,echo=FALSE,fig.show='animate'>>=
par(mfrow=c(1,1),mar=c(5,4,0.2,0.2))
plot(x,y,pch=20,xlab="x",ylab="y",ylim=range(y))
l=0.001
for (nn in 2:40){ 
  plot(x,y,pch=20,col=gray(0.9),xlab=paste("(",nn," knots)"),ylim=range(y))  
legend(-1.5,-0.4,legend=c(expression(paste(lambda,"=0.001")),expression(paste(lambda,"=0.00001")),expression(paste(lambda,"=0"))),lwd=2,col=c("blue","green","red"))
  yt=y.teo(y,x,l=0,nodi=nn)
  lines(x,yt$yteo,col="red",lwd=2)
  yt=y.teo(y,x,l=0.001,nodi=nn)
  lines(x,yt$yteo,col="blue",lwd=2)
  yt=y.teo(y,x,l=0.00001,nodi=nn)
  lines(x,yt$yteo,col="green",lwd=2)
  rug(yt$nodi,col="black",lwd=2)
}
@
\end{frame}



\begin{frame}[fragile]{Quindi quanti nodi?}
Il trucco della penalizzazione fa s\`i che possiamo fissare i nodi all'inizio, come per\`o?
\begin{itemize}
\item L'idea \`e che, siccome si usa la penalizzazione, la scelta dei nodi \`e poco importante purch\'e
\begin{itemize}
\item non siano troppo pochi (in generale: da 20 a 40 minimo),
\item ci siano osservazioni tra i nodi (almeno 4-5 osservazioni tra uno e l'altro)
\end{itemize}
\item Si possono fissare tanti nodi quante le osservazioni, pu\`o essere per\`o computazionalmente oneroso e, in genere, non \`e necessario.
\item Strategie tipiche per la scelta della posizione dei nodi sono
\begin{itemize}
\item quantili empirici di $x$ 
\item nodi equispaziati.
\end{itemize}
\end{itemize}

\begin{center}
{\bf Da qui in poi, i nodi $\nu_1,\ldots, \nu_K$ sono fissati in qualche modo.}
\end{center}

(Si noti che possiamo verificare se i nodi sono in numero sufficiente stimando il modello con un numero maggiore e verificando se le cose cambiano.)
\end{frame}



<<>>=
sim=data.frame(x=seq(0,1,length=20)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
sim$y=sim$m+rnorm(nrow(sim),0,0.4)


x=c(0, 0.05, 0.11, 0.16, 0.21, 0.26, 0.32, 0.37, 0.42, 0.47, 0.53, 0.58, 0.63, 0.68, 0.74, 0.79, 0.84, 0.89, 0.95, 1)
y=c(-0.4, 0.06, -0.04, -0.6, -0.09, 0.65, -0.15, 0, 1.15, 1.14, 0.31, 0.91, 0.99, 0.6, 0.89, 0.83, -0.6, -1.47, -0.93, 0.46)
## smoothing spline
nodi=x[-c(20)]
X=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)
X=cbind(1,x,x^2,x^3,X)
D=diag(c(rep(0,4),rep(1,length(nodi))))
l=0.000003
#l=0.001
#l=1
M=solve(t(X) %*% X + l*D ) %*% t(X)
L=X %*% M
coef0= M %*% y
yt=L %*% y

nodi1=seq(0,1,length=7)[-c(1,7)]
X1=outer(x,nodi1,FUN=function(x,y) abs(x-y)^3)
X1=cbind(1,x,x^2,x^3,X1)
D1=diag(c(rep(0,4),rep(1,length(nodi1))))

l1=0.000001
M1=solve(t(X1) %*% X1 + l1*D1 ) %*% t(X1)
L1=X1 %*% M1
coef1=M1 %*% y
yt1=L1 %*% y
eig=eigen(L)
eig$vectors=Re(eig$vectors)
eig$values=Re(eig$values)
eig1=eigen(L1)
eig1$vectors=Re(eig1$vectors)
eig1$values=Re(eig1$values)
@





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Scelta di $\lambda$}

\begin{frame}{Scelta di $\lambda$}
Il ruolo di $\lambda$ \`e analogo alla scelta del numero di vicini o della banda.

\spazio

Lo stesso criterio: validazione incrociata, pu\`o essere impiegato per la scelta.

\spazio

Si noti che la spline \`e un lisciatore lineare, quindi i risultati visti in generale per i lisciatori lineari si possono impiegare anche ora.

\spazio

Inotlre, anche la formula per il GCV \`e valida.
\end{frame}


\begin{frame}{Errore quadratico complessivo}
\onslide*<1>{
Let 
\begin{eqnarray*} 
\mbox{R}(\hat{f}) &=& E\left(\sum_{i=1}^n (\hat{f}(x_i)-f(x_i))^2 \right)    \\
&=& \sum_{i=1}^n [(E(\hat{f}(x_i))-f(x_i))^2 + V(\hat{f}(x_i))] \\
&=& (E(L{\bm y})-{\bm f})^T(E(L{\bm y})-{\bm f}) + \sum_{i=1}^n V(L{\bm y})_{ii} \\
&=& {\bm f}^T(L-I)^T(L-I){\bm f}  + \mbox{trace}[V(L{\bm y})] \\
&=& {\bm f}^T(L-I)^T(L-I){\bm f}  + \mbox{trace}[LV({\bm y})L^T] \\
&=& {\bm f}^T(L-I)^T(L-I){\bm f}  + \sigma^2_{\varepsilon}\mbox{trace}[LL^T]
\end{eqnarray*}
In questa scomposizione, la prima parte rappresenta il quadrato della distorsione, la seconda \`e la varianza.
}

\onslide*<2>{
\begin{center}
Esempio di scomposizione di R in distorsione e varianza

\begin{tabular}{cc}
<<aa,cache=TRUE,out.width='0.45\\textwidth',fig.width=5,fig.height=5>>=
b=2
c=6
a=1
sim=data.frame(x=seq(0,1,length=250))
sim$m=sin(2*pi*sim$x^3)
sigmaeps=0.2
sim$y=sim$m+rnorm(nrow(sim),0,sigmaeps)

x=sim$x
y=sim$y
m=sim$m
## smoothing spline
nodi=seq(0,1,length=20)[-c(1,20)]
X=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)
X=cbind(1,x,X)
D=diag(c(rep(0,2),rep(1,length(nodi))))

msse.bias=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  t(m) %*% t(L-diag(rep(1,nrow(L)))) %*% (L-diag(rep(1,nrow(L)))) %*% m
}
msse.var=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  sigmaeps^2*sum(diag(L %*% t(L)))
}
msse=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  t(m) %*% t(L-diag(rep(1,nrow(L)))) %*% (L-diag(rep(1,nrow(L)))) %*% m + sigmaeps^2*sum(diag(L %*% t(L)))
}
msse2.bias=function(x) mapply(msse.bias,l=x)
msse2.var=function(x) mapply(msse.var,l=x)
msse2=function(x) mapply(msse,l=x)
o=optimize(msse2,interval=c(0,1))
curve(msse2(x),from=0,to=4*o$minimum,lwd=2,ylim=c(0,1))
curve(msse2.var(x),add=TRUE)
curve(msse2.bias(x),add=TRUE)
@

&

<<cache=TRUE,out.width='0.45\\textwidth',fig.width=5,fig.height=5>>=
l=o$minimum
#l=0.001
#l=1
M=solve(t(X) %*% X + l*D ) %*% t(X)
L=X %*% M
coef0= M %*% y
yt=L %*% y

plot(x,y)
lines(x,yt,col="red")
lines(x,m)
@
\end{tabular}
\end{center}

}

\end{frame}

\begin{frame}{Gradi di libert\`a e penalizzazione}
Il coefficiente $\lambda$ determina la levigatezza della funzione stimata, \`e rilevante studiare come variano i gradi di libert\`a di $\hat{f}$ ( $\mbox{df}=\mbox{tr}(L)$) in funzione di $\lambda$.

<<cache=TRUE,out.width='0.9\\textwidth',fig.width=10,fig.height=5,fig.align='center'>>=
sim=data.frame(x=seq(0,1,length=250)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
sigmaeps=0.2
sim$y=sim$m+rnorm(nrow(sim),0,sigmaeps)

x=sim$x
y=sim$y
m=sim$m
## smoothing spline
nodi=seq(0,1,length=20)[-c(1,20)]
X=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)
X=cbind(1,x,X)
D=diag(c(rep(0,2),rep(1,length(nodi))))

edf=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  sum(diag(L))
}
edf2=function(x) mapply(edf,l=x)

par(mfrow=c(1,2),mar=c(5,4,0,0))
curve(edf2(x),from=0,to=10,lwd=2,xlab=expression(lambda),ylab="df",ylim=c(2,20))
abline(h=2,lty=2)
curve(edf2(exp(x)),from=-25,to=15,xaxt="n",lwd=2,xlab=expression(lambda),ylab="df",ylim=c(2,20))
xseq=c(1.0e-9,0.000001,0.001,1,10,1000,10^6)
axis(1,at=log(xseq),labels=xseq)
abline(h=2,lty=2)
@


\end{frame}



\begin{frame}[fragile]{Spline in R: {\tt gam} ({\tt mgcv})}
ci sono numerosi pacchetti per la stima di spline in R, uno dei pi\`u potenti e versatili \`e il pacchetto {\tt mgcv} di Wood.

\spazio

Le funzioni del pacchetto presentano molte opsioni, nella forma pi\`u semplice, comunque:

<<eval=FALSE,echo=TRUE,tidy=FALSE>>=
fit=gam(y~s(x))
fit.s=summary(fit)
plot(fit)
@

si effettua una stima scegliendo via GCV il parametro di lisciamento $\lambda$ (la scelta dei nodi \`e fatta in maniera predefinita di cui non ci preoccupiamo, volendo si possono cambiare).
\end{frame}

\begin{frame}[fragile]{Stimare una spline con {\tt gam}}
Tra gli argomenti della funzione {\tt s()}
\begin{itemize}
\item[k] dimensione della base
\item[fx] se usare una penalizzazione
\end{itemize}

<<eval=FALSE,echo=TRUE,tidy=FALSE>>=
fit2=gam(y~s(x,k=3))
fit4=gam(y~s(x,k=10))
plot(fit2)
plot(fit8)
@

\end{frame}

\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2,out.width='0.7\\textwidth'>>=
par(mar=c(2,2,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",ylim=c(-1,0.2))
@
\end{scriptsize}
\end{center}

\end{frame}


\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:3,out.width='0.7\\textwidth'>>=
par(mar=c(2,2,0.2,0.2))
fit=gam(logratio~s(range),data=lidar)
plot(fit)
@
\end{scriptsize}
\end{center}

\end{frame}

\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:3,out.width='0.7\\textwidth'>>=
par(mar=c(2,2,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",ylim=c(-1,0.2))
curve(predict(fit,newdata=data.frame(range=x)),add=TRUE)
@
\end{scriptsize}
\end{center}

\end{frame}

\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:7,out.width='0.7\\textwidth'>>=
par(mar=c(2,2,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",ylim=c(-1,0.2))
xx=seq(min(lidar$range),max(lidar$range),length=100)
pr=predict(fit,newdata=data.frame(range=xx),
              se.fit=TRUE)
matlines(xx,cbind(pr$fit-2*pr$se.fit,pr$fit,pr$fit+2*pr$se.fit),
        type="l",lty=c(2,1,2),col="black")
@
\end{scriptsize}
\end{center}

\end{frame}


\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{tiny}
<<eval=TRUE,echo=1,out.width='0.7\\textwidth',results='markup'>>=
summary(fit)
@
\end{tiny}
\end{center}

\end{frame}

\begin{frame}[fragile]{LIDAR: GAM}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:5,out.width='0.7\\textwidth'>>=
par(mar=c(2,2,0.2,0.2))
plot(lidar$range,lidar$logratio,pch=20,xlab="x",ylab="y",ylim=c(-1,0.2))
curve(predict(fit,newdata=data.frame(range=x)),add=TRUE)
fit1=gam(logratio~s(range,fx=TRUE,k=3),data=lidar)
curve(predict(fit1,newdata=data.frame(range=x)),add=TRUE,col="red")
@
\end{scriptsize}
\end{center}

\end{frame}



\begin{frame}[fragile]{Qualche esperimento con {\tt gam}}
``Verifichiamo'' che il numero di nodi \`e indifferente purch\'e abbastanza grande.
The number of knots does not matter provided it is high enough.

<<eval=FALSE,echo=TRUE,tidy=FALSE>>=
sim=data.frame(x=seq(0,1,length=200)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
sim$y=sim$m+rnorm(nrow(sim),0,0.4)
plot(sim$x,sim$y)
fit0=gam(y~s(x),data=sim)
fit1=gam(y~s(x,k=5),data=sim)
fit2=gam(y~s(x,k=12),data=sim)
fit3=gam(y~s(x,k=30),data=sim)
@

\end{frame}


\subsection{Perch\'e le spline}

\begin{frame}{Spline}

\begin{columns}

\column{0.5\textwidth}
Una spline di ordine $p$ con nodi $\nu_1,\ldots,\nu_K$ \`e un polinomio a tratti continuo con derivate continue fino all'ordine $p-1$.

\spazio

Il termine spline deriva dal nome di un attrezzo da disegno (una striscia di metallo flessibile usata per agevolare il disegno di curve).

\spazio

Una {\bf spline cubica} \`e una spline di ordine $p=3$ (continua con derivata seconda continua).

\column{0.05\textwidth}
\column{0.45\textwidth}



{\centering \includegraphics[width=0.75\textwidth]{figure/splinephys}


}

\begin{scriptsize}
A spline, or the more modern term flexible curve, consists of a long strip fixed in position at a number of points that relaxes to form and hold a smooth curve passing through those points for the purpose of transferring that curve to another material. (\href{https://en.wikipedia.org/wiki/Flat_spline}{Wikipedia})\par
\end{scriptsize}
\end{columns}
\end{frame}


\begin{frame}{Le spline naturali cubiche sono interpolanti ottimali}
Sia
\begin{itemize}
\item $(x_i,y_i)$, $i=1,\ldots,n$: dove $x_i<x_{i+1}$ 
\item $g(x)$ la spline cubica naturale che interpola i punti \\ (naturale significa che $g''(x_1)=g''(x_n)=0$)
\end{itemize}
allora $g()$ \`e l'interpolante pi\`u liscia nel senso che minimizza
\[ J(f) = \int_{x_1}^{x_n} f''(x)^2dx \]
tra le funzioni $f$ che interpolano i punti, sono assolutamente continue e hanno derivata prima continua.

\spazio

In altre parole, le spline cubiche naturali sono le funzioni pi\`u lisce per interpolare dei punti.
\end{frame}

\begin{frame}[t]{Dimostrazione}
Sia $f()$ interpolante $(x_i,y_i)$ e sia $h=f-g$
\begin{eqnarray*}
\int_{x_1}^{x_n}f''(x)^2dx 
\onslide*<1>{&=&  \int_{x_1}^{x_n}(g''(x)+h''(x))^2dx \\}
&=&  \int_{x_1}^{x_n}g''(x)^2dx + \int_{x_1}^{x_n}g''(x)h''(x)dx +\int_{x_1}^{x_n}h''(x)^2dx 
\end{eqnarray*}
\onslide*<2->{
Si ah anche, integrando per parti
\begin{align*}
\int_{x_1}^{x_n}g''(x)h''(x)dx 
\onslide*<2>{
&= g''(x_n)h'(x_n)-g''(x_1)h'(x_1) -  \int_{x_1}^{x_n}g'''(x)h'(x)dx  \\
%\intertext{si$g''(x_n)=g''(x_1)=0$}
& = -\int_{x_1}^{x_n}g'''(x)h'(x)dx  \\
&= -\sum_{i=1}^{n-1} g'''(x_i^+)\int_{x_1}^{x_n}h'(x)dx \\
&= -\sum_{i=1}^{n-1} g'''(x_i^+)(h(x_{i+1})-h(x_i))
} =0
\end{align*}
}
\onslide*<3>{
Quindi
\begin{eqnarray*}
\int_{x_1}^{x_n}f''(x)^2dx 
=  \int_{x_1}^{x_n}g''(x)^2dx +\int_{x_1}^{x_n}h''(x)^2dx \geq \int_{x_1}^{x_n}g''(x)^2dx
\end{eqnarray*}
dove si ha l'eguaglianza sse $h''(x)=0$ per $x_1<x<x_n$ cio\`e solo se $f=g$.
}
\end{frame}

\begin{frame}{Conseguenza}
La propriet\`a sopra significa che se minimizzo
\[ \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2dx \]
tra tutte le funzioni $f$ continue con derivata prima continua su $[x_1,x_n]$, allora il minimo \`e ua spline cubica naturale.

\spazio

DIM: supponiamo che $f^*$ minimizzi l'espressione sopra e non sia una spline cubica naturale, allora si prenda la spline cubica naturale che interpola $(x_i,f^*(x_i))$, questa comporta la medesima somma dei quadrati degli scarti, ma con una minore penalizzazione.

\end{frame}

\section[stima V]{Stima della varianza}

\begin{frame}[t]{Stima della varianza}

%\vspace{-5mm}

La varianza dell'errore $\sigma^2$ pu\`o essere stimata, analogamente a quanto si fa nel ML, con
\[ \hat{\sigma}^2 = \frac{\sum_{i=1}^n (y_i-\hat{f}(x_i))^2}{n-\mbox{df}} =  \frac{RSS}{n-\mbox{df}} \]
dove i gradi di libert\`a della spline sono $\mbox{tr}(L)$.

Si noti per\`o che
\begin{eqnarray*}
E(RSS) &=&  E(({\bf y}-\hat{\bf y})^T({\bf y}-\hat{\bf y})) \\
\onslide*<1>{
&=& E({\bf y}^T(L-I)^T(L-I){\bf y}) \\
&=& {\bf f}^T(L-I)^T(L-I){\bf f} + \sigma^2\mbox{tr}((L-I)^T(L-I)) \\}
&=& {\bf f}^T(L-I)^T(L-I){\bf f} + \sigma^2(\mbox{tr}(LL^T)-2\mbox{tr}(L)+n)
\end{eqnarray*}
quindi, assumendo che la distorsione sia trascurabile, uno stimatore non distorto per $\sigma^2$ \`e
\[ \tilde{\sigma}^2 = \frac{RSS}{n-2\mbox{tr}(L)+\mbox{tr}(LL^T)} \]
\onslide*<2>{
Dove si ha che
\begin{itemize}
\item $n-2\mbox{tr}(L)+\mbox{tr}(LL^T)$ sono i GdL residui del modello
\item $2\mbox{tr}(L)-\mbox{tr}(LL^T)$ \`e una misura alternativa dei GdL del modello
\end{itemize}
}
\end{frame}

\begin{frame}{Gradi di libert\`a: le due misure}
Le due misure dei gradi di libert\`a differiscono in particolare per valori ``centrali'' di  $\lambda$.

<<cache=TRUE,out.width='0.9\\textwidth',fig.width=10,fig.height=5,fig.align='center'>>=
sim=data.frame(x=seq(0,1,length=250)) #sort(runif(150,0,1)))
sim$m=sin(2*pi*sim$x^3)
sigmaeps=0.2
sim$y=sim$m+rnorm(nrow(sim),0,sigmaeps)

x=sim$x
y=sim$y
m=sim$m
## smoothing spline
nodi=seq(0,1,length=20)[-c(1,20)]
X=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)
X=cbind(1,x,X)
D=diag(c(rep(0,2),rep(1,length(nodi))))

edf=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  sum(diag(L))
}
edfB=function(l=0.000003) {
  M=solve(t(X) %*% X + l*D ) %*% t(X)
  L=X %*% M
  2*sum(diag(L))-sum(diag(L%*%t(L)))
}
edf2=function(x) mapply(edf,l=x)
edfB2=function(x) mapply(edfB,l=x)

par(mfrow=c(1,2),mar=c(5,4,0,0))
curve(edf2(x),from=0,to=10,lwd=2,xlab=expression(lambda),ylab="df",ylim=c(2,20))
curve(edfB2(x),add=TRUE,lwd=2,col="red")
abline(h=2,lty=2)
curve(edf2(exp(x)),from=-25,to=15,xaxt="n",lwd=2,xlab=expression(lambda),ylab="df",ylim=c(2,20))
curve(edfB2(exp(x)),add=TRUE,lwd=2,col="red")
xseq=c(1.0e-9,0.000001,0.001,1,10,1000,10^6)
axis(1,at=log(xseq),labels=xseq)
abline(h=2,lty=2)
@



\end{frame}


\section[Basi]{Altre basi}

\begin{frame}[allowframebreaks=0.95]{Spline: diverse rappresentazioni}
\begin{itemize}
\item
In termini generali la rappresentazione \`e
\[ y_i = \beta_1+\sum_{k=1}^K b_kB_k(z_i) + \mbox{other variables} + \varepsilon_i\]
per un dato insieme di funzioni note $B_1,\ldots,B_K$ che \`e detta {\bf base}. 
\item
Questo fa s\`i che lo spazio di funzioni in cui cerchiamo un'approssimazione di $f$ contiene funzioni del tipo
\[ s(z) = \sum_{k=1}^K b_kB_k(z) \]
\item Ci sono diverse scelte di $B_k(z)$ che fan s\`i che si ottengano forme flessibili a partire da un numero ridotto di $B_k(z)$ (basso $K$),
\item una buona scelta di queste funzioni rende gli stimatori pi\`u efficienti.
\end{itemize}

%\break
%
%Non discutiamo oltre il problema della scelta delle basi, di seguito un tipico esempio
%
%<<echo=FALSE,out.width='0.95\\textwidth',out.height='0.4\\textwidth',fig.width=9,fig.height=4,cache=FALSE>>=
%library(splines)
%par(mfrow=c(1,2),mar=c(2,2,2,0.5),cex=1.4)
%xx=seq(0,1,length=100)
%BX=bs(xx,knots=seq(0,1,length=4))
%matplot(xx,BX,type="l",lwd=2,lty=1,main="Funzioni base")
%esempicurve=replicate(5,BX %*% rnorm(7,0,4))
%matplot(esempicurve[,1,],type="l",lwd=2,lty=1,col="blue",main="s()")
%@
%\begin{itemize}
%\item {\bf Sinistra:} funzioni base $B_k(\cdot)$;
%\item {\bf Destra:} cinque combinazioni linearei delle $B_k(\cdot)$ d sinistra.
%\end{itemize}
\end{frame}



\begin{frame}{Basi alternative}
La base a polinomi troncati \`e la pi\`u agevole da trattare dal punto di vista teorico, non \`e ottimale dal punto di vista computazionale.

\spazio

Il problema principale \`e che la matrice di disegno $X$ ha colonne fortemente correlate, il che pu\`o comportare instabilit\`a numerica.

\spazio

Esistono numerose alternative
\begin{itemize}
\item $B$-spline
\item $P$-spline
\item base radiale
\item $\ldots$
\end{itemize}

Si tenga presente che, in linea di principio, un cambio di base non comporta un cambiamento del modello.
\end{frame}

\begin{frame}[fragile,allowframebreaks=0.95]{B-spline: costruzione della base}
\begin{itemize}
\item siano $\tau_1 < \ldots < \tau_K$ i nodi interni;
\item sia $[a,b]$ il supporto di $x$ ($a<\tau_1$, $b>\tau_K$);
\item fissati, arbitrariamente, $\xi_1\leq \ldots \xi_M \leq a$ e $\chi_M\geq \ldots \geq \chi_1 \geq b$ (ad es. $\xi_i=a$ e $\nu_i=b$);
\item si ha allora la sequenza $\nu_1,\ldots,\nu_{K+2M}$.
\end{itemize}
\begin{center}
<<echo=FALSE,out.width="0.8\\textwidth",fig.width=5,fig.height=2>>=
intnodi=seq(0,1,length=7)
aggnodi=c(seq(-0.5,-0.1,length=4),seq(1.1,1.5,length=4))
par(mar=c(3,0,0,0),mfrow=c(1,1))
plot(c(aggnodi,intnodi),rep(0,length=15),xaxt="n",yaxt="n",xlab="",ylab="",bty="n",pch="|",xlim=c(-0.7,1.7))
abline(h=0)
text(intnodi,rep(-0.4,length(intnodi)),labels=c(expression(tau[1]),expression(tau[2]),"...","...","...",expression(tau[K-1]),expression(tau[K])),cex=0.7)
text(aggnodi,rep(-0.4,length(intnodi)),labels=c(expression(xi[1]),"...",expression(xi[M]),"a","b",expression(nu[1]),"...",expression(nu[M])),cex=0.7)
text(aggnodi,rep(-0.8,length(aggnodi)),labels=c(expression(kappa[1]),"...",expression(kappa[M]),"","",expression(kappa[K+M+1]),"...",expression(kappa[K+2*M])),cex=0.7)
text(intnodi,rep(-0.8,length(intnodi)),labels=c(expression(kappa[M+1]),expression(kappa[M+2]),"...","...","...",expression(kappa[M+K-1]),expression(kappa[M+K])),cex=0.7)
@
\end{center}

\break

Sia $B_{i,m}$ l'$i$-esima funzione base di ordine $m<M$, $i=1,\ldots, K+2M-m$, questa \`e definita ricorsivamente da
\[
B_{i,1}(x) = \begin{cases} 1 &\mbox{if }\nu_i\leq x < \nu_{i+1} \\ 0 & \mbox{altrimenti} \end{cases}
\]
per $i = 1,\ldots,K + 2M-1$ e
\[ 
B_{i,m}(x) = \frac{x-\nu_i}{\nu_{i+m-1}-\nu_i}B_{i,m-1}(x) +
\frac{\nu_{i+m}-x}{\nu_{i+m}-\nu_{i+1}}B_{i+1,m-1}(x)
\]
$i = 1,\ldots,K + 2M-m$.

Per $M=4$ si ottengono $K+4$ spline cubiche.


\end{frame}

\begin{frame}[fragile,allowframebreaks=0.95]{B-spline}
<<bsplineA,out.width="0.8\\textwidth", out.height="0.8\\textheight",echo=FALSE>>=
nodi=c(-1,0,0.5,1)
par(mfrow=c(1,3),mar=c(3,3,2,0.2))
x=lidar$range
Xtps=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)
library(splines)
base=bs(x,knots=nodi,degree=1,intercept=TRUE)
matplot(x,base,type="l",xlab="range (standardized)",main="1st degree B-splines",lty=1)
points(nodi,rep(0,length(nodi)),pch=20)
base=bs(x,knots=nodi,degree=2,intercept=TRUE)
matplot(x,base,type="l",xlab="range (standardized)",main="2nd degree B-splines",lty=1)
points(nodi,rep(0,length(nodi)),pch=20)
base=bs(x,knots=nodi,degree=3,intercept=TRUE)
matplot(x,base,type="l",xlab="range (standardized)",main="3rd degree B-splines",lty=1)
points(nodi,rep(0,length(nodi)),pch=20)
@
\end{frame}

\begin{frame}{B-spline: penalty matrix}

Setting $M=4$, the penalty matrix is defined as
\[ \Omega_{ij} = \int_a^b B_i''(x)B_j''(x)dx  \]
Wand and Ormerod (2009) obtained formulas for calculating $\Omega$ in practice
\[ \Omega = (\tilde{B}'')^T \mbox{diag}(\mathbf w) \tilde{B}''  \]
where 
\[ [\tilde{B}'']_{ij} = \tilde{B}_j(\tilde{x}_i) \in\mathcal M_{3(K+7)\times(K+4)}\]
\[ \tilde{x} = \left(\nu_1,\frac{\nu_1+\nu_2}{2},\nu_2,\ldots,\nu_{K+7},\frac{\nu_{K+7}+\nu_{K+8}}{2},\nu_{k+8} \right) \]
\[ {\mathbf w} = \left(\frac{1}{6}(\Delta\nu)_1,\frac{4}{6}(\Delta\nu)_1,\frac{1}{6}(\Delta\nu)_1, \ldots, \frac{1}{6}(\Delta\nu)_{K+7},\frac{4}{6}(\Delta\nu)_{K+7},\frac{1}{6}(\Delta\nu)_{K+7}\right) \]
where $(\Delta\nu)_h=\nu_{h+1}-\nu_h$.
\end{frame}

\begin{frame}{P-spline}
Le $P$-spline sono costituite da
\begin{itemize}
\item una base di $B$-spline, di solito su nodi equidistanti
\item la penalizzazione
\[ \sum_{i=1}^{K-1} (b_{i+1}-b_i)^2 \]
ovvero
\[ {\bm b}^T 
\begin{bmatrix} 
1  & -1 & 0  & \cdot & \cdot \\ 
-1 &  2 & -1 & \cdot & \cdot \\
 0 & -1 &  2 & \cdot & \cdot \\
& \cdot& \cdot& \cdot& \cdot & \cdot \\
& \cdot& \cdot& \cdot& \cdot & \cdot \\
\end{bmatrix} 
{\bm b} \]
\end{itemize}
\end{frame}


\begin{frame}{Un esempio di $P$-spline}
\begin{columns}
\column{0.6\textwidth}
<<out.width='0.95\\textwidth',fig.width=5,fig.height=5.5>>=
x=seq(0,1,length=150) #sort(runif(150,0,1)))
m=sin(2*pi*x^3)
y=m+rnorm(length(x),0,0.4)
y=y-mean(y)
par(mar=c(2,2,0,0))
layout(matrix(1:6,byrow=FALSE,nrow=3))
nodi=seq(0,1,length=10)[-c(1,10)]
p=0
Xs=bs(x,knots=nodi,degree=3)
X=outer(x,0:p,FUN=function(x,y) x^y)

fit=lm(y~Xs-1)
plot(x,y)
lines(x,cbind(Xs) %*% coef(fit),lwd=2)
## base pesata
basepesata=t(apply(Xs,1,FUN=function(x) x*coef(fit)[1:11]))
matplot(x,basepesata,type="l")
## base non pesata
matplot(x,Xs,type="l")


fit1=lm.ridge(y~Xs-1,lambda=200)
fit1
plot(x,y)
lines(x,cbind(Xs) %*% coef(fit1),lwd=2)
## base pesata
matplot(x,t(apply(Xs,1,FUN=function(x) x*coef(fit1)[1:11])),type="l",ylim=range(basepesata))
## base non pesata
matplot(x,Xs,type="l")
@
\column{0.4\textwidth}
Due $P$-spline, per quella a  sinistra
\[ \sum_{i=1}^{K-1} (b_{i+1}-b_i)^2= \Sexpr{round(sum((coef(fit)[2:11]-coef(fit)[1:10])^2),2)} \]
mentre per quella a destra
\[ \sum_{i=1}^{K-1} (b_{i+1}-b_i)^2= \Sexpr{round(sum((coef(fit1)[2:11]-coef(fit1)[1:10])^2),2)} \]
(pannello di mezzo: funzioni base moltiplicate per i rispettivi coefficienti, cio\`e la cui somma \`e la curva finale.)
\end{columns}
\end{frame}



\begin{frame}{Base radiale}
La base radiale di ordine $m$ con nodi $\nu_1,\ldots,\nu_K$ \`e
\[ 1, x, \ldots, x^m, B_k(x) = |x-\nu_k|^m \]
<<radialbasis,out.width="0.8\\textwidth", fig.width=7,fig.height=2,echo=FALSE>>=
nodi=c(-1,0,0.5,1)
par(mfrow=c(1,3),mar=c(3,3,2,0.2))
x=lidar$range
Xtps=outer(x,nodi,FUN=function(x,y) abs(x-y)^3)

base=outer(x,nodi,FUN=function(x,y) abs(x-y)^1) 
matplot(x,base,type="l",xlab="range (standardized)",main="1st degree radial basis")
points(nodi,rep(0,length(nodi)),pch=20)
base=outer(x,nodi,FUN=function(x,y) abs(x-y)^2) 
matplot(x,base,type="l",xlab="range (standardized)",main="2nd degree radial basis")
points(nodi,rep(0,length(nodi)),pch=20)
base=outer(x,nodi,FUN=function(x,y) abs(x-y)^3) 
matplot(x,base,type="l",xlab="range (standardized)",main="3rd degree radial basis")
points(nodi,rep(0,length(nodi)),pch=20)
@

Con matrice di penalizzazione
\[ [D]_{ij} = |\nu_i-\nu_j|^3 \]

\end{frame}




\section[Multiplo]{Pi\`u esplicative}

\begin{frame}{Pi\`u esplicative}
Supponiamo che le osservazioni includano pi\`u esplicative
\[ x_{i},z_{i},u_{i1},\ldots,u_{iq}  \]
Si possono ipotizzare diversi modelli
\begin{itemize}
\item componenti parametriche e una componente non parametrica
\[ y_i = {\bm\beta}^T{\bf u}_i + f(x_i) + \varepsilon_i \]
\item componenti parametriche e non parametriche
\[ y_i = {\bm\beta}^T{\bf u}_i + f_x(x_i) + f_z(z_i) + \varepsilon_i \]
\item componenti parametriche e non parametrica multipla
\[ y_i = {\bm\beta}^T{\bf u}_i + f(x_i,z_i) + \varepsilon_i \]
\end{itemize}
\end{frame}

\begin{frame}{Componenti parametriche e non parametrica}
Data per la spline $f$ la rappresentazione
\[ f(x_i) = b_0 + b_1 x_i + \sum_{j=1}^K B_j(x_i)b_{1+j} \]
con matrice di penalizzazione $S$ il modello
\[ y_i = {\bm\beta}^T{\bf u} + f(x_i) + \varepsilon_i \]
\`e stimato minimizzando la funzione obiettivo
\[ \sum_{i=1}^n(y_i - {\bm\beta}^T{\bf u}_i - f(x_i))^2 + \lambda{\bf b}^TS{\bf b} \]
in forma matriciale
\[ ||{\bm y} - H{\bm\theta}||^2 + \lambda{\bm\theta}^TS'{\bm\theta} \]
dove
\[ H=[U\; X],\;\;{\bm\theta}^T=({\bm\beta},{\bf b}),\;\;S'=? \]
\end{frame}

\begin{frame}{Componenti parametriche e non parametriche}
Siano le due spline $f_x$, $f_z$ rappresentate da
\[ f_x(x_i) = b_0 + b_1 x_i + \sum_{j=1}^{K_B} B_j(x_i)b_{1+j} \]
\[ f_z(z_i) = d_1 z_i + \sum_{j=1}^{K_D} D_j(z_i)d_{1+j} \]
con penalizzazioni $S_B, S_D$.

Si noti che la rappresentazione di $f_z$ non include l'intercetta per garantire l'identificabilit\`a del modello
\[ y_i = {\bm\beta}^T{\bf u}_i + f_x(x_i) + f_z(z_i) + \varepsilon_i \]
\end{frame}

\begin{frame}{Componenti parametriche e non parametriche}
Il modello
\[ y_i = {\bm\beta}^T{\bf u}_i + f_x(x_i) + f_z(z_i) + \varepsilon_i \]
\`e stimato minimizzando la funzione obiettivo
\[ \sum_{i=1}^n(y_i - {\bm\beta}^T{\bf u}_i - f_x(x_i)-f_z(z_i))^2 + \lambda{\bf b}^TS_x{\bf b}+ \lambda{\bf d}^TS_z{\bf d} \]
in forma matriciale
\[ ||{\bm y} - H{\bm\theta}||^2 + \lambda{\bf b}^TS_x{\bf b}+ \lambda{\bf d}^TS_z{\bf d} \]
dove
\[ H=[U\; X\; Z],\;\;{\bm\theta}^T=({\bm\beta},{\bf b},{\bf d})\]
\end{frame}

\begin{frame}{Componente non parametrica multivariata}
Il modello
\[ y_i = {\bm\beta}^T{\bf u}_i + f(x_i,z_i) + \varepsilon_i \]
richiede si definisca una spline bivariata.
\begin{itemize}
\item in linea di principio questo funziona allo stesso modo
\item {\bf problema della dimensionalit\`a}
\begin{itemize}
\item l'onere computazionale pu\`o crescere esponenzialmente con la dimensione
\item MSE: se il campione ha dimensione $n$ e la spline dimensione $d$ allora tipicamente
\[ \mbox{MSE} \approx \frac{c}{n^{4/(4+d)}} \]
cio\`e la numerosit\`a campionaria richiesta per mantenere l'MSE a un livello prespecificato cresce esponenzilmente con $d$:
\[ n \approx (c/\delta)^{d/4} \]
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Regressione non lineare in breve}
Dettagli tecnici a parte, i punti essenziali sono che
\begin{itemize}
\item 
possiamo specificare un modello per $y$ in cui
\[
\begin{matrix}
y_i &=& s(z_i) &+& \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} &+& \varepsilon_i \\
y_i &=& s(z_i) &+&  \mbox{eff. lineari altre var.}                     &+& \mbox{error} 
\end{matrix}
\]
dove $s(\cdot)$ \`e una funzione liscia.
\item
disponiamo di strumenti per stimare $s(\cdot)$ con un prefissato grado di lisciamento (fix $K$, fix $\lambda$).
\item
disponiamo di strumenti per stimare $s(\cdot)$ con un grado di lisciamento ottimale.
\end{itemize}
Con questa strategia la forma della relazione tra $y$ e $z$ pu\`o essere qualunque (o quasi) senza bisogno di fare specifiche assunzioni.
\end{frame}


\section[GAM]{Modelli additivi generalizzati}

\begin{frame}{BPD data}
La displasia broncopolmonare (BPD) \`e una malattia tipica dei bambini nati prematuramente, la cui insorgenza \`e plausibilmente legata al peso alla nascita (birthweight).

\begin{columns}
\column{0.5\textwidth}
Per 223 bambini si \`e osservato
\begin{itemize}
\item birthweight
\item insorgenza di displasia broncopolmonare (BPD)
\end{itemize}
\column{0.5\textwidth}
<<out.width='0.8\\textwidth'>>=
plot(bpd$birthweight,bpd$BPD,pch="|")
@
\end{columns}

La v. risposta \`e dicotomica $\Rightarrow$ GLM.
\end{frame}

\begin{frame}[t]{Modelli additivi generalizzati}  
La generalizzazione a risposta non gaussiana funziona in modo analogo all'estensione da ML  a GLM.
\onslide*<1>{
\[
\begin{array}{ccc}
\mbox{LM} & \rightarrow & \mbox{GLM} \\
Y_i\thicksim\mathcal N(\mu_i,\sigma^2) 
& &
Y_i\thicksim\mbox{Expon}(\theta_i,\phi_i)
\\
E(Y_i) = \eta_i
& &
g(E(Y_i)) = \eta_i
\\
\eta_i = \mu_i = {\bf x}_i {\bm\beta}
& &
\eta_i = {\bf x}_i {\bm\beta}
\\
\end{array}
\]

\[
\begin{array}{ccc}
\mbox{AM} & \rightarrow & \mbox{GAM} \\
Y_i\thicksim\mathcal N(\mu_i,\sigma^2) 
& &
Y_i\thicksim\mbox{Expon}(\theta_i,\phi_i)
\\
E(Y_i) = \eta_i
& &
g(E(Y_i)) = \eta_i
\\
\eta_i = \mu_i = f(x_i)
& &
\eta_i = f(x_i)
\\
\end{array}
\]
}
\onslide*<2>{
Data una spline
\[
f(x) = \beta_1+\beta_2 x + \sum_{j=1}^K \beta_{1+j}B_j(x)
\]
il criterio dei minimi quadrati penalizzati
\[  \sum_{i=1}^n (y_i-f(x_i))^2 + \lambda S(f(x)) \]
diventa il criterio della verosimiglianza penalizzata
\[  \ell({\bf \beta},{\bf b},\phi) - \lambda S(f(x)) \]
}
dove 
\[
 \ell({\bf \beta},{\bf b},\phi) = \sum_{i=1}^n \log(p(y_i;\theta_i))
  = \sum_{i=1}^n (y_i\theta_i-r_i(\theta_i)))/\phi + c(\phi;y_i)
\]
\end{frame}

\begin{frame}{P-IRLS}
In termini pratici si usa poi l'algoritmo IRLS usato nei GLM, con passo $k$
\begin{itemize}
\item[1] calcolare pseudodati
\[ z_i^{[k]} = g'(\mu_i^{[k]})(y_i-\mu_i^{[k]}) + \eta_i^{[k]} \]
e la matrice diagonale dei pesi
\[ W_{ii}^{[k]} = \frac{1}{V(\mu_i^{[k]})g'(\mu_i^{[k]})^2}\]
\item[2] porre
\[
{\bm\beta}^{[k+1]} \leftarrow 
\underset{{\bm{\beta}}}{\mbox{argmin}} \left\Vert\sqrt{W^{[k]}} ({\bf z}^{[k]}-X{\bm\beta})\right\Vert^2
\]
\end{itemize}

In un modello additivo la funzione obiettivo nel secondo passo \`e
\[
{\bm\beta}^{[k+1]} \leftarrow 
\underset{{\bm{\beta}}}{\mbox{argmin}} \left\Vert\sqrt{W^{[k]}} ({\bf z}^{[k]}-X{\bm\beta})\right\Vert^2
 + \lambda {\bm\beta}^TS{\bm\beta}\]
\end{frame}

\subsection{Scelta di $\lambda$ con dati non Gaussiani}

\begin{frame}{Errore di previsione  con dati non Gaussiani}
Con dati Gaussiani, la scelta di $\lambda$ \`e basata sulla stima dell'errore
\[ \sum_{i=1}^n (\hat{f}(x_i)-y_i)^2 \]
ottenuta via CV, da cui il criterio del GCV (per via della lineari\`a delle spline come lisciatori).

\spazio

La stessa strategia pu\`o essere usata ora {\bf ma} il lisciatore non \`e lineare, quindi il GCV e le derivazioni teoriche sono approssimate.

\end{frame}


\begin{frame}[t]{GCV per i GAM}
L'obiettivo nella stima GAM pu\`o essere scritto
\onslide*<1>{in termini della devianza 
\[ D({\bm\beta})=2(\ell({\bm\beta}_{\mbox{max}})-\ell({\bm\beta})) \]
}
come
\[ D({\bm\beta})+\sum_{j=1}^d\lambda_j{\bm\beta}^TS_j{\bm\beta} \]
la cui approssimazione quadratica \`e, per un $\lambda$ fissato,
\[
\left\Vert\sqrt{W} ({\bf z}-X{\bm\beta})\right\Vert^2 + \sum_{j=1}^d\lambda_j{\bm\beta}^TS_j{\bm\beta}
\]
\onslide*<2->{
da cui il GCV (valido localmente)
\onslide<3>{ e quindi il GCV applicabile globalmente}
\[
\frac{n\left\Vert\sqrt{W} ({\bf z}-X{\bm\beta})\right\Vert^2}{n-\mbox{tr}(L)}
\onslide*<3>{\;\;\;\;\rightarrow\;\;\;\; \frac{nD({\bm\hat\beta})}{n-\mbox{tr}(L)}}
\]
}
\end{frame}


\begin{frame}{UBRE}
Ricordando che la CV nasce dal rischio di previsione, $E((m(x)-\hat{m}(x))^2)$, cio\`e
\[ E\left(\left\Vert{\bm\mu}-L{\bf y}\right\Vert^2\right)
=\frac{1}{n}E\left(\left\Vert{\bf y}-L{\bf y}\right\Vert^2\right) - \sigma^2 + 2\mbox{tr}(L)\frac{\sigma^2}{n}
\]
dove il parametro di scala $\sigma^2$ \`e noto si pu\`o impiegare l'UBRE (Unbiased Risk Estimator)
\[ \frac{1}{n}\left\Vert{\bf y}-L{\bf y}\right\Vert^2 - \sigma^2 + 2\mbox{tr}(L)\frac{\sigma^2}{n}
\]

\spazio

L'UBRE \`e appropriato per i GAM in cui il parametro di scala \`e noto.
\end{frame}

\begin{frame}{Calcolo dell'UBRE}
A partire da
\[ D({\bm\beta})+\sum_{j=1}^d\lambda_j{\bm\beta}^TS_j{\bm\beta} \]
e dall'approssimazione quadratica
\[
\left\Vert\sqrt{W} ({\bf z}-X{\bm\beta})\right\Vert^2 + \sum_{j=1}^d\lambda_j{\bm\beta}^TS_j{\bm\beta}
\]
si ottiene il criterio UBRE
\[
\frac{1}{n}\left\Vert\sqrt{W} ({\bf z}-X{\bm\beta})\right\Vert^2 -\sigma^2 + \frac{2\sigma^2}{n}\mbox{tr}(L)
\]
\[
\frac{1}{n}D({\bm\hat\beta}) -\sigma^2 + \frac{2\sigma^2}{n}\mbox{tr}(L)
\]
\end{frame}

\begin{frame}[fragile]{BPD data - modello stimato}

<<eval=FALSE,echo=TRUE,out.width='0.8\\textwidth'>>=
fit=gam(BPD~s(birthweight),
        data=bpd,
        family=binomial)
plot(fit)
@

\begin{center}
<<echo=FALSE,out.width='0.7\\textwidth'>>=
fit=gam(BPD~s(birthweight),data=bpd,family=binomial)
plot(fit)
@
\end{center}

\end{frame}


\begin{frame}[fragile]{BPD data - modello stimato}

\begin{center}
<<eval=TRUE,echo=2:4,out.width='0.7\\textwidth'>>=
par(mar=c(5,4,0,0))
curve(predict(fit,newdata=data.frame(birthweight=x)),
      from=400,to=1800,ylab="")
rug(bpd$birthweight[bpd$BPD==0],side=1)
rug(bpd$birthweight[bpd$BPD==1],side=3)
@
\end{center}

\end{frame}

\begin{frame}[fragile]{BPD data - modello stimato}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:4,out.width='0.7\\textwidth'>>=
par(mar=c(5,4,0,0))
curve(predict(fit,newdata=data.frame(birthweight=x),
              type="response"),
      from=400,to=1800,ylab="")
rug(bpd$birthweight[bpd$BPD==0],side=1)
rug(bpd$birthweight[bpd$BPD==1],side=3)
@
\end{scriptsize}
\end{center}

\end{frame}

\begin{frame}[fragile]{BPD data - modello stimato}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:4,out.width='0.7\\textwidth'>>=
par(mar=c(5,4,0,0))
xx=seq(450,1730,length=100)
pr=predict(fit,newdata=data.frame(birthweight=xx),
              type="response",se.fit=TRUE)
matplot(xx,cbind(pr$fit-2*pr$se.fit,pr$fit,pr$fit+2*pr$se.fit),
        type="l",lty=c(2,1,2),col="black")
rug(bpd$birthweight[bpd$BPD==0],side=1)
rug(bpd$birthweight[bpd$BPD==1],side=3)
@
\end{scriptsize}
\end{center}

\end{frame}

\begin{frame}[fragile]{BPD data - modello stimato}

\begin{center}
\begin{scriptsize}
<<eval=TRUE,echo=2:4,out.width='0.7\\textwidth'>>=
par(mar=c(5,4,0,0))
xx=seq(450,1730,length=100)
pr=predict(fit,newdata=data.frame(birthweight=xx),
              se.fit=TRUE)
matplot(xx,cbind(pr$fit-2*pr$se.fit,pr$fit,pr$fit+2*pr$se.fit),
        type="l",lty=c(2,1,2),col="black")
rug(bpd$birthweight[bpd$BPD==0],side=1)
rug(bpd$birthweight[bpd$BPD==1],side=3)
@
\end{scriptsize}
\end{center}

\end{frame}



\end{document}





