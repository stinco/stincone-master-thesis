---
output:
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::html_document2: default
documentclass: book
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
library(knitr)
```

<!-- <!-- Needed for leaving space to the quote, * is for no indentation after title --> -->

<!-- \titlespacing*{\chapter}{0pt}{80px}{35pt} -->

# **Statistical models for Non Life Insurance Pricing** {#chap:models}
\minitoc  <!--this will include a mini table of contents-->

\chaptermark{Statistical models for Non Life Insurance Pricing}

In this chapter we are going to describe some of the most widespread models for technical pricing. For each model we are going to describe its benefits and drawbacks and in section \@ref(chap:actuary-importance) we will compare them by discussing how they fit the pricing needs.


## Statistical Models

In this section we will start by describing the Generalized Linear Model (GLM), that is the most employed model in technical pricing, to then present some of its advancements: the Elastic Net and the Generalized Additive Model (GAM). After this description we will also present the Gradient Boosting Machine (GBM), that is one of the most effective general purpose machine learning models. This allows us to have a comparison between GLM based models and general purpose machine learning models.


### GLM


#### Linear Exponential Family

One of the GLM assumptions is that the response variables belong to a _Linear Exponential Family_. In this section we are going to explain what it is and which distributions fit its definition.

```{definition, linear-exp-family, name = "Linear Exponential Family"}
A Linear Exponential Family $\mathcal{F}$ is a parametrical family of probability distributions with density function (or probability function in the discrete case) that can be expressed in the form:
$$
f(y; \theta, \lambda) = \exp{\left\{ \frac{y\theta-b(\theta)}{\lambda} \right\}} c(y,\lambda), \quad y\in \mathcal{Y}\subseteq\mathbb{R}
$$
where:

\begin{itemize}
\item $\theta\in\Theta\subseteq\mathbb{R}$ is called \textit{canonical parameter};
\item $\lambda\in\Lambda\subseteq]0, +\infty[$ is called \textit{dispersion parameter};
\item $b: \Theta \rightarrow \mathbb{R}$ is a real function called \textit{cumulant function};
\item $c: (\mathcal{Y}, \Lambda) \rightarrow [0, +\infty[$ is a real function;
\item $\Theta$ is a non degenerate interval, i.e. $\text{int}\Theta$ is not empty.
\end{itemize}

```

An exponential family $\mathcal{F}$ is characterized by the elements $\left( \Theta, b(\cdot), \Lambda, c(\cdot, \cdot) \right)$. By properly choosing the sets $\Theta, \Lambda$ and the functions $b(\cdot), c(\cdot, \cdot)$, it is possible to obtain many useful families.

It can be easily shown that the families Normal, Poisson, Gamma and Binomial are exponential families. In table \@ref(tab:exp-families) the characterizations for these exponential families are reported.

```{r, exp-families-table, echo = FALSE}

table <- tibble(
  Distribution = linebreak(c("Normal", "Poisson", "Gamma", "Scaled\nBinomial"),
                           align = "l"),
  Notation = linebreak(c("$N(\\mu, \\sigma^2)$,\n$\\mu\\in\\mathbb{R}, \\ \\sigma0$",
                         "$Poisson(\\mu)$,\n$\\mu0$",
                         "$Gamma(\\alpha, \\mu)$,\n$\\alpha0, \\ \\mu0$",
                         "$Binom(n, p)/n$,\n$n\\in\\mathbb{N}, \\ p\\in]0,1[$"),
                       align = "c"),
  `$\\Theta$` = c("$\\mathbb{R}$",
                  "$\\mathbb{R}$",
                  "$]-\\infty, 0[$",
                  "$\\mathbb{R}$"),
  `$\\theta$` = c("$\\mu$",
                  "$\\log{(\\mu)}$",
                  "$-\\frac{1}{\\mu}$",
                  "$\\log{\\frac{p}{1-p}}$"),
  `$\\Lambda$` = c("$]0, +\\infty[$",
                  "$\\left\\{1\\right\\}$",
                  "$]0,+\\infty[$",
                  "$\\left\\{\\frac{1}{n}\\right\\}$"),
  `$\\lambda$` = c("$\\sigma^2$",
                  "$1$",
                  "$\\frac{1}{\\alpha}$",
                  "$\\frac{1}{n}$"),
  `$b(\\theta)$` = c("$\\frac{\\theta^2}{2}$",
                  "$e^{\\theta}$",
                  "$-\\log{\\left(-\\theta\\right)}$",
                  "$\\log\\left(1+e^{\\theta}\\right)$"),
)


# For compatibility with HTML
if(!knitr::is_latex_output()){
  
  table <- table %>% 
    mutate(
      Distribution = Distribution %>% 
        str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
        str_replace_all("\\\\\\\\", "<br>"),
      Notation = Notation %>% 
        str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
        str_replace_all("\\\\\\\\", "<br>"),
    )
  
  names(table) <- names(table) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}


table %>% 
  kable(
    # format = "latex",
    booktabs = T,
    align = "lcccccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Some Linear Exponential Families.",
    label = "exp-families",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
  
```

The distributions that belong to an exponential family have many useful properties. For example they are provided with all the moments and their moments can be obtained using the derivatives of the cumulative function $b(\cdot)$. If $Y$ is a random variable with distribution belonging to an exponential family $\mathcal{F}$ with parameters $\theta, \lambda$, its first two moments are:
```{=latex}
\begin{align}
\label{eq:exp-fam-expected-value}
E(Y)   & = b'(\theta) \\
Var(Y) & = \lambda b''(\theta)
\end{align}
```

As, within a specified family, the parameters $\theta$ and $\lambda$ determine a distribution, in practical problems the object of estimation will be the couple $(\theta, \lambda)$. In many problems it is natural to consider distributions from a linear exponential family where the dispersion parameter can be expressed as $\lambda = \frac{\phi}{\omega}$, where $\omega>0$ is a known _weight_ and $\phi>0$ is a parameter that we will keep calling _dispersion parameter_. In this case, the density of probability function depends on the parameters $\theta$ and $\phi$ and will be expressed as:
$$
f(y; \theta, \phi, \omega) = \exp{\left\{ \frac{\omega}{\phi} \left[y\theta - b(\theta) \right] \right\}} c(y, \phi, \omega), \quad y\in \mathcal{Y}\subseteq\mathbb{R}
$$

In this case the parameters $\theta$ and $\phi$ will be object of estimation, while $\omega$ is an already known value. As we will see later, this representation allows us to consider as known weights:

* the exposure $v$ in the Poisson distribution;
* the number of trials $n$ in the Binomial distribution.



#### Model assumptions

Let's assume that, for $n$ statistical units, the observations $\mathcal{D} = \left\{ (\boldsymbol{x}_1, \omega_1, y_1), \dots,  (\boldsymbol{x}_n, \omega_n, y_n) \right\}$ are available, where $\boldsymbol{x}_i$ is a vector of explanatory variables determinations, $\omega_i$ is a known weight and $y_i$ is the response variable determination. $\boldsymbol{x}_i, \omega_i, y_i$ are all real numbers. The vector $\boldsymbol{y} = (y_1, \dots, y_n)^t$ is considered a determination of the response random vector $\boldsymbol{Y} = (Y_1, \dots, Y_n)^t$.

In GLM we assume that:

1. The response variables $Y_1, \dots, Y_n$ are stochastically independent and with probability distribution belonging to a same linear exponential family; i.e. the probability distribution of $Y_i$ has density function (or probability function in the discrete case) that can be expressed as:
$$
f(y_i; \theta_i, \phi, \omega_i) = \exp{\left\{ \frac{\omega_i}{\phi} \left[y_i\theta_i - b(\theta_i) \right] \right\}} c(y_i, \phi, \omega_i), \quad y_i\in \mathcal{Y}\subseteq\mathbb{R}
$$
We highlight that only $\theta_i$ and $\omega_i$ depend on $i$, while the dispersion parameter $\phi$ is the same for all the observations.
2. The explanatory variables determinations vector $\boldsymbol{x}_i = \left(1, x_{i1}, \dots, x_{ip} \right)^t$ affects the probability distribution of the response variable $Y_i$ by the linear predictor:
$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}
$$
that is a linear function of the regression parameters $\boldsymbol{\beta} = \left( \beta_0, \beta_1, \dots, \beta_p \right)$.
3. The linear predictor $\eta_i$ is linked to the expected value of the response variable $\mu_i = E(Y_i)$ by the following relation:
$$
g(\mu_i) = \eta_i = \boldsymbol{x}_i^t \boldsymbol{\beta}
$$
where $g:\mathbb{R}\rightarrow\mathbb{R}$ is a monotonic function with continuous first and second derivatives. $g(\cdot)$ is called _link function_.

Often, the assumption 1 is called stochastic assumption, while the 2 and 3 are called structural assumptions.

Let's indicate with $\boldsymbol{X}$ the design matrix, i.e. the matrix in which each row $\boldsymbol{x}_{i\cdot}$ represents the vector of the explanatory variables for the observation $i$ and each column $\boldsymbol{x}_{\cdot j}$ represents the vector of the observations for the explanatory variable $j$. The design matrix is represented in figure \@ref(fig:design-matrix). The matrix starts with a column of 1s, that is used to model the intercept. Thus, it is a matrix $n\times(p+1)$. We assume, as it is common in actuarial datasets, that $n>p+1$.


```{tikz, design-matrix, fig.cap = "Design Matrix $\\boldsymbol{X}$.", fig.ext = 'pdf', cache = TRUE, echo = FALSE, fig.align = 'center'}
\usetikzlibrary{arrows.meta, bending, matrix, positioning}
\pgfdeclarelayer{bg}    % declare background layer
\pgfsetlayers{bg,main}  % set the order of the layers (main is the standard layer)
%\usepackage{xcolor}
\definecolor{col1}{HTML}{F8766D}
\definecolor{col2}{HTML}{00BFC4}

\begin{center}
\begin{tikzpicture}[node distance = 1mm and 0mm, baseline]

% Draw matrix
\matrix (M1) [matrix of nodes,{left delimiter=[},{right delimiter=]}]
{
    $1$      & $x_{11}$ & $\dots$  & $x_{1j}$ & $\dots$  & $x_{1p}$ \\
    $1$      & $x_{21}$ & $\dots$  & $x_{2j}$ & $\dots$  & $x_{2p}$ \\
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
    $1$      & $x_{i1}$ & $\dots$  & $x_{ij}$ & $\dots$  & $x_{ip}$ \\
    $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
    $1$      & $x_{n1}$ & $\dots$  & $x_{nj}$ & $\dots$  & $x_{np}$ \\
};

% Draw red vertical rectangle
\begin{pgfonlayer}{bg}    % select the background layer
%\draw[red!60, very thick, fill = red!60, fill opacity = 0.2] 
%        (M1-1-4.north west) -| (M1-6-4.south east) -| (M1-1-4.north west);
\draw[col1, very thick, fill = col1, fill opacity = 0.2] 
        (M1-1-4.north west) -| (M1-6-4.south east) -| (M1-1-4.north west);
\end{pgfonlayer}
\node (ev) [below = 1cm of M1-6-4.south, align = center] {$\boldsymbol{x}_{\cdot j}$\\explanatory\\variable $j$};
\draw[col1, very thick,shorten >=1mm, -{Stealth[bend]}] 
        (ev.north) to (M1-6-4.south);

% Draw blue horizontal rectangle
\begin{pgfonlayer}{bg}    % select the background layer
%\draw[blue!60, very thick, fill = blue!60, fill opacity = 0.2]
%        (M1-4-1.north west) -| (M1-4-6.south east) -| (M1-4-1.north west);
\draw[col2, very thick, fill = col2, fill opacity = 0.2]
        (M1-4-1.north west) -| (M1-4-6.south east) -| (M1-4-1.north west);
\end{pgfonlayer}
\node (obs) [right = 1cm of M1-4-6.east, align = center] {$\boldsymbol{x}_{i\cdot}$\\observation $i$};
\draw[col2, very thick,shorten >=1mm, -{Stealth[bend]}] 
        (obs.west) to (M1-4-6.east);

\end{tikzpicture}
\end{center}

```


We can then express the GLM structural assumptions in a matrix form as:
$$
\boldsymbol{g}(\boldsymbol{\mu}) = \boldsymbol{X} \boldsymbol{\beta}
$$
where $\boldsymbol{g}(\cdot)$ must be intended as the vectorial function that links every $\mu_i$ to $g(\mu_i)$.
$$
\begin{array}{cccc}
\boldsymbol{g}: & \mathbb{R}^n & \longrightarrow & \mathbb{R}^n \\
                & \left(
                    \begin{matrix} \mu_1  \\ \vdots \\ \mu_n \end{matrix}
                  \right)
                  & \longmapsto & 
                  \left(
                    \begin{matrix} g(\mu_1)  \\ \vdots \\ g(\mu_n) \end{matrix}
                  \right)
\end{array}
$$

We assume the design matrix to be a full rank matrix, i.e. $\text{rank}(\boldsymbol{X}) = p+1$. This assumption corresponds to assuming that the $\boldsymbol{X}$ columns are linearly independent.

The function $g(\cdot)$ can be chosen as any monotonic function with continuous first and second derivatives. Given a family $\mathcal{F}$ a common choice is its canonical link function that is defined as:
$$
g(\mu) = b'^{-1}(\mu)
$$
From \@ref(eq:exp-fam-expected-value) we we obtains that, as $\mu = b'(\theta)$, choosing the canonical function corresponds to using $\theta$ as the linear predictor:
$$
\eta = g(\mu) = b'^{-1}(\mu) = \theta
$$

In table \@ref(tab:can-link-fun) the canonical link functions for the families mentioned in \@ref(tab:exp-families) are reported.

```{r, can-link-fun, echo = FALSE}
table <- tibble(
  Distribution = linebreak(c("Normal", "Poisson", "Gamma", "Scaled\nBinomial"),
                           align = "l"),
  `\\makecell[c]{Cumulant function\\\\$b(\\theta)$}` = c("$\\frac{\\theta^2}{2}$",
                     "$e^{\\theta}$",
                     "$-\\log{\\left(-\\theta\\right)}$",
                     "$\\log\\left(1+e^{\\theta}\\right)$"),
  `\\makecell[c]{Derivative\\\\$b'(\\theta)$}` = c("$\\theta$",
                      "$e^\\theta$",
                      "$-\\frac{1}{\\theta}$",
                      "$\\frac{e^{\\theta}}{1 + e^{\\theta}}$"),
  `\\makecell[c]{Canonical link function\\\\$g(\\mu)=b'^{-1}(\\mu)$}` = c("$\\mu$",
                                  "$\\log{(\\mu)}$",
                                  "$-\\frac{1}{\\mu}$",
                                  "$\\log{\\left( \\frac{p}{1-p} \\right)}$"),
)

# For compatibility with HTML
if(!knitr::is_latex_output()){
  
  table <- table %>% 
    mutate(
      Distribution = Distribution %>% 
        str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
        str_replace_all("\\\\\\\\", "<br>")
    )
  
  names(table) <- names(table) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}

table %>% 
  kable(
    # format = "latex",
    booktabs = T,
    align = "lccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Canonical link functions.",
    label = "can-link-fun",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
```

In the Gamma case, its canonical function $g(\mu)=-\frac{1}{\mu}$ has the drawback that it links the expected values $\mu\in]0,+\infty[$ to $\eta\in]-\infty, 0[$. This would require some constraints on $\boldsymbol{\beta}$ because $\eta=\boldsymbol{x}^t\boldsymbol{\beta}$ would have to be $<0$. For this reason, it is preferred to use $g(\mu) = log(\mu)$ that maps $]0, +\infty[$ to $\mathbb{R}$.


#### Model fitting

The model depends on the parameters $\left(\boldsymbol{\beta}, \phi\right)$. Indeed, the parameters $\theta_i$ can be obtained by $\boldsymbol{\beta}$ as:
$$
\theta_i = b'^{-1}(\mu_i) = b'^{-1}(g^{-1}(\eta_i)) = b'^{-1}\left(g^{-1}\left(\boldsymbol{x}_i^t\boldsymbol{\beta}\right)\right)
$$

Therefore, fitting the model corresponds to estimating $\left(\boldsymbol{\beta}, \phi\right)$. The technique used in GLM is the _maximum likelihood_. Let's indicate with $L\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)$ the model likelihood. We remind that the likelihood is a function of the parameters that maps $\left(\boldsymbol{\beta}, \phi\right)$ to the density (or probability in the discrete case) of the observed values $\boldsymbol{y}$ conditioned to the parameters $\left(\boldsymbol{\beta}, \phi\right)$
$$
\begin{array}{cccc}
L: & \mathbb{R}^{p+1} \times \Lambda & \longrightarrow & [0, +\infty[ \\
   & \left(\boldsymbol{\beta}, \phi\right) & \longmapsto & f_{\boldsymbol{Y}}(\boldsymbol{y}; \boldsymbol{\theta}, \phi)
\end{array}
$$

The maximum likelihood estimates are the values $\left(\boldsymbol{\beta}, \phi\right)$ that maximize $L\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)$. In practice, $\boldsymbol{\beta}$ are the parameters of interest, while $\phi$ is considered as a disturbance parameter. It is also possible to show that conditioned to any $\phi$, the value for $\boldsymbol{\beta}$ that maximizes $L(\cdot, \cdot)$ does not depend on $\phi$. Therefore, $\boldsymbol{\beta}$ and $\phi$ can be estimated separately.

Let's indicate with $\tilde{\boldsymbol{\beta}}$ the maximum likelihood estimator for $\boldsymbol{\beta}$. Its determination $\hat{\boldsymbol{\beta}}$ is defined as:
```{=latex}
\begin{equation}
\label{eq:max-lik-est}
\hat{\boldsymbol{\beta}} = \argmax_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}{L\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)}
\end{equation}
```


Finding the values $\hat{\boldsymbol{\beta}}$ that maximize the likelihood corresponds to finding the values that maximize the log-likelihood $l\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right) = \log{\left(L\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)\right)}$. For the independence hypothesis on $Y_1, \dots, Y_n$ we get:
```{=latex}
\begin{align}
\nonumber
l\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right) & =
\log{\left(L\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)\right)}
\\ \nonumber & =
\log{\left(\prod_{i=1}^{n}{\exp{\left\{ \frac{\omega_i}{\phi} \left[y_i\theta_i - b(\theta_i) \right] \right\}} c(y_i, \phi, \omega_i)}\right)}
\\ \label{eq:log-like} & =
\sum_{i=1}^{n}{
\left\{
\frac{\omega_i}{\phi} \left[y_i\theta_i - b(\theta_i) \right] + \log{\left(c(y_i, \phi, \omega_i)\right)}
\right\}
}
\\ \nonumber & =
\sum_{i=1}^{n}{l_i\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)}
\end{align}
```

The maximum value of $l\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)$ can be obtained by imposing all its partial derivatives equal to $0$:
$$
\frac{\partial l\left(\boldsymbol{\beta}, \phi; \boldsymbol{y}\right)}
{\partial\beta_j}
= 0, \quad \forall j\in\{0,1,\dots,p\}
$$

These equations can be solved with numerical methods, such as Newton-Raphson algorithm or its variant Fisher scoring. It is possible to show that the Fisher scoring algorithm corresponds to iteratively solving a weighted least squares optimization problem.

A statistic that can be used to measures the goodness of fit of a model is the _Deviance_. It can be used by comparing the current model log-likelihood $l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)$ with the _saturated model_ log-likelihood $l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)$. The saturated model is the model with $n$ parameter, so a model where the expected values of the response variables $\mu_1, \dots, \mu_n$ are estimated with their observed values $y_1, \dots, y_n$. It is possible to show that $l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right) \ge l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)$. The closer $l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)$ is to $l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)$, the better the current model fitting is.

```{definition, deviance-def, name = "Deviance"}
Given $l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)$ the log-likelihood of the current model and $l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)$ the log-likelihood of the saturated model, the \textit{Scaled Deviance} of the current model is defined as:
$$
S(\hat{\boldsymbol{\beta}}, \phi, \boldsymbol{y}) =
-2\left(
l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)
- l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)
\right)
$$
The \textit{Deviance} of the current model is defined as:
$$
D(\hat{\boldsymbol{\beta}}, \boldsymbol{y}) =
\phi \, S(\hat{\boldsymbol{\beta}}, \phi, \boldsymbol{y})
$$

```

In deviance notation $D(\hat{\boldsymbol{\beta}}, \boldsymbol{y})$, the parameter $\phi$ is not reported because the deviance does not depend on $\phi$. Indeed, from \@ref(eq:log-like) we get:
```{=latex}
\begin{align*}
S(\hat{\boldsymbol{\beta}}, \phi, \boldsymbol{y})
& =
-2\left(
l\left(\hat{\boldsymbol{\beta}}, \phi; \boldsymbol{y}\right)
- l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)
\right)
\\ & =
-2\left(
\sum_{i=1}^{n}{
\left\{
\frac{\omega_i}{\phi} \left[y_i\hat{\theta}_i - b(\hat{\theta}_i) \right] + \log{\left(c(y_i, \phi, \omega_i)\right)}
\right\}
}
\right.
\\ & \qquad \qquad -
\left.
\sum_{i=1}^{n}{
\left\{
\frac{\omega_i}{\phi} \left[y_i\theta_i^* - b(\theta_i^*) \right] + \log{\left(c(y_i, \phi, \omega_i)\right)}
\right\}
}
\right)
\\ & =
-2\left(
\sum_{i=1}^{n}{
\frac{\omega_i}{\phi}
\left\{
\left[y_i\hat{\theta}_i - b(\hat{\theta}_i) \right]
- \left[y_i\theta_i^* - b(\theta_i^*) \right]
\right\}
}
\right)
%
\\[12pt]
%
D(\hat{\boldsymbol{\beta}}, \boldsymbol{y})
& =
-2\left(
\sum_{i=1}^{n}{
\omega_i
\left\{
\left[y_i\hat{\theta}_i - b(\hat{\theta}_i) \right]
- \left[y_i\theta_i^* - b(\theta_i^*) \right]
\right\}
}
\right)
\end{align*}
```

In table \@ref(tab:deviance) the deviances for the families mentioned in \@ref(tab:exp-families) are reported.

```{r, deviance-exp-fam, echo = FALSE}
table <- tibble(
  Distribution = linebreak(c("Normal", "Poisson", "Gamma", "Scaled\nBinomial"),
                           align = "l"),
  `Deviance $D(\\hat{\\boldsymbol{\\beta}}, \\boldsymbol{y})$` = c(
    "$\\sum_{i=1}^{n}{\\left( y_i - \\hat{\\mu}_i \\right)^2}$",
    "$2\\,\\sum_{i=1}^{n}{\\left\\{ y_i \\log{\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right)} - \\left( y_i - \\hat{\\mu}_i \\right) \\right\\}}$",
    "$2\\,\\sum_{i=1}^{n}{\\left\\{ - \\log{\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right)} + \\frac{ y_i - \\hat{\\mu}_i }{\\hat{\\mu}_i} \\right\\}}$",
    "$2\\,\\sum_{i=1}^{n}{\\left\\{ y_i \\log{\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right)}+ \\left(1-y_i\\right) \\log{\\left(\\frac{1-y_i}{1-\\hat{\\mu}_i}\\right)} \\right\\}}$"
  ))

# For compatibility with HTML
if(!knitr::is_latex_output()){
  
  table <- table %>% 
    mutate(
      Distribution = Distribution %>% 
        str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
        str_replace_all("\\\\\\\\", "<br>")
    )
  
  names(table) <- names(table) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}

table %>% 
  kable(
    # format = "latex",
    booktabs = T,
    align = "lc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Deviance for Linear Exponential Families",
    label = "deviance",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
```

As $l_{S}\left(\boldsymbol{\beta}^*, \phi; \boldsymbol{y}\right)$ does not depends on $\hat{\boldsymbol{\beta}}$, maximizing the likelihood in equation \@ref(eq:max-lik-est) is the same as minimizing the deviance, that can be seen as a _Loss Function_.

<!--
* Maximum likelihood
* Iteratively reweighted least squares
  + Newton-Raphson, Fisher scoring
* Modello saturo
* Deviance
  + Loss function. Optimization process

-->




#### Variable effects

As we mentioned in \@ref(chap:pricing-variables-encoding) the explanatory variables can be _quantitative_ or _qualitative_. In GLM the assumption is that the variables effect on the linear predictor $\eta$ is linear.

In figure \@ref(fig:expl-var-types) the effects of quantitative and qualitative variables are shown for a GLM with normal response and identity link.

(ref:expl-var-types-caption-latex) Explanatory variables types.

(ref:expl-var-types-caption-gitbook) Explanatory variables types, quantitative (top-left), qualitative (top-right), quantitative and qualitative without interaction (bottom-left) and quantitative and qualitative without interaction (bottom-right).

```{r, plot-quant-qual-build, echo = FALSE}

set.seed(42)

col1 <- hue_pal()(2)[1]
col2 <- hue_pal()(2)[2]

line_size <- 2

n <- 200
b0 <- 1
b1 <- 2
b2 <- 1
b12 <- -1
sigma <- .1

df1 <- tibble(x = runif(n = n, min = 0, max = 1)) %>% 
  mutate(mu = b0 + b1 * x)
df1$y <- rnorm(n = n, mean = df1$mu, sd = sigma)

df2 <- tibble(x = c(rep(0, times = n/2), rep(1, times = n/2))) %>% 
  mutate(mu = b0 + b2 * x)
df2$y <- rnorm(n = n, mean = df2$mu, sd = sigma)

df3 <- tibble(x1 = runif(n = 2*n, min = 0, max = 1),
              x2 = c(rep(0, times = n), rep(1, times = n))) %>% 
  mutate(mu = b0 + b1 * x1 + b2*x2)
df3$y <- rnorm(n = 2*n, mean = df3$mu, sd = sigma)

df4 <- tibble(x1 = runif(n = 2*n, min = 0, max = 1),
              x2 = c(rep(0, times = n), rep(1, times = n))) %>% 
  mutate(mu = b0 + b1 * x1 + b2 * x2 + b12 * x1 * x2)
df4$y <- rnorm(n = 2*n, mean = df4$mu, sd = sigma)


p_quant_qual_1 <- df1 %>% 
  ggplot(aes(x = x, y = y)) +
  geom_abline(
    intercept = b0,
    slope = b1,
    color = col1,
    size = line_size
  ) +
  geom_point(alpha = .5) +
  # labs(title = "Quantitative variable") +
  scale_x_continuous(limits = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )
  

p_quant_qual_2 <- df2 %>% 
  mutate(x = 1/2 * x + 1/4) %>% 
  ggplot(aes(x = x, y = y)) +
  # geom_abline(
  #   intercept = b0,
  #   slope = b2,
  #   color = col1
  # ) +
  geom_point(
    # data = tibble(x = c(0, 1), y = c(b0, b0 + b2)),
    data = tibble(x = c(1/4, 3/4), y = c(b0, b0 + b2)),
    mapping = aes(x = x, y = y),
    color = col1,
    size = 5#,
    # alpha = .8
  ) +
  geom_point(alpha = .5) +
  geom_point(
    # data = tibble(x = c(0, 1), y = c(b0, b0 + b2)),
    data = tibble(x = c(1/4, 3/4), y = c(b0, b0 + b2)),
    mapping = aes(x = x, y = y),
    color = col1,
    size = 5,
    alpha = .8
  ) +
  # labs(title = "Qualitative variable") +
  scale_x_continuous(limits = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )


p_quant_qual_3 <- df3 %>% 
  mutate(x2 = factor(x2)) %>% 
  ggplot(aes(x = x1, color = x2, y = y)) +
  geom_abline(
    intercept = b0,
    slope = b1,
    color = col1,
    size = line_size
  ) +
  geom_abline(
    intercept = b0 + b2,
    slope = b1,
    color = col2,
    size = line_size
  ) +
  geom_point(alpha = .5) +
  scale_color_manual(values = c(col1, col2)) +
  # labs(title = "Quantitative and qualitative variable without interaction") +
  scale_x_continuous(limits = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )


p_quant_qual_4 <- df4 %>% 
  mutate(x2 = factor(x2)) %>% 
  ggplot(aes(x = x1, color = x2, y = y)) +
  geom_abline(
    intercept = b0,
    slope = b1,
    color = col1,
    size = line_size
  ) +
  geom_abline(
    intercept = b0 + b2,
    slope = b1 + b12,
    color = col2,
    size = line_size
  ) +
  geom_point(alpha = .5) +
  scale_color_manual(values = c(col1, col2)) +
  # labs(title = "Quantitative and qualitative variable with interaction") +
  scale_x_continuous(limits = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )
```

```{r, plot-quant-qual-print, out.width = "50%", fig.align='center', fig.cap=ifelse(knitr::is_html_output(), "(ref:expl-var-types-caption-gitbook)", "(ref:expl-var-types-caption-latex)"), label="expl-var-types", echo=FALSE, fig.ncol=2, fig.subcap=c('Quantitative', 'Qualitative', 'Quantitative and qualitative \\\\ without interaction', 'Quantitative and qualitative \\\\ with interaction')}

# To align the plots in a subfigure environment
plot_grid_split <- function(..., align = "hv", axis = "tblr"){
  aligned_plots <- cowplot::align_plots(..., align = align, axis = axis)
  plots <- lapply(1:length(aligned_plots), function(x){
    cowplot::ggdraw(aligned_plots[[x]])
  })
  invisible(capture.output(plots))
}

plot_grid_split(p_quant_qual_1, p_quant_qual_2, p_quant_qual_3, p_quant_qual_4)

```

In top-left panel, we see the effect of the quantitative $x$ in the model $\mu_i = \beta_0 + \beta_1 x_i$. As we can see it is a straight line. The coefficient $\beta_1$ represents the slope of the line, thus $\beta_1>0$ means that $x$ and $Y$ are positively correlated, while $\beta_1<0$ means that $x$ and $Y$ are negatively correlated. For example, if $x$ is the power of the vehicle and $Y$ the yearly number of claims, $\beta_1>0$ means that the more powerful the vehicle is, the more claims the policyholder will experience on average.

In top-right panel, we see the effect of a qualitative binary variable $x$ in the model $\mu_i = \beta_0 + \beta_1 x_i$. The variable is encoded with values $0$ and $1$, so $\beta_1$ represents the effect of the modality $x=1$. In general, for a qualitative variable with $K$ modalities we will have $K-1$ dummy variables $x'_1, \dots, x'_{K-1}$ and the model will be $\mu_i = \beta_0 + \beta_1 x'_{i1} + \beta_2 x'_{i2} + \dots +  + \beta_{K-1} x'_{i, K-1}$. Thus, the $\beta_j$ coefficient represents the relative effect of the modality $j$ compared to the base level modality, that is the one not explicitly included in the dummy encoding. For example, if $x$ is the vehicle make, $Y$ the yearly number of claims, the base level for $x$ is 'Fiat' and the $j$^th^ modality is 'Ferrari', $\beta_j>0$ means that Ferrari cars on average experience more claims that Fiat cars.

In general, in a multivariate model, the coefficient $\beta_j$ represents the effect of the variable $j$ given all the others. For example, in the example of Fiat and Ferrari cars, if in the model there is also the variable 'vehicle power', the coefficient $\beta_j$ corresponding to the modality 'Ferrari' represents the how more risky a Ferrari car is compared to a Fiat car with the same power. If the explanatory variables are strongly correlated, it is important to be aware of this aspect. For example, Ferrari cars are usually more powerful that Fiat cars. So, it is possible that in general Ferrari cars are more risky than Fiat cars, but comparing a Ferrari car to a Fiat with the same power, the Ferrari could be less risky. This effect is called _Simpson paradox_ ^[<https://en.wikipedia.org/wiki/Simpson%27s_paradox>].

In bottom-left panel of figure \@ref(fig:expl-var-types), we see the effect of a quantitative variable $x_1$ and a qualitative binary variable $x_2$ together in the model $\mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$. As we can seen, the effects of $x_1$ variable in the two groups defined by $x_2$ variable are represented by two parallel straight lines. The first one is $\mu_i = \beta_0 + \beta_1 x_{i1}$ and the second is $\mu_i = \left(\beta_0 + \beta_2\right) + \beta_1 x_{i1}$. The coefficient $\beta_2$ represents the vertical distance between the two lines.

In bottom-right panel, the interaction effect between $x_1$ and $x_2$ is included in the model. The model becomes $\mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1} x_{i2}$. That means that the effect of $x_1$ variables depends on the determination of the $x_2$ variable. In the group with $x_2=0$ the effect is represented by the line $\mu_i = \beta_0 + \beta_1 x_i$; the group with $x_2=1$ the effect is represented by the line $\mu_i = \left(\beta_0 + \beta_2\right) + \left(\beta_1 + \beta_3\right) x_{i1}$.


For quantitative variables, it is possible to consider also non linear effects in GLMs. Some exmples are reported in figure \@ref(fig:expl-var-quant-effect).


(ref:expl-var-quant-effect-caption-latex) Explanatory quantitative variables effects.

(ref:expl-var-quant-effect-caption-gitbook) Explanatory quantitative variables effects, polynomial degree 2 (top-left), polynomial degree 4 (top-right), piece-wise linear (bottom-left) and piece-wise polynomial degree 2 (bottom-right).

```{r, plot-quant-effect-build, echo = FALSE}
set.seed(42)

col1 <- hue_pal()(2)[1]
col2 <- hue_pal()(2)[2]

line_size <- 2

n <- 200
b0 <- 1
b1 <- 2
b2 <- 1
b12 <- -1
sigma <- 0.05


f1 <- function(x){1.5 * (x - .3)^2 + .25}

f2 <- function(x){20*(x - .5)^4 + -4 * (x - .8)^2 - 2 * x + 2}

f3 <- function(x){
  case_when(
    x <= .25 ~ -2 * x + 1,
    x <= .75 ~ -1/2 * x + 5/8,
    TRUE ~ 1/4
  )
}

f4 <- function(x){
  case_when(
    x <= .75 ~ 1.5 * (x - .75)^2 + .2,
    TRUE ~ .2
  )
}

df <- tibble(x = runif(n = n, min = 0, max = 1)) %>% 
  mutate(
    mu1 = f1(x),
    mu2 = f2(x),
    mu3 = f3(x),
    mu4 = f4(x)
  )


df$y1 <- rnorm(n = n, mean = df$mu1, sd = sigma)
df$y2 <- rnorm(n = n, mean = df$mu2, sd = sigma)
df$y3 <- rnorm(n = n, mean = df$mu3, sd = sigma)
df$y4 <- rnorm(n = n, mean = df$mu4, sd = sigma)


p_quant_effect_1 <- df %>% 
  select(x, value = y1) %>% 
  ggplot() +
  stat_function(
    fun = f1,
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(NA, 1.05)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )


p_quant_effect_2 <- df %>% 
  select(x, value = y2) %>%  
  ggplot() +
  stat_function(
    fun = f2,
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(NA, 1.05)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )

p_quant_effect_3 <- df %>% 
  select(x, value = y3) %>%  
  ggplot() +
  # geom_vline(data = tibble(xint = c(.25, .75)),
  #            aes(xintercept = xint),
  #            linetype = "dotted") +
  geom_vline(xintercept = c(.25, .75),
             linetype = "dotted") +
  stat_function(
    fun = f3,
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(NA, 1.05)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )

p_quant_effect_4 <- df %>% 
  select(x, value = y4) %>%  
  ggplot() +
  geom_vline(xintercept = .75,
             linetype = "dotted") +
  stat_function(
    fun = f4,
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(NA, 1.05)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )
```

```{r, plot-quant-effect-print, out.width = "50%", fig.align='center', fig.cap=ifelse(knitr::is_html_output(), "(ref:expl-var-quant-effect-caption-gitbook)", "(ref:expl-var-quant-effect-caption-latex)"), label="expl-var-quant-effect", echo=FALSE, fig.ncol=2, fig.subcap=c('Polynomial degree 2', 'Polynomial degree 4', 'Piece-wise linear', 'Piece-wise polynomial degree 2')}

# # To align the plots in a subfigure environment
# plot_grid_split <- function(..., align = "hv", axis = "tblr"){
#   aligned_plots <- cowplot::align_plots(..., align = align, axis = axis)
#   plots <- lapply(1:length(aligned_plots), function(x){
#     cowplot::ggdraw(aligned_plots[[x]])
#   })
#   invisible(capture.output(plots))
# }

plot_grid_split(p_quant_effect_1, p_quant_effect_2, p_quant_effect_3, p_quant_effect_4)
```


The basic way to achieve it is by adding polynomial terms to the linear predictor. For instance, if $x$ is a quantitative variable, it is possible to add to the model the term $x^2$, obtaining the model $\mu_i = \beta_0 + \beta_1 x_{i} + \beta_2 x_i^2$. An example a model with both $x$ and $x^2$ terms is represented in top-left panel of figure \@ref(fig:expl-var-quant-effect). Adding the quadratic term, the effect graph becomes a parabola.

With the same logic, it is possible to add more power terms. In general, if we want to model $x$ with a polynomial of degree $d$, we can consider the model $\mu_i = \beta_0 + \beta_1 x_{i} + \beta_2 x_{i}^2 + \dots + \beta_d x_{i}^d$. In top-right panel of figure \@ref(fig:expl-var-quant-effect) a 4^th^ degree polynomial effect is repesented. We highlight that the model is still considered linear, as the attribute "Linear" in "General Linear Model" is referred to the relation between the parameters $\beta_j$ and the linear predictor $\eta_i$ that must be linear.

Another way to model non liner effects of explanatory varibles is to separate the effects by pieces. In bottom-left panel of figure \@ref(fig:expl-var-quant-effect) a case in which the $x$ effect is separated in 3 pieces is represented. As in all the pieces the effect is linear, the graph of the variable effect is a broken line. This effect can be achieved by adding to the model the terms $(x-\nu)_+$, where $(x)_+$ represents the positive part of $x$ ($(x)_+ = \max(0,x)$) and $\nu$ is the value of $x$ corresponding to one of the angular points. The $\nu$ values are called _nodes_, If the nodes are $\nu_1, \nu_2, \dots, \nu_m$, the model can be represented as $\mu_i = \beta_0 + \beta_1 x_i + \beta_2 (x_i-\nu_1)_+ + \beta_3 (x_i-\nu_2)_+ + \dots + \beta_{m+1} (x_i-\nu_m)_+$. This kind of functions are called _linear splines_ and will be further discussed in section \@ref(chap:gam). If we want the effect to be null from a certain point $\nu$, we can consider the variable $x' = \min(x, \nu)$. This corresponds to aggregate to $\nu$ all the $x$ after $\nu$.

The piece-wise approach can be enhanced by also considering polynomials terms. For instance, in bottom-left panel of figure \@ref(fig:expl-var-quant-effect), the model represented is $\mu_i = \beta_0 + \left( x_i - \nu \right)_-^2$, where $(x)_-$ is the negative part of $x$ ($(x)_- = \min(0,x)$). $f(x) = (x-\nu)^2$ is a parabola with vertex in $\nu$. The fact of not adding the linear term leads to a monotonic non-increasing effect made by a semiparabola and a horizontal semiline that starts from its vertex.

The examples represented in figures \@ref(fig:expl-var-types) and \@ref(fig:expl-var-quant-effect) are based on simulated data. That means that the linear predictor structure is known and the coefficients $\beta_0, \beta_1, \dots, \beta_J$ are known. In practice, the real model is not known and the coefficients and the structure must be estimated by the data. Thus, we can take assumptions on the structure and we can estimate the coefficients with $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_J$. In many cases it is not so clear whether to consider or not a variable and how to consider it. For example, with the same data both bottom-left and bottom-right models could work fine. In section \@ref(chap:variable-selection) we are going to discuss some variable selection techniques for GLM.


<!--
* Qualitative variables / binary variables
  + Dummy variables
* Quantitative variables
  + Linear effects
  + Polynomial effects
    - GAM reference
  + piece wise
* Interactions
  + Manual interactions
    Problem: Scalability. Con p parametri ho choose(p, 2) possibili interazioni
 -->

<!--
Disclaimer:
Non Ã¨ nota a priori la forma del predittore lineare e va stimata
Nei grafici ho sempre rappresentato la curva coi veri \beta tramite i quali i punti sono stati simulati
Nella pratica i \beta non sono noti e avremo solo gli \hat{\beta}
-->



#### Link functions and relativities

(ref:resp-var-caption-latex) Response variables and link functions.

(ref:resp-var-caption-gitbook) Response variables and link functions, Normal - identity (top-left), Binomial - logit (top-right), Poisson - log (bottom-left) and Gamma - log (bottom-right).


```{r, plot-resp-var, echo = FALSE}

set.seed(42)

col1 <- hue_pal()(2)[1]
col2 <- hue_pal()(2)[2]

line_size <- 2

n <- 200
b0 <- -2
b1 <- 4
# b2 <- 1
# b12 <- -1
sigma <- .2
alpha <- 2

df <- tibble(x = runif(n = n, min = 0, max = 1)) %>% 
  mutate(
    eta1 = b0 + b1 * x,
    eta2 = b0 + b1 * x,
    eta3 = b0 + b1 * x,
    eta4 = b0 + b1 * x,
    mu1 = eta1,
    mu2 = plogis(eta2),
    mu3 = exp(eta3),
    mu4 = exp(eta4)
  )

df$y1 <- rnorm(n = n, mean = df$mu1, sd = sigma)
df$y2 <- rbinom(n = n, size = 1, prob = df$mu2)
df$y3 <- rpois(n = n, lambda = df$mu3)
df$y4 <- rgamma(n = n, shape = alpha, rate = alpha/df$mu4)



p_resp_1 <- df %>% 
  select(x, value = y1) %>% 
  ggplot() +
  geom_abline(
    intercept = b0,
    slope = b1,
    col = col1,
    size = line_size
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )

p_resp_2 <- df %>% 
  select(x, value = y2) %>%  
  ggplot() +
  stat_function(
    fun = function(x){plogis(b0 + b1 * x)},
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1)) +
  easy_remove_axes(
    which = "both",
    what = "text",
    teach = FALSE
  )

p_resp_3 <- df %>% 
  select(x, value = y3) %>%  
  ggplot() +
  stat_function(
    fun = function(x){exp(b0 + b1 * x)},
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(0, 14)) #+
  # easy_remove_axes(
  #   which = "both",
  #   what = "text",
  #   teach = FALSE
  # )

p_resp_4 <- df %>% 
  select(x, value = y4) %>%  
  ggplot() +
  stat_function(
    fun = function(x){exp(b0 + b1 * x)},
    col = col1,
    size = line_size,
    xlim = c(-0.05, 1.05)
  ) +
  geom_point(aes(x = x, y = value),
             alpha = .4) +
  # scale_x_continuous(limits = c(0, 1)) +
  coord_cartesian(xlim = c(0, 1),
                  ylim = c(0, 14)) #+
  # easy_remove_axes(
  #   which = "both",
  #   what = "text",
  #   teach = FALSE
  # )

```

```{r, plot-quant-effect-print, out.width = "50%", fig.align='center', fig.cap=ifelse(knitr::is_html_output(), "(ref:resp-var-caption-gitbook)", "(ref:resp-var-caption-latex)"), label="resp-var", echo=FALSE, fig.ncol=2, fig.subcap=c('Normal - identity', 'Binomial - logit', 'Poisson - log', 'Gamma - log')}

# # To align the plots in a subfigure environment
# plot_grid_split <- function(..., align = "hv", axis = "tblr"){
#   aligned_plots <- cowplot::align_plots(..., align = align, axis = axis)
#   plots <- lapply(1:length(aligned_plots), function(x){
#     cowplot::ggdraw(aligned_plots[[x]])
#   })
#   invisible(capture.output(plots))
# }

plot_grid_split(p_resp_1, p_resp_2, p_resp_3, p_resp_4,
                align = "h")
```




<!--
link log -> modello moltiplicativo
-->

#### Variable selection {#chap:variable-selection}

<!--
* problem: effect not known
  + not known variables
  + not known shape
* aproaches
  + looking to residuals (x_i, r_i)
  + hypotheses testing
  + AIC/BIC
* manual work
  + scalability
    - choice of variables. Ho 2^p possibili modelli
    - choice of interactions. Ho choose(p, 2) possibili interazioni
    Usually actuarial models are updated once a year, so it is possible to start from models made the year before
* domain knowledge, expertise
-->



\newpage

### Elastic Net

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.


### Bayesian GLM

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.


### GAM {#chap:gam}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.


### GBM

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.



## Model comparison {#chap:model-comparison}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.



## The actuary importance {#chap:actuary-importance}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.



## Implementation

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus id mauris interdum, malesuada ante eu, tempus lacus. Aliquam blandit tortor a velit ultricies, eget pharetra nulla egestas. Suspendisse pellentesque finibus est, vitae ullamcorper magna convallis ut. Nulla a lectus in ligula iaculis convallis. Pellentesque tortor mauris, tempor nec dictum et, facilisis sit amet dolor. Mauris nibh quam, molestie non ex quis, hendrerit dignissim nulla. Aliquam sit amet dui at diam vestibulum malesuada a id lacus. Phasellus viverra orci vitae sem pretium, eu consequat libero euismod.

Cras suscipit aliquam consequat. Quisque sodales lacus ac erat malesuada, eu laoreet enim vestibulum. Sed id ante id ligula auctor ullamcorper. Sed luctus rutrum mollis. Vestibulum sed ultrices quam. Duis id orci ut enim elementum maximus id quis justo. Pellentesque rutrum ligula in aliquam rhoncus. Integer suscipit nisl at mi efficitur interdum. Aenean et orci elit.

Nam ultricies est et iaculis tempus. Quisque leo lorem, sagittis et ligula a, blandit mattis velit. Phasellus pretium, orci et semper finibus, dui nulla tempor nisl, vel vehicula magna diam nec sem. Praesent finibus commodo enim non laoreet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ut pellentesque purus. Proin hendrerit, odio vel sodales porta, ex lorem feugiat sem, non fringilla libero ex ac ligula. Quisque facilisis eros at suscipit rhoncus.








<!-- \titlespacing{\chapter}{0pt}{0pt}{35pt} -->

