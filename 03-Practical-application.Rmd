---
output:
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::html_document2: default
documentclass: book
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
library(knitr)
```

<!-- Needed for leaving space to the quote, * is for no indentation after title --> 

<!-- \titlespacing*{\chapter}{0pt}{80px}{35pt} -->

# Practical application {#chap:practical-app}
\minitoc  <!--this will include a mini table of contents-->

<!-- \chaptermark{Practical application} -->

In this final chapter we are going to present a practical application of the models described in section \@ref(chap:models) on a real-world actuarial dataset. After describing the problem and how the models have been implemented, we will assess their performance and comment the results.


## Data description

### Dataset

```{r, years_at_risk_read, echo = FALSE, cache = TRUE, message = FALSE}
tab_years_at_risk <- read_csv2("data/practical_application/tab_years_at_risk.csv")
```

The dataset used is an actuarial dataset from an Italian \ac{mtpl} portfolio. In table \@ref(tab:years-at-risk) the total exposure of the dataset is reported. In the dataset there are `r tab_years_at_risk$n` rows for a total of `r round(tab_years_at_risk$rischi_anno, 1)` years-at-risk. In the dataset, every row is a couple (policy, accounting year), so, if a policy spans in two years, we will see two rows corresponding to that policy. On average, every row has exposure `r round(tab_years_at_risk$durata_media, 3)` years-at-risk. This number is slightly lower than 0.5 because some of the policies get suspended and span more than two years.

```{r, years_at_risk_render, echo = FALSE, cache = TRUE, message = FALSE}
tab_years_at_risk %>% 
  rename(
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `\\makecell[c]{Average Exposure\\\\per Observation}` = durata_media
  ) %>% 
  kable(
    digits = c(0, 1, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "ccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    bottomrule = "\\bottomrule",
    caption = "Total dataset exposure. The exposure is measured in year-at-risk (Y.a.R)",
    caption.short = "Total dataset exposure.",
    label = "years-at-risk",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  # row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
  row_spec(0, bold = T)
```


The dataset contains policies from the period 2014-2019 coming from a specific province. The fact that we filtered the policies from only one province reduces a lot the size of the dataset and, since the models have been run locally on a single computer, this significantly reduces the time spent to run them. From a modeling point of view, the reduction to only one province reduces the importance of the geography in the models. Indeed, between the different regions of Italy there are a lot of socio-economical differences that are reflected in the claims experience. In this thesis the problem of using geographical data as explanatory variables has not been addressed.


```{r, description_train_test_read, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test <- read_csv2(
  "data/practical_application/tab_freq_sin_train_test.csv"
)


tab_freq_sin_train_test_tot <- tab_freq_sin_train_test %>% 
  bind_rows(
    tab_freq_sin_train_test %>% 
      summarize(
        n = sum(n),
        rischi_anno = sum(rischi_anno),
        n_clienti = sum(n_clienti),
        numero_causati = sum(numero_causati)
      ) %>% 
      mutate(
        rischi_anno_per_cliente = rischi_anno / n_clienti,
        freq_sin = numero_causati / rischi_anno,
        set = "tot"
      )
  )


tab_1 <- tab_freq_sin_train_test_tot %>% 
  select(
    set, n, rischi_anno, n_clienti, rischi_anno_per_cliente
  ) %>% 
  mutate(set = str_to_title(set)) %>%
  rename(
    Set = set,
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `Policyholders` = n_clienti,
    `\\makecell[c]{Exposure per\\\\Policyholder}` = rischi_anno_per_cliente,
  ) 


tab_2 <- tab_freq_sin_train_test_tot %>% 
  select(
    set, n, rischi_anno, numero_causati, freq_sin
  ) %>% 
  mutate(set = str_to_title(set)) %>%
  rename(
    Set = set,
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `Claims N.` = numero_causati,
    `Claims Freq.` = freq_sin
  )

# For compatibility with HTML
if(!knitr::is_latex_output()){
  names(tab_1) <- names(tab_1) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
  
  names(tab_2) <- names(tab_2) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}
```

The dataset has been split into a training set with 80% of the observations and a test set with the remaining 20% of the observations. The training set has been used to fit the models, while the test set has been used to assess them by comparing the predictions of the models with the observed data. The splitting has been made on the base of the policyholder id. That means that policies from the same policyholder end in the same set. This has been done because policies from the same policyholder are correlated, so splitting the observations by policyholder prevents data leakage from the test set to the training set. Table \@ref(tab:exposure-train-test) shows the exposure and the number of policyholders in the training set and the test set. As we can see, on average, each policyholder has `r round(tab_freq_sin_train_test_tot %>% filter(set == "tot") %>% .$rischi_anno_per_cliente, 2)` years-at-risk.


```{r, description_train_test_render_1, echo = FALSE, cache = TRUE, message = FALSE}

tab_1 %>% 
  kable(
    digits = c(0, 0, 1, 0, 2),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Exposure and number of policyholders in training and test set. The exposure is measured in year-at-risk (Y.a.R)",
    caption.short = "Exposure and number of policyholders in training and test set.",
    label = "exposure-train-test",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  # row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T) %>% 
  row_spec(0, bold = T) %>%
  row_spec(
    nrow(tab_1) - 1,
    extra_latex_after = "\\midrule"
  ) 

```

### Response variable

The response variable for the models is the number of claims caused by the policyholder for that policy in that accounting year. In this project we compared the different techniques in predicting the claims frequency. We didn't take into account the claim severity and we didn't split the claims by type (see section \@ref(chap:response-variables-and-distributions)).

Table \@ref(tab:freq-sin-train-test) shows the observed claims frequency in the training set and in the test set. As we can see, the claims frequency in the training set is slightly higher than the claims frequency in the test set. The difference between the two sets is given by chance, but it is also encouraged by the fact that the policies has been split by policyholder id and not by observation. The split by policyholder id implicates that, if a policyholder has many policies, all of them fall into the same set, so the hypothesis of independence for the observations within the sets is not respected. We will discuss more in depth the effect of splitting the data into training and test set by policyholder instead of observation in section \@ref(chap:exploration-splitting-policyholder).

Anyway, this issue is limited and in this analysis has been neglected, as is usually done in actuarial practice. We also mention that theoretically the difference in claims frequency in the two sets should be mostly explained by the explanatory variables we have, so the models fitted with the training set should be able to catch the characteristics of the policies of the test set that reduce their expected claims frequency.

```{r, description_train_test_render_2, echo = FALSE, cache = TRUE, message = FALSE}
tab_2 %>% 
  kable(
    digits = c(0, 0, 1, 0, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    bottomrule = "\\bottomrule",
    caption = "Claims Frequency in training and test set.",
    label = "freq-sin-train-test",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  # row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T) %>% 
  row_spec(0, bold = T) %>% 
  row_spec(
    nrow(tab_2) - 1,
    extra_latex_after = "\\midrule"
  ) 

```

As we mentioned in section \@ref(chap:model-fitting-and-data-available), the claims settlement is a long process and the response variable we use for fitting the models depends on the moment in which the claims are observed. Indeed it is possible that part of the claims we are considering as response variables will be closed as null-claims and it is possible that there are claims occurred in the period of exposure considered that are not yet reported (IBNyR) at the moment in which the policies have been observed. This phenomenon is more relevant if the period of exposure is close to the moment of observation of the claims.

The claims reported in the dataset are all observed at the date 30/06/2020. In figure \@ref(fig:freq-sin-train-test-year) the claims frequency for each year in the training set and in the test set is reported. The same data is reported in table \@ref(tab:freq-sin-train-test-year). As we can see the years 2017, 2018 and 2019 present a claims frequency lower than the years 2014, 2015 and 2018, so it looks like there is a structure in the claims frequency over the years. Unfortunately, just by looking to this data we can't say whether this is due to the different settlement of the claims over the years, to a different mix of policies or just to a overall claims trend in the whole market. Anyway, looking to the confidence intervals we realize that this is not a big issue as the effect of the year is relatively small.

Comparing the claims frequency of the training set and the test set, we see that the claims frequency in the test set is consistently lower than the claims frequency in the training set. This phenomenon can be explained with the fact that we divided the policies based on policyholder id, so the policies within each set are correlated.

```{r, freq-sin-train-test-year-read, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test_year <- read_csv2(
  "data/practical_application/tab_freq_sin_train_test_year.csv"
)
```

(ref:freq-sin-train-test-year-plot-caption) Claims frequency in training and test set in the different years. The semi-transparent ribbons represent 95% confidence intervals for the claims frequency in each year.

(ref:freq-sin-train-test-year-plot-caption-short) Claims frequency in training and test set in the different years.

```{r, freq-sin-train-test-year-plot, out.width = "100%", fig.width = 9, fig.height = 4, fig.align='center', fig.cap = "(ref:freq-sin-train-test-year-plot-caption)", fig.scap = "(ref:freq-sin-train-test-year-plot-caption-short)", label = "freq-sin-train-test-year", echo = FALSE, cache = TRUE}
tab_freq_sin_train_test_year %>% 
  filter(set != "Tot") %>% 
  mutate(
    anno_analisi = factor(anno_analisi),
    set = fct_inorder(set)
  ) %>%
  ggplot(aes(x = anno_analisi, y = freq_sin, color = set, fill = set, group = set)) +
  facet_wrap(~set) +
  geom_line() +
  geom_point() +
  geom_ribbon(aes(ymin = down, ymax = up),
              alpha = .2) +
  labs(x = "Accounting Year", y = "Claims Frequency") +
  easy_remove_legend()
```

```{r, freq-sin-train-test-year-render, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test_year %>% 
  select(set, anno_analisi, freq_sin) %>% 
  pivot_wider(
    names_from = anno_analisi,
    values_from = freq_sin
  ) %>% 
  rename(Set = set) %>% 
  kable(
    digits = c(0, 3, 3, 3, 3, 3, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    bottomrule = "\\bottomrule",
    caption = "Claims frequency in training and test set in the different years.",
    label = "freq-sin-train-test-year",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  # row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
  row_spec(0, bold = T) %>% 
  row_spec(
    2,
    extra_latex_after = "\\midrule"
  ) 

```


#### Exploration on the effect of splitting data by policyholder {#chap:exploration-splitting-policyholder}

To explore more in depth the effect of splitting the data into training and test set by observation or by policyholder, we conducted a simulation analysis. We took the whole dataset and we simulated for each observation a random flag that represents whether the observation belongs to the training set or to the test set with 80% probability of belonging to the training set and 20% probability of belonging to the test set. We repeated the simulation 10k times with a random variable based on observation and 10k times with a random variable based on policyholder. In each simulation, we fitted a GLM with only that flag as explanatory variable and the number of claims as response variable. Then, for each simulation, we looked at the p-value of the test over the significance of that flag. This p-value can be used to check whether the training and the test sets have a significative difference in term of claim frequency. Indeed, if we call $\bar{y}^\text{train}$ the average response in the training set and $\bar{y}^\text{test}$ the average response in the test set, the higher $\left|\bar{y}^\text{train} - \bar{y}^\text{test}\right|$ is, the lower the p-value is. Thus, the p-value can be seen as a normalized measure of the distance between $\bar{y}^\text{train}$ and $\bar{y}^\text{test}$.


```{r, simulation-random-read, echo = FALSE, cache = TRUE, message = FALSE}
simulations <- read_csv2(
  "data/practical_application/random_simulations_pvalue.csv"
)

simulations_count <- simulations %>% 
  mutate(
    p_value_cut = cut_width(
      p_value, width = .05,
      # closed = "right",
      closed = "left",
      boundary = 0
      # boundary = 50
    ),
    random_type = if_else(
      random_type == "c",
      "Random by policyholder",
      "Random by observation"
    ) %>% 
      fct_inorder()
  ) %>% 
  group_by(random_type) %>% 
  count(p_value_cut) %>% 
  mutate(
    freq = n/sum(n),
    density = freq / 0.05
  ) %>% 
  ungroup()
```

The distribution of the p-values for the two kinds of random variables are reported in figure \@ref(fig:simulation-random). The frequency for the first 4 buckets are also reported in table \@ref(tab:simulation-random). In theory, if the variable was totally random and the observations were independent, the distribution of the p-values should be uniform. As we can see, the distribution of the p-values for the random by policyholder is not uniform and the lower p-values are more likely. Indeed, in our simulation, with the random by policyholder the observed frequency of the p-values lower than 0.05 is `r simulations_count %>% filter(random_type == "Random by policyholder", p_value_cut == "[0,0.05)") %>% .$freq`, that is quite higher than 0.05.

(ref:simulation-random-plot-caption) Distribution of the p-values from the simulation of the random variables. With the random by policyholder, the lower p-values are more likely.

(ref:simulation-random-plot-caption-short) Distribution of the p-values from the simulation of the random variables.

```{r, simulation-random-plot, out.width = "100%", fig.width = 9, fig.height = 4, fig.align='center', fig.cap = "(ref:simulation-random-plot-caption)", fig.scap = "(ref:simulation-random-plot-caption-short)", label = "simulation-random", echo = FALSE, cache = TRUE}
simulations %>% 
  mutate(
    random_type = if_else(
      random_type == "c",
      "Random by policyholder",
      "Random by observation"
    ) %>% 
      fct_inorder()
  ) %>% 
  ggplot(aes(x = p_value, fill = random_type, color = random_type)) +
  geom_histogram(
    # binwidth = .05
    # bins = seq(from = -0.01, to = 1.01, by = .05)
    aes(y = ..density..),
    breaks = seq(0, 1, b = 0.05),
    alpha = .5
  ) +
  facet_wrap(~random_type) +
  easy_remove_legend() +
  labs(
    x = "p-value", y = "Density"
  )
```


```{r, simulation-random-render, echo = FALSE, cache = TRUE, message = FALSE}
simulations_count %>% 
  filter(as.numeric(p_value_cut) < 5) %>% 
  select(random_type, p_value_cut, freq) %>%
  mutate(
    p_value_cut = p_value_cut %>% 
      as.character() %>% 
      str_replace_all(",", ", ")
  ) %>% 
  pivot_wider(
    names_from = p_value_cut,
    values_from = freq
  ) %>% 
  rename(
    `Random type` = random_type
  ) %>% 
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    bottomrule = "\\bottomrule",
    caption = "Distribution of the p-values from the simulation of the random variables. With a random variable, these frequencies should be around 0.05. With the random by policyholder, the frequencies for low p-values are higher than that.",
    caption.short = "Distribution of the p-values from the simulation of the random variables.",
    label = "simulation-random",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  # row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
  row_spec(0, bold = T)
```

As we already mentioned, the higher frequency of small p-values in the case of split by policyholder comes from the fact that the p-values are computed under the hypothesis of independence, so, as in this setting the observations within the sets are correlated, the assumptions used for computing the p-values are not correctly specified. The fact that there is correlation in the observations within the sets means that the variances of $\bar{y}^{\text{train}}$ and $\bar{y}^{\text{test}}$ are underestimated so, if we interpret these values as p-values of the hypothesis testing for the difference of $E(y^{\text{train}})$ and $E(y^{\text{test}})$, we can say that they have not been correctly calculated. However, if we just interpret the p-value calculated under the hypothesis of independence as a normalized measure of distance between $\bar{y}^{\text{train}}$ and $\bar{y}^{\text{test}}$, we can say that, with the random by policyholder, $\bar{y}^{\text{train}}$ and $\bar{y}^{\text{test}}$ are on average more distant than with the random by observation.

Figure \@ref(fig:simulation-random-diff) shows the distribution of the distances of the claim frequency in the training and test set $\left|\bar{y}^\text{train} - \bar{y}^\text{test}\right|$ from the simulation of the random variables. The distributions have been smoothed with a gaussian kernel density function. As we can see from the plot, with the random by policyholder, the central values are less likely and the tails are heavier. This result is consistent with the fact that with the random by policyholder the low p-values are more frequent.

(ref:simulation-random-diff-plot-caption) Distribution of the distances $\left|\bar{y}^\text{train} - \bar{y}^\text{test}\right|$ from the simulation of the random variables. With the random by policyholder, central values are less likely and the tails are heavier.

(ref:simulation-random-diff-plot-caption-short) Distribution of the distances $\left|\bar{y}^\text{train} - \bar{y}^\text{test}\right|$ from the simulation of the random variables.

```{r, simulation-random-diff-plot, out.width = "80%", fig.width = 8, fig.height = 4, fig.align='center', fig.cap = "(ref:simulation-random-diff-plot-caption)", fig.scap = "(ref:simulation-random-diff-plot-caption-short)", label = "simulation-random-diff", echo = FALSE, cache = TRUE, message = FALSE}

simulations_freq_sin <- read_csv2(
  "data/practical_application/results/random_simulations_freq_sin.csv"
)

simulations_freq_sin <- simulations_freq_sin %>% 
  mutate(
    diff_sign = train - test,
    random_type = str_sub(random, 1, 1),
    random_number = str_sub(random, 2, -1),
    random_type = if_else(
      random_type == "c",
      "Random by policyholder",
      "Random by observation"
    ) %>% 
      fct_relevel("Random by observation")
  )

simulations_freq_sin %>% 
  ggplot(aes(x = diff_sign, fill = random_type, color = random_type)) +
  geom_density(alpha = .2, size = 1, bw = 3e-4) +
  labs(x = "Difference", y = "Density", 
       color = "Split Criterion",
       fill = "Split Criterion")


```

Regardless the observations we made, in our analysis we accept this potential difference in the claims frequency between the training set and the test set and we keep the splitting made with the random by policyholder.


### Explanatory variables

```{r, explanatory-variables-read, echo = FALSE, cache = TRUE, message = FALSE}
explantory_varibles <- read_csv2(
  "data/practical_application/tab_explanatory_variables.csv"
)


# Sum the last row of each column if numeric 
func <- function(z){ if (is.numeric(z)) sum(z) else '' }
sumrow <- as_tibble(lapply(explantory_varibles, func))

# Give name to the first element of the new data frame created above
sumrow["Description"] <- "Total"

# Add the original and new data frames together
explantory_varibles_total <- bind_rows(explantory_varibles, sumrow)

explanatory_variables_number <- explantory_varibles_total %>%
  filter(Description == "Total") %>%
  .$`Number of variables per category`


```

In the dataset we have a total of `r explanatory_variables_number` explanatory variables that are considered meaningful for predicting the claims frequency. In table \@ref(tab:explanatory-variables) we can see the number of variables split by category. The categories are described in section \@ref(chap:pricing-variables).

In the modeling process we will assess whether these variables are significant or not and only some of them will be used to predict the response variable.

```{r, explanatory-variables-render, echo = FALSE, cache = TRUE, message = FALSE}

# Make Table
explantory_varibles_total %>% 
  mutate(
    Description = case_when(
      !knitr::is_latex_output() ~ Description,
      row_number() == n() ~ Description,
      TRUE ~ str_c("\\em{", Description, "}")
    )
  ) %>% 
  select(-group, -category) %>% 
  mutate_all(
    .funs = function(x){
      if_else(
        knitr::is_latex_output() & (row_number() %% 2 == 1) & (row_number() != n()),
        str_c("\\cellcolor{gray!6}{", x, "}"),
        as.character(x)
      )
    }
  ) %>% 
  rename(
    `\\makecell[c]{Number of variables\\\\per category}` = `Number of variables per category`
  ) %>% 
  # kable(caption = "Normalized Annual Energy Demand", booktabs = TRUE) %>%
  # kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  # row_spec(nrow(explantory_varibles_total), bold = T) %>% # format last row
  # column_spec(1, italic = T) # format first column
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lc",
    vline = "",
    toprule = "\\toprule",
    midrule = "\\midrule[\\heavyrulewidth]",
    linesep = "",
    bottomrule = "",
    # bottomrule = "\\bottomrule",
    caption = "Number of explanatory variables per category.",
    label = "explanatory-variables",
    escape = FALSE
  )  %>%
  kable_styling(
    position = "center",
    # latex_options = c("striped", "hold_position"),
    latex_options = "hold_position",
    full_width = FALSE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>% 
  row_spec(0, bold = T) %>%
  # row_spec(
  #   ifelse(
  #     !knitr::is_latex_output(),
  #     nrow(explantory_varibles_total),
  #     nrow(explantory_varibles_total) + 1
  #   ),
  #   bold = T,
  # ) %>% 
  # row_spec(nrow(explantory_varibles_total) - 1,
  #          hline_after = TRUE) %>%
  row_spec(
    nrow(explantory_varibles_total) - 1,
    # extra_latex_after = "\\bottomrule"
    # extra_latex_after = "\\midrule[\\heavyrulewidth]"
    extra_latex_after = "\\midrule"
  ) %>% 
  row_spec(
    nrow(explantory_varibles_total),
    extra_latex_after = "\\bottomrule"
  ) %>% 
  row_spec(nrow(explantory_varibles_total),
           bold = T,
           italic = F) #%>%
  # column_spec(1, italic = T)
```


<!-- \newpage -->

## Models assessment

<!-- The splitting of the dataset into training set and test set allows us to use the training set to fit the model and the test set to assess them by comparing the predictions of the models with the observed data. -->

The metric adopted to assess the performance of each model is the Poisson deviance computed on the test set (see section \@ref(chap:glm-model-fitting) for more details):
$$
D(\hat{\boldsymbol{\beta}}, \boldsymbol{y}) = 2\,\sum_{i=1}^{n}{\left\{ y_i \log{\left(\frac{y_i}{\hat{\mu}_i}\right)} - \left( y_i - \hat{\mu}_i \right) \right\}}
$$

The deviance can be seen as a distance between the predictions $\hat{\mu}_i$ and the observed response variable $y_i$. The lower the deviance on the test set is, the better the model is.


<!-- \newpage -->

## Models description

### List of models

In table \@ref(tab:models-list) the models developed are listed. We started from a GLM that takes into account all the available variables (_GLM Tot_). Then, with the same variables, we considered an Elastic Net (_Elastic Net Tot_) and a Ridge regression (_Ridge Tot_). After this, we computed a feature selection with a stepwise approach based on AIC obtaining a GLM with just a subset of the available variables (_GLM AIC_). With the same subset of variables we also fitted an Elastic Net (_Elastic Net AIC_) and a GAM (_GAM AIC_). Finally, to have a comparison with a widely used general purpose machine learning model, we fitted a GBM with all the available variables (_GBM Tot_). More details on the models listed in table \@ref(tab:models-list) are described more in depth in section \@ref(chap:models-details).


```{r, models-results-read, echo = FALSE, cache = TRUE, message = FALSE}
results <- read_csv2(
  "data/practical_application/results/results.csv"
)

results <- results %>% 
  # select(time, time1, time2) %>%
  mutate(
    time_period = seconds_to_period(time),
    time_hour = hour(time_period),
    time_minute = minute(time_period),
    time_second = second(time_period),
    time_string = str_c(
      if_else(time_hour > 0, str_c(time_hour, "h "), ""),
      if_else(time_hour > 0 | time_minute > 0, str_c(time_minute, "m "), ""),
      format(time_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time_period, time_hour, time_minute, time_second)
  ) %>% 
  mutate(
    time1_period = seconds_to_period(time1),
    time1_hour = hour(time1_period),
    time1_minute = minute(time1_period),
    time1_second = second(time1_period),
    time1_string = str_c(
      if_else(time1_hour > 0, str_c(time1_hour, "h "), ""),
      if_else(time1_hour > 0 | time1_minute > 0, str_c(time1_minute, "m "), ""),
      format(time1_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time1_period, time1_hour, time1_minute, time1_second)
  ) %>% 
  mutate(
    time2_period = seconds_to_period(time2),
    time2_hour = hour(time2_period),
    time2_minute = minute(time2_period),
    time2_second = second(time2_period),
    time2_string = str_c(
      if_else(time2_hour > 0, str_c(time2_hour, "h "), ""),
      if_else(time2_hour > 0 | time2_minute > 0, str_c(time2_minute, "m "), ""),
      format(time2_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time2_period, time2_hour, time2_minute, time2_second)
  ) %>% 
  mutate(
    id = str_c(
      "mod", row_number()
    )
  ) %>% 
  select(
    id, everything()
  )


```


```{r, models-list-render, echo = FALSE, cache = TRUE, message = FALSE}
# Make Table
results %>%
  select(id, model) %>% 
  mutate(
    id = str_to_title(id),
    model = model %>% 
      str_to_title() %>% 
      str_replace_all("Aic", "AIC") %>% 
      str_replace_all("Glm", "GLM") %>% 
      str_replace_all("Gam", "GAM") %>% 
      str_replace_all("Gbm", "GBM")
  ) %>% 
  rename_all(
    .funs = str_to_title
  ) %>% 
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "ll",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    # midrule = "\\hline\\addlinespace",
    # midrule = "\\hline",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "List of models developed.",
    label = "models-list",
    escape = FALSE
  ) %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>% 
  row_spec(0, bold = T) #%>% 
  # column_spec(2, italic = T)

```


### Implementation details

All the models have been fitted with H2O, except for the GAM. On H2O the following functions are available:

* [`h2o.glm()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html) that allows to build GLMs with Elastic Net penalization;
* [`h2o.gam()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gam.html) that allows to build GAMs with both GAM penalization to splines and Elastic Net penalization to other variables;
* [`h2o.gbm()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html) that allows to build GBMs.

Anyway, as described in H2O documentation, "GAM models are currently experimental". We had some trials with GAMs on H2O, but there are still many bugs and many functions for the hyperparameter tuning are not available yet. For this reason, for the GAM model we used the function `gam()` from the package `mgcv`.

The big advantage of H2O is that it provides embedded multi-threading implementation and provides built-in functions for hyper-parameter tuning. In the practice, that translates to a high-performance solution that is substantially faster and is much more memory-efficient than what the other R packages offer.

Due to the lower efficiency and the lack of hyper-parameter tuning functions, we used the function from `mgcv` just as a quick trial, without performing a deep hyper-parameter tuning.

Al the pre-processing and the model fitting has been run locally on a single PC with 16GB of DDR4 \ac{ram} and a \ac{cpu} Intel Core i7-8750H, that is a 2.20GHz hexa-core \ac{cpu} with hyper-threading. Within these resources, 12GB of the 16GB available \ac{ram} and 11 of the 12 available threads have been dedicated to the H2O instance.


### Approach adopted in the modeling

<!-- In the whole modeling process we tried to keep an approach as more objective as possible, in order to not condition the results. -->

One of the strengths of the GLM-based methods is that they allow to extensively introduce external information in the process of fitting. That means that, still using the same techniques, a skilled modeler is able to build a better model than a beginner.

The aim of our analysis is to perform a comparison just between the models, so the skills of the modeler would be a disturbance factor. In order to prevent this factor to influence our results, in the whole modeling process we adopted techniques as much objective as possible.

To achieve this goal in the basic GLM, for the feature selection we performed a stepwise algorithm based on AIC instead of a manual process.

The only manual element in the model fitting has been the definition of the effects of the quantitative variables. For the quantitative variables we considered piece-wise polynomial effects based on the plots of the residuals. Anyway we have been quite generous in the number of terms taken into account and we let the algorithm choose which of them to keep in the model.

For the interactions, we decided to not consider any interaction term in the GLM-based models. This choice comes from the fact that from our exploration there were not particularly important interactions between the variables and from the fact that, in GLM-based models, the choices of interactions terms is usually a manual process and the ability of the modeler influences a lot the result, introducing a disturbance factor.

We will return on some considerations of the benefits of the modeler intervention in section \@ref(chap:conclusions).

<!-- \newpage -->

### Models details {#chap:models-details}

```{r, read-coeffcients, echo = FALSE, cache = TRUE, message = FALSE}

coefficients_encripted_table_h2o_th_glm_tot <- read_csv2(
  "data/practical_application/results/coefficients_encripted_table_h2o_th_glm_tot.csv"
)

coefficients_encripted_table_h2o_th_elastic_net_tot_best <- read_csv2(
  "data/practical_application/results/coefficients_encripted_table_h2o_th_elastic_net_tot_best.csv"
)

coefficients_encripted_table_h2o_th_ridge_tot_best <- read_csv2(
  "data/practical_application/results/coefficients_encripted_table_h2o_th_ridge_tot_best.csv"
)

coefficients_encripted_table_h2o_th_glm_aic <- read_csv2(
  "data/practical_application/results/coefficients_encripted_table_h2o_th_glm_aic.csv"
)

coefficients_encripted_table_h2o_th_elastic_net_aic_best <- read_csv2(
  "data/practical_application/results/coefficients_encripted_table_h2o_th_elastic_net_aic_best.csv"
)


coefficients <- coefficients_encripted_table_h2o_th_glm_tot %>% 
  select(
    1:2,
    coeff_glm_tot = coefficients,
    p_value_glm_tot = p_value
  ) %>% 
  full_join(
    coefficients_encripted_table_h2o_th_elastic_net_tot_best %>% 
      select(
        1:2,
        coeff_en_tot = coefficients
      )
  ) %>% 
  full_join(
    coefficients_encripted_table_h2o_th_ridge_tot_best %>% 
      select(
        1:2,
        coeff_ridge_tot = coefficients
      )
  ) %>% 
  full_join(
    coefficients_encripted_table_h2o_th_glm_aic %>% 
      select(
        1:2,
        coeff_glm_aic = coefficients,
        p_value_glm_aic = p_value
      )
  ) %>% 
  full_join(
    coefficients_encripted_table_h2o_th_elastic_net_aic_best %>% 
      select(
        1:2,
        coeff_en_aic = coefficients
      )
  )


coff_en_tot_0_count <- coefficients %>% 
  count(
    glm_tot_0 = is.na(coeff_glm_tot),
    en_tot_0 = coeff_en_tot == 0
  ) %>% 
  filter(!glm_tot_0, en_tot_0) %>% 
  .$n

coff_glm_aic_0_count <- coefficients %>% 
  count(
    glm_tot_0 = is.na(coeff_glm_tot),
    glm_aic_0 = is.na(coeff_glm_aic)
  ) %>% 
  filter(!glm_tot_0, glm_aic_0) %>% 
  .$n


```


#### GLM Tot

In the first GLM we considered all the `r explanatory_variables_number` explanatory variables listed in table \@ref(tab:explanatory-variables). Considering all the levels of the qualitative variables and all the polynomial terms for the quantitative components, it results into `r nrow(coefficients_encripted_table_h2o_th_glm_tot)` free parameters^[Within these `r nrow(coefficients_encripted_table_h2o_th_glm_tot)` parameters we are not counting the parameters corresponding to the base levels of the qualitative variables that in the GLM usual parametrization are set to exactly 0 and we are considering the intercept.].

The fitting in H2O has been quite fast and it took just `r format(results$time[1], digits = 2)` seconds.


#### Elastic Net Tot

```{r, en-tot-hp-tuning-read, echo = FALSE, cache = TRUE, message = FALSE}
h2o_th_elastic_net_tot_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_th_elastic_net_tot_grid_perf_tb.csv"
)

```

With the same parametrization of the first GLM, we fitted an Elastic Net. To choose the optimal parameters $\alpha$ and $\lambda$ we performed a grid search with a cross validation. In the cross validation we tested `r nrow(h2o_th_elastic_net_tot_grid_perf_tb)` models with a total computation time of `r str_extract(results$time_string[2], ".*m")`.

The figure \@ref(fig:en-tot-hp-tuning-plot) shows the Cross Validation Deviance for the different sets of hyper-parameters. As we can see, for each value of $\alpha$ the points draw a curve that starts high, decreases and, after an elbow, increases again. The light blue vertical line corresponds to the optimal set of hyper-parameters that are $\alpha = `r results$alpha[2]`$ and $\lambda = `r format(results$lambda[2], digits=3, scientific=T)`$.

In the plot there are some gaps between the points because, in order to reduce the execution time, the algorithm computed has been a random grid search, that means that just a random subset of the grid has been tested. The time constraint of `r str_extract(results$time_string[2], ".*m")` was an input of the algorithm.

One aspect we must underline is that, while the split between training set and test set has been made by policy id, the split into the sets of the cross validations has been made by observation. This implies that the hyper-parameter set that results from the cross validation could not be the optimal one for predicting the test set. This choice has been made because in the H2O base implementation it is not possible to specify an id for the splitting in the cross validation and we don't expect this aspect to deeply influence our results.

Within the `r nrow(coefficients_encripted_table_h2o_th_glm_tot)` free parameters of the GLM, in the optimal Elastic Net `r coff_en_tot_0_count` of them are shrunk to exactly 0.

(ref:en-tot-hp-tuning-caption) Elastic Net Tot hyper-parameter tuning. The light blue vertical line corresponds to the optimal point. The red vertical line corresponds to the optimal point subjected to $\alpha=0$, i.e. the Ridge Regression solution.

(ref:en-tot-hp-tuning-caption-short) Elastic Net Tot hyper-parameter tuning.

```{r, en-tot-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:en-tot-hp-tuning-caption)", fig.scap = "(ref:en-tot-hp-tuning-caption-short)", label = "en-tot-hp-tuning-plot", echo = FALSE, cache = TRUE, message = FALSE}

h2o_th_elastic_net_tot_grid_perf_tb %>% 
  filter(residual_deviance < 35575) %>% 
  ggplot(aes(x = lambda, y = residual_deviance, color = alpha)) +
  geom_vline(
    xintercept = h2o_th_elastic_net_tot_grid_perf_tb %>% 
      filter(alpha == "[0.0]") %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    # color = "red",
    color = hue_pal()(6)[1],
    alpha = 0.8
  ) +
  geom_vline(
    xintercept = h2o_th_elastic_net_tot_grid_perf_tb %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    # color = "blue",
    color = hue_pal()(6)[4],
    alpha = 0.8
  ) +
  geom_point(
    alpha = 0.8
  ) +
  scale_x_log10() +
  labs(
    x = expression(lambda),
    y = "Cross Validation Deviance",
    color = expression(alpha)
  )


```


#### Ridge Regression Tot

In figure \@ref(fig:en-tot-hp-tuning-plot), the red vertical line corresponds to the optimal point subjected to $\alpha=0$. This point corresponds to the Ridge Regression solution. In the Ridge regression all the coefficients are different from 0.



#### GLM AIC

Starting from the GLM with all the explanatory variables, we applied a stepwise algorithm based on AIC to find the optimal GLM. In the model comparison we decided to move only in backward direction, since considering at each step both the forward and the backward option would have resulted in a too long process.
<!-- and because from some trials we found that it would have probably resulted into the exact same model. -->

H2O doesn't support a function for stepwise regression, so we ran the algorithm in R with the function `stepAIC()` from the package `MASS`. Running the algorithm in R is much less efficient than in H2O and it took a total time of `r str_extract(results$time1_string[4], ".*m")`.


```{r, coeff-0-en-aic, echo = FALSE, cache = TRUE, message = FALSE}
coeff_0_en_aic_1 <- coefficients %>% 
  count(
    glm_tot_0 = is.na(coeff_glm_tot),
    en_tot_0 = coeff_en_tot == 0,
    glm_aic_0 = is.na(coeff_glm_aic)
  ) %>% 
  filter(en_tot_0, !glm_aic_0) %>% 
  .$n
  
coeff_0_en_aic_2 <- coefficients %>% 
  count(
    glm_tot_0 = is.na(coeff_glm_tot),
    en_tot_0 = coeff_en_tot == 0,
    glm_aic_0 = is.na(coeff_glm_aic)
  ) %>% 
  filter(!glm_tot_0, !en_tot_0, glm_aic_0) %>% 
  .$n
```

In the final GLM obtained with the stepwise algorithm, within the `r nrow(coefficients_encripted_table_h2o_th_glm_tot)` free parameters of the initial GLM, `r coff_glm_aic_0_count` of them have been removed from the model and only `r nrow(coefficients_encripted_table_h2o_th_glm_tot) - coff_glm_aic_0_count` lasted in the model. In table \@ref(tab:coeff-0) the number of parameters equal to 0 in each model is reported. The horizontal line separate the free parameters of the initial GLM from the parameters fixed to 0 in it. From this table we can see that there are `r coeff_0_en_aic_1` parameters that are set equal to 0 in the Elastic Net Tot and not in the GLM AIC and there are `r coeff_0_en_aic_2` that are set equal to 0 in the GLM AIC and not in the Elastic Net. That means that the two feature selection criteria lead to different subsets of the explanatory variables.

```{r coeff-0, echo = FALSE, cache = TRUE, message = FALSE}
coefficients %>% 
  # filter(variable_encripted != 0) %>% 
  count(
    glm_tot_0 = is.na(coeff_glm_tot),
    en_tot_0 = coeff_en_tot == 0,
    glm_aic_0 = is.na(coeff_glm_aic)
  ) %>% 
  group_by(glm_tot_0) %>% 
  mutate(n_tot = sum(n)) %>% 
  ungroup() %>% 
  mutate_if(
    is.logical,
    # as.numeric
    function(x){
      if_else(x, "$0$", "$\\ne0$")
    }
  ) %>% 
  mutate_at(
    .vars = c("glm_tot_0", "en_tot_0", "glm_aic_0", "n"),
    .funs = function(x){
      if_else(
        knitr::is_latex_output() & (row_number() %% 2 == 1),
        str_c("\\cellcolor{gray!6}{", x, "}"),
        as.character(x)
      )
    }
  ) %>% 
  mutate(
    n_tot = case_when(
      !knitr::is_latex_output() ~ as.character(n_tot),
      row_number() == 4 ~ str_c("\\multirow{-4}{*}{\\centering\\arraybackslash ", n_tot, "}"),
      row_number() == 6 ~ str_c("\\multirow{-2}{*}{\\centering\\arraybackslash ", n_tot, "}"),
      TRUE ~ ""
    )
  ) %>% 
  # mutate(
  #   glm_tot_0 = if_else(
  #     knitr::is_latex_output() & (row_number() %% 2 == 1),
  #     str_c("\\cellcolor{gray!6}{", glm_tot_0, "}"),
  #     as.character(glm_tot_0)
  #   ),
  #   en_tot_0 = if_else(
  #     knitr::is_latex_output() & (row_number() %% 2 == 1),
  #     str_c("\\cellcolor{gray!6}{", en_tot_0, "}"),
  #     as.character(en_tot_0)
  #   ),
  #   glm_aic_0 = if_else(
  #     knitr::is_latex_output() & (row_number() %% 2 == 1),
  #     str_c("\\cellcolor{gray!6}{", glm_aic_0, "}"),
  #     as.character(glm_aic_0)
  #   ),
  # ) %>% 
  rename(
    `GLM Tot` = glm_tot_0,
    `Elastic Net Tot` = en_tot_0,
    `GLM AIC` = glm_aic_0,
    ` ` = n_tot
  ) %>% 
  kable(
    digits = 0,
    format.args = list(big.mark = " "),
    format = "latex",
    booktabs = T,
    align = "cccc",
    vline = "",
    toprule = "\\toprule",
    # toprule = "\\cmidrule[\\heavyrulewidth]{1-4}",
    # midrule = "\\midrule",
    midrule = "\\midrule[\\heavyrulewidth]",
    # midrule = "\\cmidrule{1-4}",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    # bottomrule = "\\cmidrule[\\heavyrulewidth]{1-4}",
    caption = "Parameters equal to 0 in the models.",
    label = "coeff-0",
    escape = FALSE
  ) %>%
  kable_styling(
    position = "center",
    # latex_options = c("striped", "hold_position"),
    latex_options = c("hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>%
  # collapse_rows(
  #   column = 5,
  #   valign = "middle",
  #   # latex_hline = "none",
  #   # headers_to_remove = 5
  # ) %>%
  row_spec(0, bold = T) %>%
  # column_spec(2, italic = T)  %>%
  # landscape()
  row_spec(4, extra_latex_after = "\\midrule")
# row_spec(4, extra_latex_after = "\\cmidrule{1-4}")
  # row_spec(4, extra_latex_after = "\\cmidrule{1-4}")

```




#### Elastic Net AIC

```{r, en-aic-hp-tuning-read, echo = FALSE, cache = TRUE, message = FALSE}
h2o_th_elastic_net_aic_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_th_elastic_net_aic_grid_perf_tb.csv"
)

```

After fitting the GLM, we also fitted an Elastic Net using only the variables obtained with the stepwise selection. The figure \@ref(fig:en-aic-hp-tuning-plot) shows the Cross Validation Deviance for the different sets of the hyper-parameters. In the cross validation we tested `r nrow(h2o_th_elastic_net_aic_grid_perf_tb)` models with a total computation time of `r results %>% filter(id == "mod5") %>% .$time2_string %>% str_extract(".*m")`.

The red vertical line in figure \@ref(fig:en-aic-hp-tuning-plot) corresponds to the optimal set of hyper-parameters that are $\alpha = 0$ and
<!-- $\lambda = `r results$lambda[5]`$. -->
$\lambda = `r format(results$lambda[5], digits=3, scientific=T)`$. In this case the best Elastic Net model corresponds to the Ridge Regression. That means that no coefficient has been shrunk to exactly 0. This is probably due to the fact that the stepwise algorithm already selected a set of variables that have a significant effect for predicting the response. Moreover, comparing the Cross Validation Deviance of the optimal point and the points with lower $\lambda$, we see that there is no substantial difference. That means that a classic GLM would perform more or less the same as the optimal Ridge Regression.


(ref:en-aic-hp-tuning-caption) Elastic Net AIC hyper-parameter tuning.

(ref:en-aic-hp-tuning-caption-short) Elastic Net AIC hyper-parameter tuning.

```{r, en-aic-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:en-aic-hp-tuning-caption)", fig.scap = "(ref:en-aic-hp-tuning-caption-short)", label = "en-aic-hp-tuning-plot", echo = FALSE, cache = TRUE, message = FALSE}

h2o_th_elastic_net_aic_grid_perf_tb %>% 
  ggplot(aes(x = lambda, y = residual_deviance, color = alpha)) +
  geom_vline(
    xintercept = h2o_th_elastic_net_aic_grid_perf_tb %>% 
      filter(alpha == "[0.0]") %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    color = hue_pal()(6)[1],
    alpha = 0.8
  ) +
  geom_point(
    alpha = 0.8
  ) +
  scale_x_log10() +
  labs(
    x = expression(lambda),
    y = "Cross Validation Deviance",
    color = expression(alpha)
  )


```


#### Coefficients comparison

Considering all the GLM-based models we developed, we compared the estimated coefficients. The scatterplot and the correlation matrix of the coefficients are represented in figure \@ref(fig:coeff-pairs). Here we considered also the base levels of the qualitative explanatory variables that are set to exactly 0 by the classic GLM models and we excluded the intercept, resulting into `r nrow(coefficients_encripted_table_h2o_th_elastic_net_tot_best)-1` coefficients. In the plot and in the computation of the correlation coefficients we also excluded an anomalous point that highly influenced the correlation coefficients and highly reduced the readability of the plots. The coefficients estimated from each model for this anomalous point are shown in table \@ref(tab:anomalous-point)

```{r, anomalous-point-render, echo = FALSE, cache = TRUE, message = FALSE}
# Make Table
coefficients %>% 
  filter(variable_encripted == 2 & level_encripted == 4) %>% 
  select(contains("coeff")) %>% 
  pivot_longer(
    cols = everything(),
    names_to = "model"
  ) %>% 
  mutate(
    model = model %>% 
      str_replace_all("coeff_", "") %>% 
      str_replace_all("_", " ") %>% 
      str_to_title() %>% 
      str_replace_all("Glm", "GLM") %>% 
      str_replace_all("En", "EN") %>% 
      str_replace_all("Aic", "AIC")
  ) %>% 
  kable(
    digits = 3,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    # midrule = "\\hline\\addlinespace",
    # midrule = "\\hline",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "Coefficient of the anomalous point.",
    label = "anomalous-point",
    escape = FALSE
  ) %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>% 
  row_spec(0, bold = T) #%>% 
  # column_spec(2, italic = T)

```


As we can see from figure \@ref(fig:coeff-pairs), all the models have positively correlated coefficients. The most correlated are the Elastic Net Tot with the Ridge Tot and the GLM Tot with the GLM AIC. In the scatterplots the oblique line represented is the line $f(x)=x$ that helps us in understanding the different scales of the coefficients in the different models. As we can see, the GLM Tot has a much wider range of coefficients compared to the Elastic Net and the Ridge Regression, where the bigger coefficients have been shrunk to smaller values.


(ref:coeff-pairs-plot-caption) Coefficients comparison between GLM-based models.

(ref:coeff-pairs-plot-caption-short) Coefficients comparison between GLM-based models.

```{r, coeff-pairs-plot, out.width = "100%", fig.width = 9, fig.height = 6, fig.align='center', fig.cap = "(ref:coeff-pairs-plot-caption)", fig.scap = "(ref:coeff-pairs-plot-caption-short)", label = "coeff-pairs", echo = FALSE, cache = TRUE}

lowerfun <- function(data, mapping){
  ggplot(data = data, mapping = mapping) +
    geom_abline(intercept = 0, slope = 1, alpha = .8, color = "grey20") +
    geom_point(alpha = .2) #+
    # scale_x_continuous(limits = c(-2, 1.2)) +
    # scale_y_continuous(limits = c(-2, 1.2))
}


coefficients %>% 
  filter(
    variable_encripted != 0,
    !(variable_encripted == 2 & level_encripted == 4)
  ) %>% 
  select(contains("coeff")) %>% 
  mutate_all(
    function(x){
      if_else(is.na(x), 0, x)
    }
  ) %>% 
  rename_all(
    function(x){
      x %>% 
        str_replace_all("coeff_", "") %>% 
        str_replace_all("_", " ") %>% 
        str_to_title() %>% 
        str_replace_all("Glm", "GLM") %>% 
        str_replace_all("En", "EN") %>% 
        str_replace_all("Aic", "AIC")
    }
  ) %>% 
  ggpairs(
    # lower = list(continuous = wrap("points", alpha = 0.2)),
    lower = list(continuous = wrap(lowerfun)),
  )


```


#### GAM AIC

As for the GLM and the Elastic Net, we also fitted a gam using the variables obtained with the stepwise selection. As already mentioned, due to the still experimental state of the GAM modeling in H2O, we fitted the GAM in R and we just took a quick trial without a deep hyper-parameters tuning. The fitting of one single GAM in R took `r results %>%  filter(id == "mod6") %>% .$time2_string`, that means that a deep hyper-parameters tuning would have taken many hours to run.


#### GBM Tot

```{r, gbm-tot-hp-tuning-read, echo = FALSE, cache = TRUE, message = FALSE}
h2o_gbm_tot_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_gbm_tot_grid_perf_tb.csv"
)
```

Finally, we fitted a GBM with all the `r explanatory_variables_number` variables available. The GBM is one of the most widely used general purpose machine learning models. One of its befits is that it automatically performs the feature selection and automatically considers the interactions.

The GBM have some hyper-parameters that are: `ntrees`, `max_depth`, `learn_rate`, `sample_rate` and `col_sample_rate`. To understand what these parameters are we refer to the [H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html). For selecting the best set of variables we computed a grid search with a Cross Validation. Figure \@ref(fig:gbm-tot-hp-tuning-plot) shows the Cross Validation Deviance for the different sets of hyper-parameters. The optimal set of hyper-parameters is:
`ntrees` = `r results %>% filter(id == "mod7") %>% .$ntrees`,
`max_depth` = `r results %>% filter(id == "mod7") %>% .$max_depth`,
`learn_rate` = `r results %>% filter(id == "mod7") %>% .$learn_rate`,
`sample_rate` = `r results %>% filter(id == "mod7") %>% .$sample_rate` and
`col_sample_rate` = `r results %>% filter(id == "mod7") %>% .$col_sample_rate`.

In total, `r nrow(h2o_gbm_tot_grid_perf_tb)` set of hyper-parameters have been tested in a total amount of time of `r results %>% filter(id == "mod7") %>% .$time_string %>% str_extract(".*m")`.


(ref:gbm-tot-hp-tuning-caption) GBM Tot hyper-parameter tuning.

(ref:gbm-tot-hp-tuning-caption-short) GBM Tot hyper-parameter tuning.

```{r, gbm-tot-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:gbm-tot-hp-tuning-caption)", fig.scap = "(ref:gbm-tot-hp-tuning-caption-short)", label = "gbm-tot-hp-tuning-plot", echo = FALSE, cache = TRUE, message = FALSE}

h2o_gbm_tot_grid_perf_tb %>% 
  ggplot(aes(x = learn_rate, y = residual_deviance, color = max_depth)) +
  geom_vline(
    xintercept = h2o_gbm_tot_grid_perf_tb %>%
      filter(residual_deviance == min(residual_deviance)) %>%
      select(learn_rate) %>%
      as_vector(),
    color = viridis_pal()(10)[8],
    alpha = .8
  ) +
  geom_point(
    alpha = .8
  ) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_viridis(direction = -1) +
  coord_cartesian(ylim = c(NA, 0.22)) +
  labs(
    x = "Learn rate",
    y = "Cross Validation Deviance",
    color = "Max depth"
  )

```




<!-- \newpage -->

## Results {#chap:results}

In table \@ref(tab:models-results) the results of the models are reported. The Test Deviance is also represented in figure \@ref(fig:models-results-plot). As we can see, the best model in term of Test Deviance has been the Ridge Regression with all the variables (Ridge Tot). The fact that the Ridge Regression performed the best, means that also in the variables that the Elastic Net and the stepwise based on AIC set to exactly 0 there is some useful information. As expected, the GAM performed much worse than the other models. Further experiments should be conducted to understand whether in general the GAM performs worse on this data or with a proper hyper-parameter tuning it could perform as the GLM or even better. One unexpected result is the worse performance of the GBM compared to GLM-based models. As the GBM was free to also consider the interactions, we expected it to outperform the GLM-based models. Probably in our data the interaction terms don't bring too much information to the model.

In table \@ref(tab:models-results) the execution time is also reported. For the models based on AIC the reported time is given by the sum of the execution time of the stepwise algorithm (`r results %>% filter(model == "glm aic") %>% .$time1_string %>% str_extract(".*m")`) and the execution time of the final fitting. Comparing the models by the execution time, we see that running an Elastic Net on the whole dataset is much more efficient than running a stepwise algorithm based on AIC. This difference in efficiency would get even bigger by increasing the number of variables in the model, as the complexity of the stepwise algorithm grows exponentially with the number of explanatory variables.

Conducting a cost-benefit analysis with these results, we can say that, with this implementation setting, the increasing of execution time of the GLM advancements is huge and, even if it is just machine time and not human time dedicated to a manual work, it could not be justified by the small decrease of deviance. Anyway, adopting solutions with higher performance, such us the cluster computing, this trade off could considerably move towards the more sophisticated techniques.


```{r, models-results-render, echo = FALSE, cache = TRUE, message = FALSE}

# Make Table
tab <- results %>%
  select(
    id, model, test_deviance,
    time_string,
    # time1_string, time2_string,
    alpha, lambda,
    # ntrees, max_depth, learn_rate,
    # sample_rate, col_sample_rate
  ) %>%
  mutate(
    id = str_to_title(id),
    model = model %>%
      str_to_title() %>%
      str_replace_all("Aic", "AIC") %>%
      str_replace_all("Glm", "GLM") %>%
      str_replace_all("Gam", "GAM") %>%
      str_replace_all("Gbm", "GBM"),
    time_string = if_else(
      str_detect(time_string, "m"),
      str_extract(time_string, ".*m"),
      time_string,
    ),
    alpha = if_else(
      alpha == 0, "0",
      format(alpha, digits = 2)
    ),
    lambda = if_else(
      lambda == 0, "0",
      format(lambda, digits = 3)
    )
  ) %>% 
  mutate(
    test_deviance = test_deviance %>% 
      format(big.mark = " ") %>% 
      if_else(
        test_deviance == min(test_deviance),
        cell_spec(., bold = T),
        .
      )
  ) %>% 
  # mutate(
  #   test_deviance = format(test_deviance, big.mark = " ")
  # ) %>% 
  # format_cell(3, 3, "bold") %>%
  rename_all(
    .funs = function(x){
      str_to_title(x) %>% 
        str_replace_all("_string", "") %>% 
        str_replace_all("_", " ")
    }
  ) %>%
  rename(
    "$\\alpha$" = Alpha,
    "$\\lambda$" = Lambda,
    `\\makecell[c]{Test\\\\Deviance}` = `Test deviance`
  ) 

# For compatibility with HTML
if(!knitr::is_latex_output()){
  names(tab) <- names(tab) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
  
  names(tab) <- names(tab) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}


tab %>%
  kable(
    digits = c(0,0,3,0,4,5),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "llccccccccccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    # midrule = "\\hline\\addlinespace",
    # midrule = "\\hline",
    midrule = "\\midrule[\\heavyrulewidth]",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "Models results. In the columns we reported the deviance computed in the test set, the execution time and, for the GLM-based models, the hyper-parameters.",
    caption.short = "Models results.",
    label = "models-results",
    escape = FALSE
    # escape = TRUE
  ) %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>%
  # row_spec(3, bold = T) %>%
  row_spec(0, bold = T) #%>%
  # column_spec(2, italic = T)  %>%
  # landscape()

```



(ref:models-results-caption) Deviance computed in the test set for the models considered.

(ref:models-results-caption-short) Deviance computed in the test set for the models considered.

```{r, models-results-plot, out.width = "100%", fig.width = 6, fig.height = 3, fig.align='center', fig.cap = "(ref:models-results-caption)", fig.scap = "(ref:models-results-caption-short)", label = "models-results-plot", echo = FALSE, cache = TRUE}
results %>% 
  mutate(
    id = str_c(
      "mod", row_number()
    )
  ) %>% 
  select(
    id, model, test_deviance
  ) %>% 
  # filter(!(model %in% c("gam aic", "gbm tot"))) %>%
  mutate(
    id = str_to_title(id),
    model = model %>% 
      str_to_title() %>% 
      str_replace_all("Aic", "AIC") %>% 
      str_replace_all("Glm", "GLM") %>% 
      str_replace_all("Gam", "GAM") %>% 
      str_replace_all("Gbm", "GBM"),
    model = model %>% 
      fct_inorder() %>% 
      fct_rev()
  ) %>% 
  ggplot(aes(x = model, y = test_deviance)) +
  geom_segment(
    aes(xend = model, yend = -Inf),
    # size = 1
  ) +
  geom_point(
    # size = 4
  ) +
  coord_flip(ylim = c(NA, 8458.5)) +
  ylim(8456, NA) +
  labs(x = "", y = "Test Deviance")
```



\newpage

## Conclusions and possible improvements {#chap:conclusions}

In conclusion, we can say that with our exploration we found out that the GLM Advancements can bring an improvement in performance compared to basic GLM implementations, still maintaining all the benefits of high interpretability and high control of the effects of the variables.

One interesting improvement of the Elastic Net models would be to consider different penalizations for the different variables as described in section \@ref(chap:bayes-glm). This approach can not only improve the performance of the models, but would also allow us to systematically introduce prior information in a Bayesian fashion and better control the variables effects.

Another improvement would be exploring the interaction effects in the GLM-based models. On this topic, the GBM fitting could be a nice starting point to get some insights on the interactions and guide the manual introduction of interaction terms in the GLM-based models. GBM models are particularly convenient for these kind of tasks because they work mostly automatically and the manual intervention is minimal, so they can be launched without too much human effort.

Considering the GAM, to better understand their potential, further explorations should be conducted. We hope for the GAM development in H2O to progress. Otherwise we can still try other implementations.

A really important topic in modeling that has not been faced in this analysis and deserves a separate dissertation is the geographical modeling. The geographical modeling is really important in Non-Life Insurance Pricing and requires specific modeling tools that haven't be discussed in this thesis. We remind that in our analysis the choice of using policies from only one specific province strongly reduces the importance of geographical modeling.

The other aspect of considering policies from only one province is that this choice reduces significantly the size of the dataset. This solution allowed us to work locally on one single PC without going out of memory and with still acceptable execution times, but it is clear that with a whole country usual \ac{mtpl} dataset working locally on a consumer PC would not be possible. The proper solution would be to adopt a cluster of computers big enough to deal with the whole dataset. As we discussed in section \@ref(chap:implementation), the cluster computing offers a highly scalable solution and H2O can be easily exploited in a cluster.



