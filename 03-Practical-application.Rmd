---
output:
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::html_document2: default
documentclass: book
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
library(knitr)
```

<!-- Needed for leaving space to the quote, * is for no indentation after title --> 

<!-- \titlespacing*{\chapter}{0pt}{80px}{35pt} -->

# Practical application {#chap:practical-app}
\minitoc  <!--this will include a mini table of contents-->

\chaptermark{Practical application}

In this final chapter we are going to describe the practical application on an actuarial dataset of the models described in section \@ref(chap:models). After describing the problem and how the models have been implemented, we will assess their performance and comment the results.


## Data description

### Dataset

```{r, years_at_risk_read, echo = FALSE, cache = TRUE, message = FALSE}
tab_years_at_risk <- read_csv2("data/practical_application/tab_years_at_risk.csv")
```

The dataset used is an actuarial dataset from an Italian MTPL portfolio. In table \@ref(tab:years-at-risk) the total exposure of the dataset is reported. In the dataset there are `r tab_years_at_risk$n` rows for a total of `r round(tab_years_at_risk$rischi_anno, 1)` years-at-risk. In the dataset, every row is a couple (policy, accounting year), so, if a policy spans in two years, we will see two rows corresponding to that policy. On average, every row has exposure `r round(tab_years_at_risk$durata_media, 3)` years-at-risk. This number is slightly lower than 0.5 because some of the policies get suspended and span in more than two years.

```{r, years_at_risk_render, echo = FALSE, cache = TRUE, message = FALSE}
tab_years_at_risk %>% 
  rename(
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `\\makecell[c]{Average Exposure\\\\per Observation}` = durata_media
  ) %>% 
  kable(
    digits = c(0, 1, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "ccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Total dataset exposure. The exposure is measured in year-at-risk (Y.a.R)",
    caption.short = "Total dataset exposure.",
    label = "years-at-risk",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
```


The dataset contains policies from the period 2014-2019 coming from a specific province. The fact that we filtered the policies from only one province reduces a lot the size of the dataset and, since the models has been run locally on a single computer, this significantly reduces the time spent to run them. From a modeling point of view, the reduction to only one province reduces the importance of the geography in the models. Indeed, between the different regions of Italy there are a lot of socio-economical differences that are reflected to the claims experience. In this thesis the problem of using geographical data as explanatory variables has not been addressed.


```{r, description_train_test_read, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test <- read_csv2(
  "data/practical_application/tab_freq_sin_train_test.csv"
)

tab_1 <- tab_freq_sin_train_test %>% 
  select(
    set, n, rischi_anno, n_clienti, rischi_anno_per_cliente
  ) %>% 
  mutate(set = str_to_title(set)) %>%
  rename(
    Set = set,
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `Policyholders` = n_clienti,
    `\\makecell[c]{Exposure per\\\\Policyholder}` = rischi_anno_per_cliente,
  ) 


tab_2 <- tab_freq_sin_train_test %>% 
  select(
    set, n, rischi_anno, numero_causati, freq_sin
  ) %>% 
  mutate(set = str_to_title(set)) %>%
  rename(
    Set = set,
    Observations = n,
    `Exposure (Y.a.R.)` = rischi_anno,
    `Claims N.` = numero_causati,
    `Claims Freq.` = freq_sin
  )

# For compatibility with HTML
if(!knitr::is_latex_output()){
  names(tab_1) <- names(tab_1) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
  
  names(tab_2) <- names(tab_2) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}
```

The dataset has been split into a training set with 80% of the observations and a test set with the remaining 20% of the observations. The training set has been used to fit the models, while the test set has been be used to assess them by comparing the predictions of the models with the observed data. The splitting has been made on the base of the policyholder id. That means that policies from the same policyholder end in the same set. This has been done because policies from the same policyholder are correlated, so splitting the observations by policyholder prevents data leakage from the test set to the training set. Table \@ref(tab:exposure-train-test) shows the exposure and the number of policyholders in the training set and the test set. As we can see, on average, each policyholder has `r round(sum(tab_freq_sin_train_test$rischi_anno) / sum(tab_freq_sin_train_test$n_clienti), 1)` years-at-risk.


```{r, description_train_test_render_1, echo = FALSE, cache = TRUE, message = FALSE}

tab_1 %>% 
  kable(
    digits = c(0, 0, 1, 0, 2),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Exposure and number of policyholders in training and test set. The exposure is measured in year-at-risk (Y.a.R)",
    caption.short = "Exposure and number of policyholders in training and test set.",
    label = "exposure-train-test",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)

```

### Response variable

The response variable for the models is the number of claims caused by the policyholder for that policy in that accounting year. In this project we compared the different techniques in predicting the claims frequency. We didn't take into account the claim severity and we didn't split the claims by typology (see section \@ref(chap:response-variables-and-distributions)).

Table \@ref(tab:freq-sin-train-test) shows the observed claims frequency in the training set and in the test set. As we can see, the claims frequency in the training set is slightly higher than the claims frequency in the test set. The difference between the two sets is given by chance, but it is also encouraged by the fact that the policies has been split by policyholder id and not by policy. The split by policyholder id implicates that, if a policyholder has many policies, all of them fall into the same set, so the hypothesis of independence for the observations within the sets is not respected. Anyway this issue is restrained and in this analysis has been neglected, as it is usually done in actuarial practice. We also mention that theoretically the difference in claims frequency in the two sets should be mostly explained by the explanatory variables we have, so the models fitted with the training set should be able to catch the characteristics of the policies of the test set that reduce their expected claims frequency.

```{r, description_train_test_render_2, echo = FALSE, cache = TRUE, message = FALSE}
tab_2 %>% 
  kable(
    digits = c(0, 0, 1, 0, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Claims Frequency in training and test set.",
    label = "freq-sin-train-test",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)

```

As we mentioned in section \@ref(chap:model-fitting-and-data-available), the claims settlement is a long process and the response variable we use for fitting the models depends on the moment in which the claims are observed. Indeed it is possible that part of the claims we are considering as response variables will be closed as null-claims and it is possible that there are claims occurred in the period of exposure considered that are not yet reported (IBNyR) at the moment in which the claims have been observed. This phenomenon is more relevant if the period of exposure is close to the moment of observation of the claims.

The claims reported in the dataset are all observed at the date 30/06/2020. In figure \@ref(fig:freq-sin-train-test-year) the claims frequency for each year in the training set and in the test set is reported. The same data is reported in table \@ref(tab:freq-sin-train-test-year). As we can see the years 2017, 2018 and 2019 present a claims frequency lower than the years 2014, 2015 and 2018, so it looks like there is a structure in the claims frequency over the years. Unfortunately, just by looking to this data we can't say whether this is due to the different settlement of the claims over the years, to a different mix of policies or just to a overall claims trend in the whole market. Anyway, looking to the confidence intervals we realize that this is not a big issue as the effect of the year is not too heavy.

Comparing the claims frequency of the training set and the test set, we see that the claims frequency in the test set is consistently lower than the claims frequency in the training set. This phenomenon can be explained with the fact that we divided the policies based on policyholder id, so the policies within each set are correlated.

```{r, freq-sin-train-test-year-read, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test_year <- read_csv2(
  "data/practical_application/tab_freq_sin_train_test_year.csv"
)
```


(ref:freq-sin-train-test-year-plot-caption) Claims frequency in training and test set in the different years. The semi-transparent ribbons represent 95% confidence intervals for the claims frequency in each year.

(ref:freq-sin-train-test-year-plot-caption-short) Claims frequency in training and test set in the different years.

```{r, freq-sin-train-test-year-plot, out.width = "100%", fig.width = 9, fig.height = 4, fig.align='center', fig.cap = "(ref:freq-sin-train-test-year-plot-caption)", fig.scap = "(ref:freq-sin-train-test-year-plot-caption-short)", label = "freq-sin-train-test-year", echo = FALSE, cache = TRUE}
tab_freq_sin_train_test_year %>% 
  mutate(
    anno_analisi = factor(anno_analisi),
    set = fct_inorder(set)
  ) %>%
  ggplot(aes(x = anno_analisi, y = freq_sin, color = set, fill = set, group = set)) +
  facet_wrap(~set) +
  geom_line() +
  geom_point() +
  geom_ribbon(aes(ymin = down, ymax = up),
              alpha = .2) +
  labs(x = "Accounting Year", y = "Claims Frequency") +
  easy_remove_legend()
```

```{r, freq-sin-train-test-year-render, echo = FALSE, cache = TRUE, message = FALSE}
tab_freq_sin_train_test_year %>% 
  select(set, anno_analisi, freq_sin) %>% 
  pivot_wider(
    names_from = anno_analisi,
    values_from = freq_sin
  ) %>% 
  rename(Set = set) %>% 
  kable(
    digits = c(0, 3, 3, 3, 3, 3, 3),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Claims frequency in training and test set in the different years.",
    label = "freq-sin-train-test-year",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)

```


#### Exploration on the effect of splitting data by policyholder

To explore more in depth the effect of splitting data into training and test set by observation or by policyholder, we conducted a simulation analysis. We took the whole dataset and we simulated for each observation a random flag that represents whether the observation belongs to the training set or to the test set with 80% probability of belonging to the training set and 20% probability of belonging to the test set. We repeated the simulation 10k times with a random variable based on observation and 10k times with a random variable based on policyholder. In each simulation, we fitted a GLM with only that flag as explanatory variable and the number of claims as response variable. Then, for each simulation, we looked at the p-value of the test over the significance of that flag. This p-value can be seen as a measure for checking whether the training and the test sets have a significative difference in term of claim frequency.

```{r, simulation-random-read, echo = FALSE, cache = TRUE, message = FALSE}
simulations <- read_csv2(
  "data/practical_application/random_simulations_pvalue.csv"
)

simulations_count <- simulations %>% 
  mutate(
    p_value_cut = cut_width(
      p_value, width = .05,
      # closed = "right",
      closed = "left",
      boundary = 0
      # boundary = 50
    ),
    random_type = if_else(
      random_type == "c",
      "Random by policyholder",
      "Random by observation"
    ) %>% 
      fct_inorder()
  ) %>% 
  group_by(random_type) %>% 
  count(p_value_cut) %>% 
  mutate(
    freq = n/sum(n),
    density = freq / 0.05
  ) %>% 
  ungroup()
```

The distribution of the p-values for the two kinds of random variables are reported in figure \@ref(fig:simulation-random). The frequency for the first 4 buckets are also reported in table \@ref(tab:simulation-random). In theory, if the variable was totally random, the distribution of the p-values should be uniform. As we can see, the distribution of the p-values for the random by policyholder is not uniform and the lower p-values are more likely. Indeed, in our simulation, the observed frequency of the p-values lower than 0.05 with the random by policyholder is `r simulations_count %>% filter(random_type == "Random by policyholder", p_value_cut == "[0,0.05)") %>% .$freq`, that is quite higher than 0.05. Anyway, we accept this potential difference in the claims frequency between the training set and the test set and we keep the splitting made with the random by policyholder.


(ref:simulation-random-plot-caption) Distribution of the p-values from the simulation of the random variables. With the random by policyholder, the lower p-values are more likely.

(ref:simulation-random-plot-caption-short) Distribution of the p-values from the simulation of the random variables.

```{r, simulation-random-plot, out.width = "100%", fig.width = 9, fig.height = 4, fig.align='center', fig.cap = "(ref:simulation-random-plot-caption)", fig.scap = "(ref:simulation-random-plot-caption-short)", label = "simulation-random", echo = FALSE, cache = TRUE}
simulations %>% 
  mutate(
    random_type = if_else(
      random_type == "c",
      "Random by policyholder",
      "Random by observation"
    ) %>% 
      fct_inorder()
  ) %>% 
  ggplot(aes(x = p_value, fill = random_type, color = random_type)) +
  geom_histogram(
    # binwidth = .05
    # bins = seq(from = -0.01, to = 1.01, by = .05)
    aes(y = ..density..),
    breaks = seq(0, 1, b = 0.05),
    alpha = .5
  ) +
  facet_wrap(~random_type) +
  easy_remove_legend() +
  labs(
    x = "p-value", y = "Density"
  )
```


```{r, simulation-random-render, echo = FALSE, cache = TRUE, message = FALSE}
simulations_count %>% 
  filter(as.numeric(p_value_cut) < 5) %>% 
  select(random_type, p_value_cut, freq) %>%
  mutate(
    p_value_cut = p_value_cut %>% 
      as.character() %>% 
      str_replace_all(",", ", ")
  ) %>% 
  pivot_wider(
    names_from = p_value_cut,
    values_from = freq
  ) %>% 
  rename(
    `Random type` = random_type
  ) %>% 
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lcccc",
    vline = "",
    toprule = "\\toprule", midrule = "\\toprule\\addlinespace",
    linesep = "\\addlinespace\\hline\\addlinespace", bottomrule = "\\bottomrule",
    caption = "Distribution of the p-values from the simulation of the random variables. With a random variable, these frequencies should be around 0.05. With the random by policyholder, the frequencies for low p-values are higher than that.",
    caption.short = "Distribution of the p-values from the simulation of the random variables.",
    label = "simulation-random",
    escape = FALSE
  ) %>% 
  kable_styling(
    position = "center",
    latex_options = "hold_position",
    full_width = FALSE
  ) %>% 
  row_spec(ifelse(!knitr::is_latex_output(), 0, 1), bold = T)
```







### Explanatory variables

```{r, explanatory-variables-read, echo = FALSE, cache = TRUE, message = FALSE}
explantory_varibles <- read_csv2(
  "data/practical_application/tab_explanatory_variables.csv"
)


# Sum the last row of each column if numeric 
func <- function(z){ if (is.numeric(z)) sum(z) else '' }
sumrow <- as_tibble(lapply(explantory_varibles, func))

# Give name to the first element of the new data frame created above
sumrow["Description"] <- "Total"

# Add the original and new data frames together
explantory_varibles_total <- bind_rows(explantory_varibles, sumrow)

explanatory_variables_number <- explantory_varibles_total %>%
  filter(Description == "Total") %>%
  .$`Number of variables per category`


```

In the dataset we have a total of `r explanatory_variables_number` explanatory variables that are considered meaningful for predicting the claims frequency. In table \@ref(tab:explanatory-variables) we can see the number of variables split by category. The categories are the ones described in section \@ref(chap:pricing-variables).

In the modeling process we will assess whether these variables are significative or not and only some of them will be used to predict the response variable.

```{r, explanatory-variables-render, echo = FALSE, cache = TRUE, message = FALSE}

# Make Table
explantory_varibles_total %>% 
  select(-group, -category) %>% 
  rename(
    `\\makecell[c]{Number of variables\\\\per category}` = `Number of variables per category`
  ) %>% 
  # kable(caption = "Normalized Annual Energy Demand", booktabs = TRUE) %>%
  # kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  # row_spec(nrow(explantory_varibles_total), bold = T) %>% # format last row
  # column_spec(1, italic = T) # format first column
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "lc",
    vline = "",
    toprule = "\\toprule",
    midrule = "\\midrule",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "Number of explanatory variables per category.",
    label = "explanatory-variables",
    escape = FALSE
  )  %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>% 
  row_spec(0, bold = T) %>%
  # row_spec(
  #   ifelse(
  #     !knitr::is_latex_output(),
  #     nrow(explantory_varibles_total),
  #     nrow(explantory_varibles_total) + 1
  #   ),
  #   bold = T,
  # ) %>% 
  row_spec(nrow(explantory_varibles_total) - 1,
           hline_after = TRUE) %>%
  row_spec(nrow(explantory_varibles_total),
           bold = T) %>%
  column_spec(1, italic = T)
```


\newpage

## Models assessment

The splitting of the dataset into training set and test set allow us to use the training set to fit the model and the test set to assess them by comparing the predictions
of the models with the observed data.

The metric adopted to assess the performance of each model is the Poisson deviance (see section \@ref(chap:glm-model-fitting) for more details):
$$
D(\hat{\boldsymbol{\beta}}, \boldsymbol{y}) = 2\,\sum_{i=1}^{n}{\left\{ y_i \log{\left(\frac{y_i}{\hat{\mu}_i}\right)} - \left( y_i - \hat{\mu}_i \right) \right\}}
$$

The lower the deviance on the test set is, the better the model is.


\newpage

## Models description

### List of models

In table \@ref(tab:models-list) the models developed are listed. We started from a GLM that takes into account all the variables available (_GLM Tot_). Then, with the same variables, we considered an Elastic Net (_Elastic Net Tot_) and a Ridge regression (_Ridge Tot_). After this we computed a feature selection with a stepwise approach based on AIC obtaining a GLM with just a subset of the available variables (_GLM AIC_). With the same subset of variables we also fitted an Elastic Net (_Elastic Net AIC_) and a GAM (_GAM AIC_). Finally, to have a comparison with a widely used general purpose machine learning model, we fitted a GBM with all the variables available (_GBM Tot_).


```{r, models-results-read, echo = FALSE, cache = TRUE, message = FALSE}
results <- read_csv2(
  "data/practical_application/results/results.csv"
)

results <- results %>% 
  # select(time, time1, time2) %>%
  mutate(
    time_period = seconds_to_period(time),
    time_hour = hour(time_period),
    time_minute = minute(time_period),
    time_second = second(time_period),
    time_string = str_c(
      if_else(time_hour > 0, str_c(time_hour, "h "), ""),
      if_else(time_hour > 0 | time_minute > 0, str_c(time_minute, "m "), ""),
      format(time_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time_period, time_hour, time_minute, time_second)
  ) %>% 
  mutate(
    time1_period = seconds_to_period(time1),
    time1_hour = hour(time1_period),
    time1_minute = minute(time1_period),
    time1_second = second(time1_period),
    time1_string = str_c(
      if_else(time1_hour > 0, str_c(time1_hour, "h "), ""),
      if_else(time1_hour > 0 | time1_minute > 0, str_c(time1_minute, "m "), ""),
      format(time1_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time1_period, time1_hour, time1_minute, time1_second)
  ) %>% 
  mutate(
    time2_period = seconds_to_period(time2),
    time2_hour = hour(time2_period),
    time2_minute = minute(time2_period),
    time2_second = second(time2_period),
    time2_string = str_c(
      if_else(time2_hour > 0, str_c(time2_hour, "h "), ""),
      if_else(time2_hour > 0 | time2_minute > 0, str_c(time2_minute, "m "), ""),
      format(time2_second, digits = 1, nsmall = 1), "s"
    )
  ) %>% 
  select(
    -c(time2_period, time2_hour, time2_minute, time2_second)
  ) %>% 
  mutate(
    id = str_c(
      "mod", row_number()
    )
  ) %>% 
  select(
    id, everything()
  )


```


```{r, models-list-render, echo = FALSE, cache = TRUE, message = FALSE}
# Make Table
results %>%
  select(id, model) %>% 
  mutate(
    id = str_to_title(id),
    model = model %>% 
      str_to_title() %>% 
      str_replace_all("Aic", "AIC") %>% 
      str_replace_all("Glm", "GLM") %>% 
      str_replace_all("Gam", "GAM") %>% 
      str_replace_all("Gbm", "GBM")
  ) %>% 
  rename_all(
    .funs = str_to_title
  ) %>% 
  kable(
    digits = 4,
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "ll",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    # midrule = "\\hline\\addlinespace",
    # midrule = "\\hline",
    midrule = "\\midrule",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "List of models developed.",
    label = "models-list",
    escape = FALSE
  ) %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>% 
  row_spec(0, bold = T) #%>% 
  # column_spec(2, italic = T)

```




All the models have been fitted with H2O, except for the GAM. On H2O the following functions are available:

* [`h2o.glm()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html) that allows to build GLMs with Elastic Net penalization;
* [`h2o.gam()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gam.html) that allows to build GAMs with both GAM penalization to splines and Elastic Net penalization to other variables;
* [`h2o.gbm()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html) that allows to build GBMs.

Anyway, as described in H2O documentation, "GAM models are currently experimental". We had some trials with GAMs on H2O, but, when we are writing, there are still many bugs and many functions for the hyperparameter tuning are not available yet. For this reason, for the GAM model we used the function `gam()` from the package `mgcv`.

The big advantage of H2O is that it provides embedded multi-threading implementation and provides built-in functions for hyper-parameter tuning. In the practice, that is translated to a high-performance solution that is substantially faster and is much more memory-efficient than what the other R packages offer.

Due to the lower efficiency and the lack of hyper-parameter tuning functions, we used the function from `mgcv` just as a quick trial, without performing a deep hyper-parameter tuning.

More details on the models listed in table \@ref(tab:models-list) are described more in depth in section \@ref(chap:models-details).


### Approach adopted in the modeling

In the whole modeling process we tried to keep an approach the more objective as possible in order to not condition the results.

One of the strengths of the GLM-based methods is that they allows to extensively introduce external information in the process of fitting. That means that, still using the same techniques, a more skilled modeler is able to build a better model than a less skilled one. 

As the aim of our analysis is to built a comparison just between the models, the skills of the modeler would be a disturbance factor. For this reason we adopted techniques as much objective as possible. We will return on some considerations of the benefits of the modeler intervention in section \@ref(chap:results).

For this reason, we performed the feature selection procedure in the GLM with a stepwise algorithm based on AIC instead than with a manual process.

The only manual element in the model fitting has been the definition of the effects of the quantitative variables. For the quantitative variables we considered split-wise polynomials effects based on the plots of the residuals. Anyway we have been quite generous in the number of terms taken into account and we let the algorithm choosing which of them to keep in the model.

For the interactions, we decided not to consider any interaction term in the GLM-based models. This choice comes from the fact that from our exploration there were not particularly important interactions between the variables and from the fact that, in GLM-based models, the choices of interactions terms is usually a manual process and the ability of the modeler influences a lot the result, introducing a disturbance factor.



\newpage

### Models details {#chap:models-details}


(ref:en-tot-hp-tuning-caption) Elastic Net Tot hyper-parameter tuning.

(ref:en-tot-hp-tuning-caption-short) Elastic Net Tot hyper-parameter tuning.

```{r, en-tot-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:en-tot-hp-tuning-caption)", fig.scap = "(ref:en-tot-hp-tuning-caption-short)", label = "en-tot-hp-tuning-plot", echo = FALSE, cache = FALSE, message = FALSE}

h2o_th_elastic_net_tot_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_th_elastic_net_tot_grid_perf_tb.csv"
)

h2o_th_elastic_net_tot_grid_perf_tb %>% 
  filter(residual_deviance < 35575) %>% 
  ggplot(aes(x = lambda, y = residual_deviance, color = alpha)) +
  geom_vline(
    xintercept = h2o_th_elastic_net_tot_grid_perf_tb %>% 
      filter(alpha == "[0.0]") %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    # color = "red",
    color = hue_pal()(6)[1],
    alpha = 0.8
  ) +
  geom_vline(
    xintercept = h2o_th_elastic_net_tot_grid_perf_tb %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    # color = "blue",
    color = hue_pal()(6)[4],
    alpha = 0.8
  ) +
  geom_point(
    alpha = 0.8
  ) +
  scale_x_log10() +
  labs(
    x = expression(lambda),
    y = "Cross Validation Deviance",
    color = expression(alpha)
  )


```


(ref:en-aic-hp-tuning-caption) Elastic Net AIC hyper-parameter tuning.

(ref:en-aic-hp-tuning-caption-short) Elastic Net AIC hyper-parameter tuning.

```{r, en-aic-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:en-aic-hp-tuning-caption)", fig.scap = "(ref:en-aic-hp-tuning-caption-short)", label = "en-aic-hp-tuning-plot", echo = FALSE, cache = FALSE, message = FALSE}

h2o_th_elastic_net_aic_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_th_elastic_net_aic_grid_perf_tb.csv"
)

h2o_th_elastic_net_aic_grid_perf_tb %>% 
  ggplot(aes(x = lambda, y = residual_deviance, color = alpha)) +
  geom_vline(
    xintercept = h2o_th_elastic_net_aic_grid_perf_tb %>% 
      filter(alpha == "[0.0]") %>% 
      filter(residual_deviance == min(residual_deviance)) %>% 
      .$lambda,
    color = hue_pal()(6)[1],
    alpha = 0.8
  ) +
  geom_point(
    alpha = 0.8
  ) +
  scale_x_log10() +
  labs(
    x = expression(lambda),
    y = "Cross Validation Deviance",
    color = expression(alpha)
  )


```




(ref:gbm-tot-hp-tuning-caption) GBM Tot hyper-parameter tuning.

(ref:gbm-tot-hp-tuning-caption-short) GBM Tot hyper-parameter tuning.

```{r, gbm-tot-hp-tuning-plot, out.width = "100%", fig.width = 7, fig.height = 3.5, fig.align='center', fig.cap = "(ref:gbm-tot-hp-tuning-caption)", fig.scap = "(ref:gbm-tot-hp-tuning-caption-short)", label = "gbm-tot-hp-tuning-plot", echo = FALSE, cache = FALSE, message = FALSE}

h2o_gbm_tot_grid_perf_tb <- read_csv2(
  "data/practical_application/results/h2o_gbm_tot_grid_perf_tb.csv"
)

h2o_gbm_tot_grid_perf_tb %>% 
  ggplot(aes(x = learn_rate, y = residual_deviance, color = max_depth)) +
  geom_vline(
    xintercept = h2o_gbm_tot_grid_perf_tb %>%
      filter(residual_deviance == min(residual_deviance)) %>%
      select(learn_rate) %>%
      as_vector(),
    color = viridis_pal()(10)[8],
    alpha = .8
  ) +
  geom_point(
    alpha = .8
  ) +
  scale_x_log10() +
  scale_y_log10() +
  scale_color_viridis(direction = -1) +
  coord_cartesian(ylim = c(NA, 0.22)) +
  labs(
    x = "Learn rate",
    y = "Cross Validation Deviance",
    color = "Max depth"
  )

```




\newpage

## Results {#chap:results}

<!--
Tabella coi Risultati
-->




```{r, models-results-render, echo = FALSE, cache = TRUE, message = FALSE}

# Make Table
tab <- results %>%
  select(
    id, model, test_deviance,
    time_string,
    # time1_string, time2_string,
    alpha, lambda,
    # ntrees, max_depth, learn_rate,
    # sample_rate, col_sample_rate
  ) %>%
  mutate(
    id = str_to_title(id),
    model = model %>%
      str_to_title() %>%
      str_replace_all("Aic", "AIC") %>%
      str_replace_all("Glm", "GLM") %>%
      str_replace_all("Gam", "GAM") %>%
      str_replace_all("Gbm", "GBM"),
    time_string = if_else(
      str_detect(time_string, "m"),
      str_extract(time_string, ".*m"),
      time_string,
    ),
    alpha = if_else(
      alpha == 0, "0",
      format(alpha, digits = 2)
    ),
    lambda = if_else(
      lambda == 0, "0",
      format(lambda, digits = 3)
    )
  ) %>% 
  rename_all(
    .funs = function(x){
      str_to_title(x) %>% 
        str_replace_all("_string", "") %>% 
        str_replace_all("_", " ")
    }
  ) %>%
  rename(
    "$\\alpha$" = Alpha,
    "$\\lambda$" = Lambda,
    `\\makecell[c]{Test\\\\Deviance}` = `Test deviance`
  ) 

# For compatibility with HTML
if(!knitr::is_latex_output()){
  names(tab) <- names(tab) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
  
  names(tab) <- names(tab) %>% 
    str_replace_all("\\\\makecell\\[[lrc]\\]\\{(.*)\\}", "\\1") %>% 
    str_replace_all("\\\\\\\\", "<br>")
}


tab %>%
  kable(
    digits = c(0,0,3,0,4,5),
    format.args = list(big.mark = " "),
    # format = "latex",
    booktabs = T,
    align = "llccccccccccc",
    vline = "",
    toprule = "\\toprule",
    # midrule = "\\toprule\\addlinespace",
    # midrule = "\\hline\\addlinespace",
    # midrule = "\\hline",
    midrule = "\\midrule",
    # linesep = "\\addlinespace\\hline\\addlinespace",
    linesep = "",
    bottomrule = "\\bottomrule",
    caption = "Models results.",
    label = "models-results",
    escape = FALSE
  ) %>%
  kable_styling(
    position = "center",
    latex_options = c("striped", "hold_position"),
    full_width = FALSE
    # full_width = TRUE
  ) %>%
  # row_spec(
  #   ifelse(!knitr::is_latex_output(), 0, 1),
  #   bold = T,
  # ) %>%
  row_spec(0, bold = T) #%>%
  # column_spec(2, italic = T)  %>%
  # landscape()

```



(ref:models-results-caption) Models results.

(ref:models-results-caption-short) Models results.

```{r, models-results-plot, out.width = "100%", fig.width = 6, fig.height = 3, fig.align='center', fig.cap = "(ref:models-results-caption)", fig.scap = "(ref:models-results-caption-short)", label = "models-results-plot", echo = FALSE, cache = FALSE}
results %>% 
  mutate(
    id = str_c(
      "mod", row_number()
    )
  ) %>% 
  select(
    id, model, test_deviance
  ) %>% 
  # filter(!(model %in% c("gam aic", "gbm tot"))) %>%
  mutate(
    id = str_to_title(id),
    model = model %>% 
      str_to_title() %>% 
      str_replace_all("Aic", "AIC") %>% 
      str_replace_all("Glm", "GLM") %>% 
      str_replace_all("Gam", "GAM") %>% 
      str_replace_all("Gbm", "GBM"),
    model = model %>% 
      fct_inorder() %>% 
      fct_rev()
  ) %>% 
  ggplot(aes(x = model, y = test_deviance)) +
  geom_segment(
    aes(xend = model, yend = -Inf),
    # size = 1
  ) +
  geom_point(
    # size = 4
  ) +
  coord_flip(ylim = c(NA, 8458.5)) +
  ylim(8456, NA) +
  labs(x = "", y = "Test Deviance")
```



<!-- HP Tuning -->


<!-- Comparison between models -->












